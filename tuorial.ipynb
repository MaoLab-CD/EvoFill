{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928e399c",
   "metadata": {},
   "source": [
    "# EvoFill working demo\n",
    "\n",
    "ver 4.   new imputation loss with evo loss\n",
    "\n",
    "ver 4.1  long range modules integrated in stage1 training\n",
    "\n",
    "ver 4.2  stage 3 fine tuning with under-reprensted population samples.\n",
    "\n",
    "last update: 2025/11/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efe343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "os.chdir('/mnt/qmtang/EvoFill/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed77817",
   "metadata": {},
   "source": [
    "## 0. Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23548ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver:   3.10.19 | packaged by conda-forge | (main, Oct 13 2025, 14:08:27) [GCC 14.3.0]\n",
      "Pytorch ver:  2.8.0+cu129\n",
      "Mamba ver:    2.2.5\n",
      "GPU in use:   NVIDIA H100 PCIe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mamba_ssm\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW, SparseAdam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Python ver:  ', sys.version)\n",
    "print('Pytorch ver: ', torch.__version__)\n",
    "print('Mamba ver:   ', mamba_ssm.__version__)\n",
    "print('GPU in use:  ', torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca64e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import GenotypeEncoder, GenomicDataset, GenomicDataset_Missing, ImputationDataset\n",
    "from src.model import EvoFill\n",
    "from src.loss import ImputationLoss, ImputationLoss_Missing\n",
    "from src.utils import setup_workdir, set_seed, precompute_maf, metrics_by_maf, print_maf_stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c4b1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path('/mnt/qmtang/EvoFill_data/20251121_chr22_v2')\n",
    "setup_workdir(work_dir)\n",
    "os.chdir(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7064af",
   "metadata": {},
   "source": [
    "## 1. Encoding all vcfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0a57f",
   "metadata": {},
   "source": [
    "### 1.1 pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d332f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 14,867 个位点  \n",
      "[DATA] EvoMat shape: (2904, 2904)\n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill_data/20251121_chr22_v2/pretrain\n",
      "[DATA] 位点矩阵 = (2904, 14867)，稀疏度 = 45.53%\n",
      "[DATA] 位点字典 = {'0|1': 1, '1|1': 2, '0|0': 0, '.|.': 3}，字典深度 = 4\n",
      "[DATA] 2,904 Samples\n",
      "[DATA] 14,867 Variants Sites\n",
      "[DATA] 4 seq_depth\n"
     ]
    }
   ],
   "source": [
    "gt_enc = GenotypeEncoder(phased = False, gts012 = False, save2disk = True, save_dir = Path(work_dir / \"pretrain\"))\n",
    "gt_enc = gt_enc.encode_new(vcf_path   = \"pretrain/major_pops.vcf.gz\" ,\n",
    "                           default_gt = 'ref',\n",
    "                           evo_mat    = \"pretrain/evo_mat_major_pops.tsv\")\n",
    "\n",
    "print(f\"[DATA] {gt_enc.n_samples:,} Samples\")\n",
    "print(f\"[DATA] {gt_enc.n_variants:,} Variants Sites\")\n",
    "print(f\"[DATA] {gt_enc.seq_depth} seq_depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71b241",
   "metadata": {},
   "source": [
    "### 1.2 augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "258362bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 14,867 个位点  \n",
      "[DATA] EvoMat shape: (17629, 17629)\n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill_data/20251121_chr22_v2/augment\n",
      "[DATA] 位点矩阵 = (17629, 14867)，稀疏度 = 59.42%，缺失率 = 0.00%\n",
      "[DATA] 位点字典 = {'0|1': 1, '1|1': 2, '0|0': 0, '.|.': 3, '0/1': 1, '1/1': 2, '0/0': 0, './.': 3}，字典深度 = 4\n",
      "[DATA] 17,629 Samples\n",
      "[DATA] 14,867 Variants Sites\n",
      "[DATA] 4 seq_depth\n"
     ]
    }
   ],
   "source": [
    "gt_enc_aug = GenotypeEncoder(phased=False, gts012=False, save2disk = True, save_dir = Path(work_dir / \"augment\"))\n",
    "gt_enc_aug = gt_enc_aug.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pretrain\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        default_gt    = 'miss',\n",
    "        vcf_path      = \"augment/chr22_trimmed_AADR_renamed.vcf.gz\",\n",
    "        evo_mat       = \"augment/evo_mat_aDNA.tsv\")\n",
    "\n",
    "print(f\"[DATA] {gt_enc_aug.n_samples:,} Samples\")\n",
    "print(f\"[DATA] {gt_enc_aug.n_variants:,} Variants Sites\")\n",
    "print(f\"[DATA] {gt_enc_aug.seq_depth} seq_depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f8de5",
   "metadata": {},
   "source": [
    "### 1.3 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de16b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 14,867 个位点  \n",
      "[DATA] EvoMat shape: (29, 29)\n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill_data/20251121_chr22_v2/finetune\n",
      "[DATA] 位点矩阵 = (29, 14867)，稀疏度 = 45.87%，缺失率 = 0.00%\n",
      "[DATA] 位点字典 = {'0|1': 1, '1|1': 2, '0|0': 0, '.|.': 3}，字典深度 = 4\n",
      "[URP] 29 samples, 14867 variants\n"
     ]
    }
   ],
   "source": [
    "gt_enc_urp = GenotypeEncoder(phased=False, gts012=False, save2disk=True, save_dir = Path(work_dir / \"finetune\"))\n",
    "gt_enc_urp = gt_enc_urp.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pretrain\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        default_gt    = 'ref',\n",
    "        vcf_path      = work_dir/\"finetune\"/\"minor_pops.10pct.vcf.gz\",\n",
    "        evo_mat       = work_dir/\"finetune\"/\"evo_mat_minor_pops.10pct.tsv\")\n",
    "\n",
    "print(f'[URP] {gt_enc_urp.n_samples} samples, {gt_enc_urp.n_variants} variants')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e19437",
   "metadata": {},
   "source": [
    "### 1.4 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae1b2e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 14,867 个位点  \n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill_data/20251121_chr22_v2/impute_in\n",
      "[DATA] 位点矩阵 = (269, 14867)，稀疏度 = 72.75%，缺失率 = 50.01%\n",
      "[DATA] 位点字典 = {'0|1': 1, '1|1': 2, '0|0': 0, '.|.': 3}，字典深度 = 4\n",
      "[INFER] 269 samples, 14867 variants\n"
     ]
    }
   ],
   "source": [
    "gt_enc_imp = GenotypeEncoder(phased=False, gts012=False, save2disk=True, save_dir = Path(work_dir / \"impute_in\"))\n",
    "gt_enc_imp = gt_enc_imp.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pretrain\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        default_gt    = 'ref',\n",
    "        vcf_path      = work_dir/\"impute_in\"/\"minor_pops.90pct.masked50p.vcf.gz\" )\n",
    "\n",
    "print(f'[INFER] {gt_enc_imp.n_samples} samples, {gt_enc_imp.n_variants} variants')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db71c0",
   "metadata": {},
   "source": [
    "## 2. Pretaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /mnt/qmtang/EvoFill/\n",
    "nohup env OMP_NUM_THREADS=4 CUDA_LAUNCH_BLOCKING=1 \\\n",
    "  accelerate launch --config_file ds_zero3.yaml \\\n",
    "  train_stage1_deepspeed.py \\\n",
    "  > logs/pretrian_chr22_251121.log 2>&1 &\n",
    "%%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc271a60",
   "metadata": {},
   "source": [
    "# 3. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /mnt/qmtang/EvoFill/\n",
    "nohup env OMP_NUM_THREADS=8 \\\n",
    "  accelerate launch --config_file ds_zero3.yaml \\\n",
    "  train_stage3_deepspeed.py \\\n",
    "  > logs/aug_chr22_251121.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31c2bd",
   "metadata": {},
   "source": [
    "### 1.4 Ultra-Long-Range LD Module Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e507273",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs_per_pair = 100\n",
    "lr                 = 5e-4\n",
    "weight_decay       = 1e-5\n",
    "earlystop_patience = 15\n",
    "batch_size         = 8\n",
    "verbose            = True\n",
    "\n",
    "criterion = ImputationLoss(use_r2=True, use_evo=True, r2_weight=1, evo_weight=4, evo_lambda=10)\n",
    "\n",
    "# ----------- 逐个 chunk 加载权重-----------\n",
    "# for cid in range(model.n_chunks):\n",
    "#     chunk_file = f'{work_dir}/models/{model_name}_chunk_{cid}.pth'\n",
    "#     ckpt = torch.load(chunk_file, map_location='cpu')\n",
    "#     model.chunk_embeds[cid].load_state_dict(ckpt['chunk_embed_state'])\n",
    "#     model.chunk_modules[cid].load_state_dict(ckpt['chunk_module_state'])\n",
    "\n",
    "# ----------- 加载第一阶段完整权重-----------\n",
    "ckpt = torch.load(f'{work_dir}/models/{model_name}_stage1.pth', map_location='cpu')\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "\n",
    "model.eval()        # chunk 专家冻结（requires_grad=False）\n",
    "\n",
    "# 收集所有稀疏参数（主要是嵌入层）\n",
    "sparse_params = []\n",
    "dense_params = []\n",
    "# 全局输出层的卷积参数（密集）\n",
    "dense_params.extend([model.global_out.w1, model.global_out.b1])\n",
    "dense_params.extend([model.global_out.w2, model.global_out.b2])\n",
    "# ULR默认启用\n",
    "if hasattr(model.global_out, 'ulr_mamba'):\n",
    "    for name, param in model.global_out.ulr_mamba.named_parameters():\n",
    "        if 'idx_embed' in name:\n",
    "            sparse_params.append(param)\n",
    "        else:\n",
    "            dense_params.append(param)\n",
    "\n",
    "# 创建分离优化器\n",
    "optim_sparse = SparseAdam(sparse_params, lr=lr) if sparse_params else None\n",
    "optim_dense = AdamW(dense_params, lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler_sparse = ReduceLROnPlateau(optim_sparse, mode='min', factor=0.5, patience=5, min_lr=1e-9) if optim_sparse else None\n",
    "scheduler_dense = ReduceLROnPlateau(optim_dense, mode='min', factor=0.5, patience=5, min_lr=1e-9)\n",
    "\n",
    "pair_list = list(combinations(range(model.n_chunks), 2))\n",
    "np.random.shuffle(pair_list)          # 打乱\n",
    "total_pairs = len(pair_list)\n",
    "\n",
    "for pair_idx, (cid1, cid2) in enumerate(pair_list, 1):\n",
    "    # ====== 构造并集 mask ======\n",
    "    union_mask = (model.chunk_masks[cid1] + model.chunk_masks[cid2]).clamp(max=1).bool()\n",
    "    train_logs_sum = None\n",
    "    val_logs_sum   = None\n",
    "    \n",
    "    # 并集 MAF\n",
    "    union_maf, union_bin_cnt = precompute_maf(\n",
    "        gt_enc.X_gt[:, union_mask.cpu().numpy()].toarray(),\n",
    "        mask_int=gt_enc.seq_depth\n",
    "    )\n",
    "\n",
    "    # ====== 早停变量 ======\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    is_early_stopped = False\n",
    "\n",
    "    # ====== 训练循环 ======\n",
    "    for epoch in range(max_epochs_per_pair):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_prob, train_gts, train_mask = [], [], []\n",
    "\n",
    "        pbar = tqdm(train_loader,\n",
    "                    desc=f'Comb {pair_idx}/{total_pairs}  '\n",
    "                         f'{cid1+1}-{cid2+1}  Epoch {epoch+1}/{max_epochs_per_pair}',\n",
    "                    leave=False)\n",
    "        for x, target, evo_mat in pbar:\n",
    "            x,  target = x.to(device), target.to(device)\n",
    "            if evo_mat.numel() == 0:\n",
    "                evo_mat = None\n",
    "            else:\n",
    "                evo_mat = evo_mat.to(device)\n",
    "\n",
    "            optim_sparse.zero_grad()\n",
    "            optim_dense.zero_grad()\n",
    "\n",
    "            logits, prob, mask_idx = model(x, [cid1, cid2])\n",
    "            loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:,mask_idx], evo_mat) \n",
    "            loss.backward()\n",
    "\n",
    "            optim_sparse.step()   # 只更新嵌入表\n",
    "            optim_dense.step()    # 更新其余所有参数\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if train_logs_sum is None:          # 第一次初始化\n",
    "                train_logs_sum = {k: 0.0 for k in logs}\n",
    "            for k, v in logs.items():\n",
    "                train_logs_sum[k] += v\n",
    "            # pbar.set_postfix({'loss': loss.item(), 'ce':logs['ce'], 'r2':logs['r2'], 'evo':logs['evo']})\n",
    "\n",
    "            # 收集指标\n",
    "            miss_mask = x[:,union_mask][..., -1].bool()\n",
    "            train_prob.append(prob[:, mask_idx].detach())\n",
    "            train_gts.append(target[:,mask_idx].detach())\n",
    "            train_mask.append(miss_mask)\n",
    "\n",
    "        # 训练集 MAF\n",
    "        train_prob = torch.cat(train_prob, dim=0)\n",
    "        train_gts  = torch.cat(train_gts,    dim=0)\n",
    "        train_mask = torch.cat(train_mask,   dim=0)\n",
    "\n",
    "        # ----------- 验证 -----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_prob, val_gts = [], []\n",
    "        with torch.no_grad():\n",
    "            if val_logs_sum is None:\n",
    "                val_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "            for x, target, evo_mat in test_loader:\n",
    "                x = x.to(device)\n",
    "                target = target.to(device)\n",
    "                evo_mat = evo_mat.to(device) if evo_mat.numel() else None\n",
    "                logits, prob, mask_idx = model(x, [cid1, cid2])\n",
    "                loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:,mask_idx], evo_mat)\n",
    "                val_loss += loss.item()\n",
    "                for k, v in logs.items():\n",
    "                    val_logs_sum[k] += v\n",
    "                val_prob.append(prob[:,mask_idx])\n",
    "                val_gts.append(target[:,mask_idx])\n",
    "\n",
    "        val_prob = torch.cat(val_prob, dim=0)\n",
    "        val_gts  = torch.cat(val_gts,    dim=0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss   = val_loss   / len(test_loader)\n",
    "        avg_train_logs = {k: v / len(train_loader) for k, v in train_logs_sum.items()}\n",
    "        avg_val_logs = {k: v / len(test_loader) for k, v in val_logs_sum.items()}\n",
    "        \n",
    "        scheduler_sparse.step(val_loss)\n",
    "        scheduler_dense.step(val_loss)\n",
    "\n",
    "        current_denselr = optim_dense.param_groups[0]['lr']\n",
    "        current_sparselr = optim_sparse.param_groups[0]['lr']\n",
    "\n",
    "        log_str = (f'Comb {pair_idx}/{total_pairs}  '\n",
    "            f'{cid1+1}-{cid2+1}  Epoch {epoch+1}/{max_epochs_per_pair} '\n",
    "            f'Total Loss, Train = {avg_train_loss:.1f}, '\n",
    "            f'Val = {avg_val_loss:.1f}, '\n",
    "            f'dense LR: {current_denselr:.2e}, '\n",
    "            f'sparse LR: {current_sparselr:.2e}')\n",
    "        log_str += '\\n        Train'\n",
    "        for k, v in avg_train_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        log_str += '\\n        Val  '\n",
    "        for k, v in avg_val_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        print(log_str)\n",
    "        # 清空累加器，供下一个 epoch 使用\n",
    "        train_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "        val_logs_sum   = {k: 0.0 for k in val_logs_sum}\n",
    "        # 早停\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'comb': (cid1, cid2),\n",
    "                'global_out': model.global_out.state_dict(),\n",
    "                'best_val_loss': best_loss,\n",
    "                'epoch': epoch,\n",
    "            }, f'{work_dir}/models/{model_name}_chunk[{cid1}-{cid2}].pth')\n",
    "            # MAF 表格\n",
    "            predres_with_bestloss = (train_prob, train_gts, val_prob, val_gts)\n",
    "            if verbose:\n",
    "                train_bins_metrics = metrics_by_maf(train_prob, train_gts, gt_enc.hap_map, union_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_prob,   val_gts, gt_enc.hap_map, union_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"val\":   val_bins_metrics})\n",
    "                print(f'  --> updated {model_name}_chunk[{cid1+1}-{cid2+1}].pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= earlystop_patience:\n",
    "                is_early_stopped = True\n",
    "                print(f'Pair {cid1+1}-{cid2+1} early stopping')\n",
    "                train_prob, train_gts, val_prob, val_gts = predres_with_bestloss\n",
    "                train_bins_metrics = metrics_by_maf(train_prob, train_gts, gt_enc.hap_map, union_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_prob,   val_gts, gt_enc.hap_map, union_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"val\":   val_bins_metrics})\n",
    "                break\n",
    "            \n",
    "    if not is_early_stopped:\n",
    "        predres_with_bestloss = (train_prob, train_gts, val_prob, val_gts)\n",
    "        train_bins_metrics = metrics_by_maf(train_prob, train_gts, gt_enc.hap_map, union_maf, mask=train_mask)\n",
    "        val_bins_metrics   = metrics_by_maf(val_prob,   val_gts, gt_enc.hap_map, union_maf, mask=None)\n",
    "        print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"val\":   val_bins_metrics})\n",
    "\n",
    "    # del optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ----------- 全部 pair 结束 -> 保存最终模型 -----------\n",
    "torch.save({\n",
    "    'model_state': model.state_dict(),\n",
    "    'ulr_enabled': True,\n",
    "}, f'{work_dir}/models/{model_name}_stage2_final.pth')\n",
    "print(f'==> STAGE2 training finished: {work_dir}/models/{model_name}_stage2_final.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca99878",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning (Few-shot URP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0498be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 99,314 个位点  \n",
      "[DATA] EvoMat shape: (16, 16)\n",
      "[DATA] 位点矩阵 = (16, 99314)，稀疏度 = 27.35%，缺失率 = 0.00%\n",
      "[DATA] 位点字典 = {'0|0': 0, '0|1': 1, '1|1': 2, '.|.': 3}，字典深度 = 4\n",
      "[URP] 16 samples, 99314 variants\n"
     ]
    }
   ],
   "source": [
    "# %%  载入 URP 微调数据\n",
    "gt_enc_urp = GenotypeEncoder(phased=False, gts012=False, save2disk=False)\n",
    "gt_enc_urp = gt_enc_urp.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pre_train\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        vcf_path      = work_dir/\"urp_finetune\"/\"minor_pops.10pct.vcf.gz\",\n",
    "        evo_mat       = work_dir/\"urp_finetune\"/\"evo_mat_minor_pops.10pct.tsv\")\n",
    "\n",
    "print(f'[URP] {gt_enc_urp.n_samples} samples, {gt_enc_urp.n_variants} variants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62237762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# %%  Stage-3 超参与配置\n",
    "model_name  = 'hg19_chr22trim'\n",
    "stage3_tag       = 'stage3'\n",
    "max_epochs       = 50\n",
    "warmup_epochs    = 3\n",
    "lr_dense         = 1e-4          # GlobalOut 中稠密参数\n",
    "lr_sparse        = 5e-5          # ULR 中的 idx_embed\n",
    "weight_decay     = 1e-4\n",
    "earlystop_pat    = 9\n",
    "mask_rate_range  = (0.2, 0.6)    # 数据增强：随机缺失率\n",
    "k_fold           = 5             # 交叉验证\n",
    "batch_size       = 4             # 样本少，用小 batch\n",
    "accumulate_grad  = 2             # 梯度累加，等效 batch=8\n",
    "\n",
    "# %%  重新建立「微调」Dataset / Loader\n",
    "urp_dataset = GenomicDataset(\n",
    "        gt_enc_urp.X_gt,\n",
    "        evo_mat      = gt_enc_urp.evo_mat,\n",
    "        seq_depth    = gt_enc_urp.seq_depth,\n",
    "        mask         = True,\n",
    "        masking_rates= mask_rate_range,\n",
    "        indices      = None)               # 全部用于微调\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = torch.stack([b[0] for b in batch])\n",
    "    y = torch.stack([b[1] for b in batch])\n",
    "    idx = [b[2] for b in batch]\n",
    "    if gt_enc_urp.evo_mat is not None:\n",
    "        evo = gt_enc_urp.evo_mat[np.ix_(idx, idx)]\n",
    "        evo = torch.FloatTensor(evo)\n",
    "    else:\n",
    "        evo = torch.empty(0)\n",
    "    return x, y, evo\n",
    "\n",
    "urp_loader = DataLoader(urp_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4,\n",
    "                        pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# 1. 准备 URP 数据\n",
    "urp_idx = np.arange(gt_enc_urp.n_samples)\n",
    "kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a16f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  载入 Stage-2 最终权重\n",
    "ckpt = torch.load(f'{work_dir}/models/{model_name}_stage1.pth', map_location='cpu')\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "print('[Stage3] Stage-2 weights loaded.')\n",
    "\n",
    "# %%  参数分组 & 优化器\n",
    "for p in model.parameters():                # 先全部冻结\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 只解冻需要的部分\n",
    "trainable_dense, trainable_sparse = [], []\n",
    "# 1. GlobalOut 全部\n",
    "for name, p in model.global_out.named_parameters():\n",
    "    if 'idx_embed' in name:\n",
    "        trainable_sparse.append(p)\n",
    "    else:\n",
    "        trainable_dense.append(p)\n",
    "# 2. Chunk-Embedding（可选，若显存紧张可留冻）\n",
    "for emb in model.chunk_embeds:\n",
    "    for p in emb.parameters():\n",
    "        trainable_dense.append(p)\n",
    "\n",
    "for p in trainable_dense+trainable_sparse:\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt_dense  = AdamW(trainable_dense,  lr=lr_dense,\n",
    "                   weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "opt_sparse = SparseAdam(trainable_sparse, lr=lr_sparse)\n",
    "\n",
    "# 余弦退火 + 热身\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return epoch / warmup_epochs\n",
    "    return 0.5*(1+np.cos(np.pi*(epoch-warmup_epochs)/(max_epochs-warmup_epochs)))\n",
    "\n",
    "sched_dense  = torch.optim.lr_scheduler.LambdaLR(opt_dense,  lr_lambda)\n",
    "sched_sparse = torch.optim.lr_scheduler.LambdaLR(opt_sparse, lr_lambda)\n",
    "\n",
    "# %%  训练循环\n",
    "criterion = ImputationLoss(use_r2=True, use_evo=True,\n",
    "                           r2_weight=1, evo_weight=4, evo_lambda=10)\n",
    "\n",
    "best_avg_val_loss, patience_counter = np.inf, 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    fold_val_loss = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(urp_idx)):\n",
    "        # ---- 当前折数据 ----\n",
    "        train_dataset = GenomicDataset(\n",
    "                gt_enc_urp.X_gt, evo_mat=gt_enc_urp.evo_mat,\n",
    "                seq_depth=gt_enc_urp.seq_depth, mask=True,\n",
    "                masking_rates=(0.2, 0.6), indices=train_idx)\n",
    "        val_dataset   = GenomicDataset(\n",
    "                gt_enc_urp.X_gt, evo_mat=gt_enc_urp.evo_mat,\n",
    "                seq_depth=gt_enc_urp.seq_depth, mask=True,\n",
    "                masking_rates=(0.2, 0.6), indices=val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8,\n",
    "                                  shuffle=True, num_workers=2,\n",
    "                                  collate_fn=collate_fn, pin_memory=True)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=8,\n",
    "                                  shuffle=False, num_workers=2,\n",
    "                                  collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "        # ---- 训练 ----\n",
    "        for step, (x, y, evo) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            evo  = evo.to(device) if evo.numel() else None\n",
    "\n",
    "            logits, prob, mask_idx = model(x)\n",
    "            loss, _ = criterion(logits[:, mask_idx], prob[:, mask_idx],\n",
    "                                y[:, mask_idx], evo)\n",
    "            loss.backward()\n",
    "\n",
    "            if (step+1) % accumulate_grad == 0 or (step+1) == len(train_loader):\n",
    "                opt_dense.step(); opt_sparse.step()\n",
    "                opt_dense.zero_grad(set_to_none=True); opt_sparse.zero_grad(set_to_none=True)\n",
    "\n",
    "        # ---- 验证 ----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y, evo in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                evo  = evo.to(device) if evo.numel() else None\n",
    "                logits, prob, mask_idx = model(x)\n",
    "                loss, _ = criterion(logits[:, mask_idx], prob[:, mask_idx],\n",
    "                                    y[:, mask_idx], evo)\n",
    "                val_loss += loss.item()\n",
    "        fold_val_loss.append(val_loss / len(val_loader))\n",
    "\n",
    "    # ---- epoch 级日志 & 调度 ----\n",
    "    avg_val_loss = np.mean(fold_val_loss)\n",
    "    print(f'Epoch {epoch+1}: avg_val_loss={avg_val_loss:.3f}, '\n",
    "          f'lr_dense={opt_dense.param_groups[0][\"lr\"]:.1e}')\n",
    "    sched_dense.step(); sched_sparse.step()\n",
    "\n",
    "    # ---- 早停 ----\n",
    "    if avg_val_loss < best_avg_val_loss:\n",
    "        best_avg_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({'model_state': model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'avg_val_loss': avg_val_loss},\n",
    "                   f'{work_dir}/models/{model_name}_{stage3_tag}_best.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= earlystop_pat:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "torch.save({'model_state': model.state_dict(),\n",
    "            'stage3_tag': stage3_tag},\n",
    "           f'{work_dir}/models/{model_name}_{stage3_tag}_final.pth')\n",
    "print(f'==> Stage-3 KFold-loss fine-tuning finished. Best avg_val_loss={best_avg_val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718f797",
   "metadata": {},
   "source": [
    "## 4. Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034fdcb",
   "metadata": {},
   "source": [
    "### 3.1 Load the trained model\n",
    "\n",
    "Choose a path where including `<work_dir>/model` and have trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df4c2e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Dir: /mnt/qmtang/EvoFill_data/20251121_chr22_v2\n",
      "[INF] Model[chr22_trim] loaded.\n"
     ]
    }
   ],
   "source": [
    "work_dir = Path(\"/mnt/qmtang/EvoFill_data/20251121_chr22_v2/\")\n",
    "print(f\"Work Dir: {work_dir}\")\n",
    "\n",
    "# ---- 1. 加载模型 ----\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "json_path = f\"{work_dir}/models/model_meta.json\"\n",
    "meta = json.load(open(json_path))\n",
    "model = EvoFill(\n",
    "    d_model=int(meta[\"d_model\"]),\n",
    "    n_alleles=int(meta[\"alleles\"]),\n",
    "    total_sites=int(meta[\"total_sites\"]),\n",
    "    chunk_size=int(meta[\"chunk_size\"]),\n",
    "    chunk_overlap=int(meta[\"overlap\"])\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(f'{work_dir}/models/{meta[\"model_name\"]}_stage1.pth', map_location=device)\n",
    "# ckpt = torch.load(f'{work_dir}/models/{meta[\"model_name\"]}_stage3.pth', map_location=device)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "model.eval()\n",
    "print(f'[INF] Model[{meta[\"model_name\"]}] loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542df7b",
   "metadata": {},
   "source": [
    "### 3.2 Encode .vcf file need be impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55e661be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 14,867 个位点  \n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill_data/20251121_chr22_v2/impute_in\n",
      "[DATA] 位点矩阵 = (269, 14867)，稀疏度 = 72.75%，缺失率 = 50.01%\n",
      "[DATA] 位点字典 = {'0|1': 1, '1|1': 2, '0|0': 0, '.|.': 3}，字典深度 = 4\n",
      "[INFER] 269 samples, 14867 variants\n",
      "[ImputationDataset] 269 samples, missing rate = 50.01%\n"
     ]
    }
   ],
   "source": [
    "gt_enc_imp = GenotypeEncoder(phased=False, gts012=False, save2disk=True, save_dir = Path(work_dir / \"impute_in\"))\n",
    "gt_enc_imp = gt_enc_imp.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pretrain\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        default_gt    = 'ref',\n",
    "        vcf_path      = work_dir/\"impute_in\"/\"minor_pops.90pct.masked50p.vcf.gz\" )\n",
    "\n",
    "print(f'[INFER] {gt_enc_imp.n_samples} samples, {gt_enc_imp.n_variants} variants')\n",
    "\n",
    "# ---- 2. 构建推理 Dataset / Loader ----\n",
    "imp_dataset = ImputationDataset(\n",
    "    x_gts_sparse=gt_enc_imp.X_gt,\n",
    "    seq_depth=gt_enc_imp.seq_depth,\n",
    "    indices=None                 # 可传入指定样本索引\n",
    ")\n",
    "imp_dataset.print_missing_stat()          # 查看原始缺失比例\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_onehot = torch.stack([item[0] for item in batch])\n",
    "    real_idx_list = [item[1] for item in batch]\n",
    "    return x_onehot, real_idx_list   # 无 y\n",
    "\n",
    "imp_loader = torch.utils.data.DataLoader(\n",
    "    imp_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809730fd",
   "metadata": {},
   "source": [
    "### 3.3 Inferring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d5223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputing: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INF] 概率矩阵已保存 → /mnt/qmtang/EvoFill_data/20251121_chr22_v2/impute_out/impute_prob.npy with shape = (269, 14867, 3) \n"
     ]
    }
   ],
   "source": [
    "y_prob = []\n",
    "y_mask = []\n",
    "with torch.no_grad():\n",
    "    for x_onehot, real_idx in tqdm(imp_loader, desc='Imputing'):\n",
    "        x_onehot = x_onehot.to(device)\n",
    "        _, prob, _ = model(x_onehot)\n",
    "        miss_mask = x_onehot[..., -1].bool()\n",
    "        y_prob.append(prob)\n",
    "        y_mask.append(miss_mask)\n",
    "y_prob = torch.cat(y_prob, dim=0).cpu().numpy()\n",
    "y_mask = torch.cat(y_mask, dim=0).cpu().numpy()\n",
    "# 4. 保存\n",
    "out_dir = os.path.join(work_dir, 'impute_out')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "np.save(os.path.join(out_dir, 'impute_prob.npy'), y_prob)\n",
    "np.save(os.path.join(out_dir, 'impute_mask.npy'), y_mask)\n",
    "print(f'[INF] 概率矩阵已保存 → {out_dir}/impute_prob.npy '\n",
    "      f'with shape = {y_prob.shape} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27d710",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the imputation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b435b8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 14,867 个位点  \n",
      "[DATA] 位点矩阵 = (269, 14867)，稀疏度 = 45.44%，缺失率 = 0.00%\n",
      "[DATA] 位点字典 = {'0|1': 1, '1|1': 2, '0|0': 0, '.|.': 3}，字典深度 = 4\n",
      "     MAF_bin Counts val_Acc val_INFO val_IQS val_MaCH\n",
      "(0.00, 0.05)   1546   0.978    0.299   0.319    0.572\n",
      "(0.05, 0.10)   1246   0.957    0.539   0.585    0.796\n",
      "(0.10, 0.20)   2205   0.934    0.672   0.705    0.901\n",
      "(0.20, 0.30)   3112   0.904    0.768   0.779    0.960\n",
      "(0.30, 0.40)   5193   0.888    0.804   0.801    0.976\n",
      "(0.40, 0.50)   1565   0.878    0.795   0.790    0.970\n"
     ]
    }
   ],
   "source": [
    "gt_enc_true = GenotypeEncoder(phased=False, gts012=False, save2disk=False)\n",
    "gt_enc_true = gt_enc_true.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pretrain\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        default_gt    = 'ref',\n",
    "        vcf_path      = work_dir/\"impute_out\"/\"minor_pops.90pct.vcf.gz\" )\n",
    "y_true = gt_enc_true.X_gt.toarray()\n",
    "maf, bin_cnt = precompute_maf(y_true,  mask_int=gt_enc_true.seq_depth)\n",
    "y_true_oh = np.eye(gt_enc_true.seq_depth - 1)[y_true]\n",
    "bins_metrics   = metrics_by_maf(y_prob, y_true_oh, hap_map = gt_enc_true.hap_map, maf_vec = maf, mask=y_mask)\n",
    "print_maf_stat_df(bin_cnt,{'val': bins_metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39543bcb",
   "metadata": {},
   "source": [
    "= STAGE 1 =\n",
    "\n",
    "|      MAF_bin | Counts | val_Acc | val_INFO | val_IQS | val_MaCH |\n",
    "| :----------: | :----: | ------: | -------: | ------: | -------: |\n",
    "| (0.00, 0.05) |   1546 |  0.978  |    0.299 |  0.319  |    0.572 |\n",
    "| (0.05, 0.10) |   1246 |  0.957  |    0.539 |  0.585  |    0.796 |\n",
    "| (0.10, 0.20) |   2205 |  0.934  |    0.672 |  0.705  |    0.901 |\n",
    "| (0.20, 0.30) |   3112 |  0.904  |    0.768 |  0.779  |    0.960 |\n",
    "| (0.30, 0.40) |   5193 |  0.888  |    0.804 |  0.801  |    0.976 |\n",
    "| (0.40, 0.50) |   1565 |  0.878  |    0.795 |  0.790  |    0.970 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94e3af",
   "metadata": {},
   "source": [
    "### 3.5 Saving to .vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3c07302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INF] 缺失位点填充完成 → /mnt/qmtang/EvoFill_data/20251107_ver4/impute_out/imputed.vcf.gz\n"
     ]
    }
   ],
   "source": [
    "from cyvcf2 import VCF, Writer\n",
    "\n",
    "# 0. 路径\n",
    "ref_vcf = \"/mnt/qmtang/EvoFill_data/20251107_ver4/minor_pops.masked30p.vcf.gz\"\n",
    "out_vcf = os.path.join(out_dir, 'imputed.vcf.gz')\n",
    "\n",
    "n_site = gt_enc.n_variants\n",
    "n_samp = gt_enc.n_samples\n",
    "n_alleles = gt_enc.seq_depth - 1\n",
    "assert y_prob.shape == (n_samp, n_site, n_alleles)\n",
    "\n",
    "# 2. 反向映射  idx -> '0|0' / '0|1' / ...\n",
    "rev_hap_map = {v: k for k, v in gt_enc.hap_map.items()}\n",
    "\n",
    "samp2idx = {sid: i for i, sid in enumerate(gt_enc.sample_ids)}\n",
    "\n",
    "# 4. 打开参考 VCF\n",
    "invcf = VCF(ref_vcf)\n",
    "tmpl  = invcf\n",
    "tmpl.set_samples(gt_enc.sample_ids)   # 替换样本列\n",
    "\n",
    "out = Writer(out_vcf, tmpl, mode='wz')\n",
    "\n",
    "for rec_idx, rec in enumerate(invcf):\n",
    "    # 当前位点全部样本的 GT\n",
    "    gt_int_pairs = []\n",
    "    for samp_idx, sample_id in enumerate(gt_enc.sample_ids):\n",
    "        old_gt = rec.genotypes[samp_idx]          # [allele1, allele2, phased]\n",
    "        if old_gt[0] == -1 or old_gt[1] == -1:    # 缺失\n",
    "            prob_vec = y_prob[samp_idx, rec_idx, :].ravel()\n",
    "            best_idx = int(prob_vec.argmax())\n",
    "            gt_str   = rev_hap_map[best_idx]\n",
    "            alleles  = list(map(int, gt_str.split('|')))\n",
    "            phased   = old_gt[2] if old_gt[2] != -1 else 1\n",
    "            gt_int_pairs.append([alleles[0], alleles[1], phased])\n",
    "        else:                                       # 非缺失，保持原样\n",
    "            gt_int_pairs.append(old_gt)\n",
    "\n",
    "    # 转成 int8 二维数组  (n_sample, 3)  last dim = [a1,a2,phased]\n",
    "    gt_array = np.array(gt_int_pairs, dtype=np.int8)\n",
    "    rec.set_format('GT', gt_array)\n",
    "    out.write_record(rec)\n",
    "\n",
    "invcf.close()\n",
    "out.close()\n",
    "\n",
    "# 5. tabix\n",
    "os.system(f'tabix -fp vcf {out_vcf}')\n",
    "print(f'[INF] 缺失位点填充完成 → {out_vcf}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
