{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6bc4ec",
   "metadata": {},
   "source": [
    "ver2: 位点相对坐标改绝对坐标(cM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bb00d",
   "metadata": {},
   "source": [
    "## Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757312f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # 设置用GPU1\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "from typing import Union\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mamba_ssm import Mamba2\n",
    "from mamba_ssm.modules.mamba2_simple import Mamba2Simple as Mamba2Block # 原Mamba2Block\n",
    "from torch_optimizer import Lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975df815",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5eeded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_FILE_FORMATS = {\"vcf\", \"csv\", \"tsv\"}\n",
    "class DataReader:\n",
    "    def __init__(self):\n",
    "        self.target_is_gonna_be_phased = None\n",
    "        self.target_set = None\n",
    "        self.target_sample_value_index = 2\n",
    "        self.ref_sample_value_index = 2\n",
    "        self.target_file_extension = None\n",
    "        self.allele_count = 2\n",
    "        self.genotype_vals = None\n",
    "        self.ref_is_phased = None\n",
    "        self.reference_panel = None\n",
    "        self.VARIANT_COUNT = 0\n",
    "        self.is_phased = False\n",
    "        self.MISSING_VALUE = None\n",
    "        self.ref_is_hap = False\n",
    "        self.target_is_hap = False\n",
    "        self.ref_n_header_lines = []\n",
    "        self.ref_n_data_header = \"\"\n",
    "        self.target_n_header_lines = []\n",
    "        self.target_n_data_header = \"\"\n",
    "        self.ref_separator = None\n",
    "        self.map_values_1_vec = np.vectorize(self.__map_hap_2_ind_parent_1)\n",
    "        self.map_values_2_vec = np.vectorize(self.__map_hap_2_ind_parent_2)\n",
    "        self.map_haps_to_vec = np.vectorize(self.__map_haps_2_ind)\n",
    "        self.delimiter_dictionary = {\"vcf\": \"\\t\", \"csv\": \",\", \"tsv\": \"\\t\", \"infer\": \"\\t\"}\n",
    "        self.ref_file_extension = \"vcf\"\n",
    "        self.test_file_extension = \"vcf\"\n",
    "        self.target_is_phased = True\n",
    "        self.id2gmap = {}          # ID -> (rate_cM_Mb, cM)\n",
    "\n",
    "    def __read_csv(self, file_path, is_vcf=False, is_reference=False, separator=\"\\t\", first_column_is_index=True,\n",
    "                   comments=\"##\") -> pd.DataFrame:\n",
    "        \"\"\"Read CSV/VCF files\"\"\"\n",
    "        print(\"DATA: Reading the file...\")\n",
    "        data_header = None\n",
    "        line_counter = 0\n",
    "        root, ext = os.path.splitext(file_path)\n",
    "        with gzip.open(file_path, 'rt') if ext == '.gz' else open(file_path, 'rt') as f_in:\n",
    "            while True:\n",
    "                line = f_in.readline()\n",
    "                if line.startswith(comments):\n",
    "                    line_counter += 1\n",
    "                    if is_reference:\n",
    "                        self.ref_n_header_lines.append(line)\n",
    "                    else:\n",
    "                        self.target_n_header_lines.append(line)\n",
    "                else:\n",
    "                    data_header = line\n",
    "                    break\n",
    "        if data_header is None:\n",
    "            raise IOError(\"The file only contains comments!\")\n",
    "\n",
    "        # ------------ 新增：抽 ID 与 GeneticPos_cM ------------\n",
    "        if is_vcf and is_reference:\n",
    "            df_head = pd.read_csv(\n",
    "                file_path,\n",
    "                comment='#',\n",
    "                sep='\\t',\n",
    "                header=0,\n",
    "                usecols=[2, 7],          # ID 列 和 INFO 列\n",
    "                names=['ID', 'INFO'],\n",
    "                iterator=True,\n",
    "                chunksize=50_000\n",
    "            )\n",
    "            cM_pat   = re.compile(r'GeneticPos_cM=([\\d.]+)')\n",
    "            rate_pat = re.compile(r'RecombRate_cM_Mb=([\\d.]+)')\n",
    "            for chunk in df_head:\n",
    "                for _id, _info in zip(chunk.ID, chunk.INFO):\n",
    "                    info = str(_info)\n",
    "                    m_cM   = cM_pat.search(info)\n",
    "                    m_rate = rate_pat.search(info)\n",
    "                    cM   = float(m_cM.group(1))   if m_cM   else 0.0\n",
    "                    rate = float(m_rate.group(1)) if m_rate else 0.0\n",
    "                    self.id2gmap[_id] = (rate, cM)\n",
    "            print(f\"DATA: ref gmap range: \"\n",
    "                  f\"rate {min(t[0] for t in self.id2gmap.values()):.3f}-\"\n",
    "                  f\"{max(t[0] for t in self.id2gmap.values()):.3f} cM/Mb, \"\n",
    "                  f\"cM {min(t[1] for t in self.id2gmap.values()):.3f}-\"\n",
    "                  f\"{max(t[1] for t in self.id2gmap.values()):.3f} cM\")\n",
    "\n",
    "        df = dt.fread(file=file_path, sep=separator, header=True, skip_to_line=line_counter + 1)\n",
    "        df = df.to_pandas()\n",
    "        if first_column_is_index:\n",
    "            df.set_index(df.columns[0], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def __find_file_extension(self, file_path, file_format, delimiter):\n",
    "        separator = \"\\t\"\n",
    "        found_file_format = None\n",
    "        if file_format not in [\"infer\"] + list(SUPPORTED_FILE_FORMATS):\n",
    "            raise ValueError(\"File extension must be one of {'vcf', 'csv', 'tsv', 'infer'}.\")\n",
    "        if file_format == 'infer':\n",
    "            file_name_tokenized = file_path.split(\".\")\n",
    "            for possible_extension in file_name_tokenized[::-1]:\n",
    "                if possible_extension in SUPPORTED_FILE_FORMATS:\n",
    "                    found_file_format = possible_extension\n",
    "                    separator = self.delimiter_dictionary[possible_extension] if delimiter is None else delimiter\n",
    "                    break\n",
    "            if found_file_format is None:\n",
    "                logging.warning(\"Could not infer the file type. Using tsv as the last resort.\")\n",
    "                found_file_format = \"tsv\"\n",
    "        else:\n",
    "            found_file_format = file_format\n",
    "            separator = self.delimiter_dictionary[file_format] if delimiter is None else delimiter\n",
    "        return found_file_format, separator\n",
    "\n",
    "    def assign_training_set(self, file_path: str, target_is_gonna_be_phased_or_haps: bool,\n",
    "                            variants_as_columns: bool = False, delimiter=None, file_format=\"infer\",\n",
    "                            first_column_is_index=True, comments=\"##\") -> None:\n",
    "        self.target_is_gonna_be_phased = target_is_gonna_be_phased_or_haps\n",
    "        self.ref_file_extension, self.ref_separator = self.__find_file_extension(file_path, file_format, delimiter)\n",
    "\n",
    "        self.reference_panel = self.__read_csv(file_path, is_reference=True, is_vcf=False, separator=self.ref_separator,\n",
    "                                               first_column_is_index=first_column_is_index,\n",
    "                                               comments=comments) if self.ref_file_extension != 'vcf' else self.__read_csv(\n",
    "            file_path, is_reference=True, is_vcf=True, separator='\\t', first_column_is_index=False, comments=\"##\")\n",
    "\n",
    "        if self.ref_file_extension != \"vcf\":\n",
    "            if variants_as_columns:\n",
    "                self.reference_panel = self.reference_panel.transpose()\n",
    "            self.reference_panel.reset_index(drop=False, inplace=True)\n",
    "            self.reference_panel.rename(columns={self.reference_panel.columns[0]: \"ID\"}, inplace=True)\n",
    "        else:\n",
    "            self.ref_sample_value_index += 8\n",
    "\n",
    "        self.ref_is_hap = not (\"|\" in self.reference_panel.iloc[0, self.ref_sample_value_index - 1] or \"/\" in\n",
    "                               self.reference_panel.iloc[0, self.ref_sample_value_index - 1])\n",
    "        self.ref_is_phased = \"|\" in self.reference_panel.iloc[0, self.ref_sample_value_index - 1]\n",
    "\n",
    "        if self.ref_is_hap and not target_is_gonna_be_phased_or_haps:\n",
    "            raise ValueError(\n",
    "                \"Reference contains haploids while target will be unphased diploids. Model cannot predict target.\")\n",
    "\n",
    "        if not (self.ref_is_phased or self.ref_is_hap) and target_is_gonna_be_phased_or_haps:\n",
    "            raise ValueError(\n",
    "                \"Reference contains unphased diploids while target will be phased/haploid. Model cannot predict target.\")\n",
    "\n",
    "        self.VARIANT_COUNT = self.reference_panel.shape[0]\n",
    "        print(\n",
    "            f\"DATA: {self.reference_panel.shape[1] - (self.ref_sample_value_index - 1)} {'haploid' if self.ref_is_hap else 'diploid'} samples with {self.VARIANT_COUNT} variants found!\")\n",
    "\n",
    "        self.is_phased = target_is_gonna_be_phased_or_haps and (self.ref_is_phased or self.ref_is_hap)\n",
    "\n",
    "        original_allele_sep = \"|\" if self.ref_is_phased or self.ref_is_hap else \"/\"\n",
    "        final_allele_sep = \"|\" if self.is_phased else \"/\"\n",
    "\n",
    "        def get_diploid_alleles(genotype_vals):\n",
    "            allele_set = set()\n",
    "            for genotype_val in genotype_vals:\n",
    "                if genotype_val not in [\".\", \".|.\", \"./.\"]:\n",
    "                    if final_allele_sep in genotype_val:\n",
    "                        v1, v2 = genotype_val.split(final_allele_sep)\n",
    "                        allele_set.update([v1, v2])\n",
    "                    else:\n",
    "                        allele_set.add(genotype_val)  # For haploids\n",
    "            return np.array(list(allele_set))\n",
    "\n",
    "        genotype_vals = pd.unique(self.reference_panel.iloc[:, self.ref_sample_value_index - 1:].values.ravel('K'))\n",
    "        print(f\"DATA: Unique genotypes in dataset: {genotype_vals[:10]}...\")  # Show first 10\n",
    "\n",
    "        if self.ref_is_phased and not target_is_gonna_be_phased_or_haps:\n",
    "            phased_to_unphased_dict = {}\n",
    "            for i in range(genotype_vals.shape[0]):\n",
    "                key = genotype_vals[i]\n",
    "                if \"|\" in key and key not in [\".\", \".|.\"]:\n",
    "                    v1, v2 = [int(s) for s in genotype_vals[i].split(original_allele_sep)]\n",
    "                    genotype_vals[i] = f\"{min(v1, v2)}/{max(v1, v2)}\"\n",
    "                    phased_to_unphased_dict[key] = genotype_vals[i]\n",
    "            if phased_to_unphased_dict:\n",
    "                self.reference_panel.iloc[:, self.ref_sample_value_index - 1:].replace(phased_to_unphased_dict,\n",
    "                                                                                       inplace=True)\n",
    "\n",
    "        self.genotype_vals = np.unique(genotype_vals)\n",
    "        self.alleles = get_diploid_alleles(self.genotype_vals) if not self.ref_is_hap else self.genotype_vals\n",
    "        self.allele_count = len(self.alleles)\n",
    "        self.MISSING_VALUE = self.allele_count if self.is_phased else len(self.genotype_vals)\n",
    "\n",
    "        print(f\"DATA: self.genotype_vals: {self.genotype_vals}\")\n",
    "        print(f\"DATA: self.alleles: {self.alleles}\")\n",
    "        print(f\"DATA: is_phased: {self.is_phased}\")\n",
    "\n",
    "        if self.is_phased:\n",
    "            self.hap_map = {str(v): i for i, v in enumerate(list(sorted(self.alleles)))}\n",
    "            self.hap_map.update({\".\": self.MISSING_VALUE})\n",
    "            self.r_hap_map = {i: k for k, i in self.hap_map.items()}\n",
    "            self.map_preds_2_allele = np.vectorize(lambda x: self.r_hap_map[x])\n",
    "            print(f\"DATA: hap_map: {self.hap_map}\")\n",
    "        else:\n",
    "            unphased_missing_genotype = \"./.\"\n",
    "            self.replacement_dict = {g: i for i, g in enumerate(list(sorted(self.genotype_vals)))}\n",
    "            self.replacement_dict[unphased_missing_genotype] = self.MISSING_VALUE\n",
    "            self.reverse_replacement_dict = {v: k for k, v in self.replacement_dict.items()}\n",
    "            print(f\"DATA: replacement_dict: {self.replacement_dict}\")\n",
    "\n",
    "        self.SEQ_DEPTH = self.allele_count + 1 if self.is_phased else len(self.genotype_vals) + 1\n",
    "        print(f\"DATA: self.SEQ_DEPTH: {self.SEQ_DEPTH}\")\n",
    "\n",
    "    def assign_test_set(self, file_path, variants_as_columns=False, delimiter=None,\n",
    "                        file_format=\"infer\", first_column_is_index=True, comments=\"##\") -> None:\n",
    "        \"\"\"Assign test set for imputation\"\"\"\n",
    "        if self.reference_panel is None:\n",
    "            raise RuntimeError(\"First you need to use 'DataReader.assign_training_set(...) to assign a training set.'\")\n",
    "\n",
    "        self.target_file_extension, separator = self.__find_file_extension(file_path, file_format, delimiter)\n",
    "\n",
    "        test_df = self.__read_csv(file_path, is_reference=False, is_vcf=False, separator=separator,\n",
    "                                  first_column_is_index=first_column_is_index,\n",
    "                                  comments=comments) if self.target_file_extension != 'vcf' else self.__read_csv(\n",
    "            file_path, is_reference=False, is_vcf=True, separator='\\t', first_column_is_index=False, comments=\"##\")\n",
    "\n",
    "        if self.target_file_extension != \"vcf\":\n",
    "            if variants_as_columns:\n",
    "                test_df = test_df.transpose()\n",
    "            test_df.reset_index(drop=False, inplace=True)\n",
    "            test_df.rename(columns={test_df.columns[0]: \"ID\"}, inplace=True)\n",
    "        else:\n",
    "            self.target_sample_value_index += 8\n",
    "\n",
    "        self.target_is_hap = not (\"|\" in test_df.iloc[0, self.target_sample_value_index - 1] or \"/\" in\n",
    "                                  test_df.iloc[0, self.target_sample_value_index - 1])\n",
    "        is_phased = \"|\" in test_df.iloc[0, self.target_sample_value_index - 1]\n",
    "        test_var_count = test_df.shape[0]\n",
    "        print(f\"DATA: {test_var_count} {'haplotype' if self.target_is_hap else 'diplotype'} variants found!\")\n",
    "\n",
    "        # Validate compatibility\n",
    "        if (self.target_is_hap or is_phased) and not (self.ref_is_phased or self.ref_is_hap):\n",
    "            raise RuntimeError(\"The training set contains unphased data. The target must be unphased as well.\")\n",
    "        if self.ref_is_hap and not (self.target_is_hap or is_phased):\n",
    "            raise RuntimeError(\"The training set contains haploids. Target set should be phased or haploids.\")\n",
    "\n",
    "        # Merge with reference panel to align variants\n",
    "        self.target_set = test_df.merge(right=self.reference_panel[[\"ID\"]], on='ID', how='right')\n",
    "        if self.target_file_extension == \"vcf\" == self.ref_file_extension:\n",
    "            self.target_set[self.reference_panel.columns[:9]] = self.reference_panel[self.reference_panel.columns[:9]]\n",
    "\n",
    "        self.target_set = self.target_set.astype('str')\n",
    "        missing_value = \".\" if self.target_is_hap else \".|.\" if self.is_phased else \"./.\"\n",
    "        self.target_set.fillna(missing_value, inplace=True)\n",
    "        self.target_set.replace(\"nan\", missing_value, inplace=True)\n",
    "        print(\"DATA: Target set assignment done!\")\n",
    "\n",
    "    def __map_hap_2_ind_parent_1(self, x) -> int:\n",
    "        return self.hap_map[x.split('|')[0]]\n",
    "\n",
    "    def __map_hap_2_ind_parent_2(self, x) -> int:\n",
    "        return self.hap_map[x.split('|')[1]]\n",
    "\n",
    "    def __map_haps_2_ind(self, x) -> int:\n",
    "        return self.hap_map[x]\n",
    "\n",
    "    def get_ref_set(self, starting_var_index=0, ending_var_index=0) -> np.ndarray:\n",
    "        if 0 <= starting_var_index < ending_var_index:\n",
    "            data = self.reference_panel.iloc[starting_var_index:ending_var_index, self.ref_sample_value_index - 1:]\n",
    "        else:\n",
    "            data = self.reference_panel.iloc[:, self.ref_sample_value_index - 1:]\n",
    "\n",
    "        if self.is_phased:\n",
    "            is_haps = \"|\" not in data.iloc[0, 0]\n",
    "            if not is_haps:\n",
    "                # diploids to hap vecs\n",
    "                _x = np.empty((data.shape[1] * 2, data.shape[0]), dtype=np.int32)\n",
    "                _x[0::2] = self.map_values_1_vec(data.values.T)\n",
    "                _x[1::2] = self.map_values_2_vec(data.values.T)\n",
    "                return _x\n",
    "            else:\n",
    "                return self.map_haps_to_vec(data.values.T)\n",
    "        else:\n",
    "            return data.replace(self.replacement_dict).values.T.astype(np.int32)\n",
    "\n",
    "    def get_target_set(self, starting_var_index=0, ending_var_index=0) -> np.ndarray:\n",
    "        \"\"\"Get target data for imputation\"\"\"\n",
    "        if 0 <= starting_var_index < ending_var_index:\n",
    "            data = self.target_set.iloc[starting_var_index:ending_var_index, self.target_sample_value_index - 1:]\n",
    "        else:\n",
    "            data = self.target_set.iloc[:, self.target_sample_value_index - 1:]\n",
    "\n",
    "        if self.is_phased:\n",
    "            is_haps = \"|\" not in data.iloc[0, 0]\n",
    "            if not is_haps:\n",
    "                # diploids to hap vecs\n",
    "                _x = np.empty((data.shape[1] * 2, data.shape[0]), dtype=np.int32)\n",
    "                _x[0::2] = self.map_values_1_vec(data.values.T)\n",
    "                _x[1::2] = self.map_values_2_vec(data.values.T)\n",
    "                return _x\n",
    "            else:\n",
    "                return self.map_haps_to_vec(data.values.T)\n",
    "        else:\n",
    "            return data.replace(self.replacement_dict).values.T.astype(np.int32)\n",
    "\n",
    "    def get_cM(self, start=0, end=None):\n",
    "        _, cM = self.get_gmap(start, end)\n",
    "        return cM\n",
    "    \n",
    "    def get_gmap(self, start=0, end=None):\n",
    "        \"\"\"返回 (rate, cM) 两个向量，与 get_ref_set 同序\"\"\"\n",
    "        if end is None:\n",
    "            end = self.VARIANT_COUNT\n",
    "        ids = self.reference_panel.iloc[start:end][\"ID\"]\n",
    "        rate_vec = np.array([self.id2gmap.get(i, (0., 0.))[0] for i in ids], dtype=np.float32)\n",
    "        cM_vec   = np.array([self.id2gmap.get(i, (0., 0.))[1] for i in ids], dtype=np.float32)\n",
    "        return np.stack([rate_vec, cM_vec], axis=1)   # shape: (n_variants, 2)\n",
    "\n",
    "    def __convert_unphased_probs_to_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        \"\"\"Convert unphased probabilities to genotypes\"\"\"\n",
    "        n_samples, n_variants, n_alleles = allele_probs.shape\n",
    "        genotypes = np.zeros((n_samples, n_variants), dtype=object)\n",
    "\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            for j in range(n_variants):\n",
    "                unphased_probs = allele_probs[i, j]\n",
    "                variant_genotypes = np.vectorize(self.reverse_replacement_dict.get)(\n",
    "                    np.argmax(unphased_probs, axis=-1)).flatten()\n",
    "                genotypes[i, j] = variant_genotypes\n",
    "        return genotypes\n",
    "\n",
    "    def __convert_hap_probs_to_diploid_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        \"\"\"Convert haplotype probabilities to diploid genotypes\"\"\"\n",
    "        n_haploids, n_variants, n_alleles = allele_probs.shape\n",
    "\n",
    "        if n_haploids % 2 != 0:\n",
    "            raise ValueError(\"Number of haploids should be even.\")\n",
    "\n",
    "        n_samples = n_haploids // 2\n",
    "        genotypes = np.empty((n_samples, n_variants), dtype=object)\n",
    "        haploids_as_diploids = allele_probs.reshape((n_samples, 2, n_variants, -1))\n",
    "        variant_genotypes = self.map_preds_2_allele(np.argmax(haploids_as_diploids, axis=-1))\n",
    "\n",
    "        def process_variant_in_sample(haps_for_sample_at_variant, variant_genotypes_for_sample_at_variant):\n",
    "            if n_alleles > 2:\n",
    "                return '|'.join(variant_genotypes_for_sample_at_variant)\n",
    "            else:\n",
    "                # Output GP (genotype probabilities)\n",
    "                phased_probs = np.outer(haps_for_sample_at_variant[0], haps_for_sample_at_variant[1]).flatten()\n",
    "                unphased_probs = np.array([phased_probs[0], phased_probs[1] + phased_probs[2], phased_probs[-1]])\n",
    "                unphased_probs_str = \",\".join([f\"{v:.6f}\" for v in unphased_probs])\n",
    "                alt_dosage = np.dot(unphased_probs, [0, 1, 2])\n",
    "                return '|'.join(variant_genotypes_for_sample_at_variant) + f\":{unphased_probs_str}:{alt_dosage:.3f}\"\n",
    "\n",
    "        def process_sample(i):\n",
    "            return np.array([\n",
    "                process_variant_in_sample(haploids_as_diploids[i, :, j, :], variant_genotypes[i, :, j])\n",
    "                for j in range(n_variants)\n",
    "            ])\n",
    "\n",
    "        # Parallel processing\n",
    "        genotypes = Parallel(n_jobs=-1)(delayed(process_sample)(i) for i in tqdm(range(n_samples)))\n",
    "        return np.array(genotypes)\n",
    "\n",
    "    def __convert_hap_probs_to_hap_genotypes(self, allele_probs) -> np.ndarray:\n",
    "        \"\"\"Convert hap probabilities to hap genotypes\"\"\"\n",
    "        return np.argmax(allele_probs, axis=1).astype(str)\n",
    "\n",
    "    def __get_headers_for_output(self, contain_probs, chr=22):\n",
    "        \"\"\"Get VCF headers for output file\"\"\"\n",
    "        headers = [\n",
    "            \"##fileformat=VCFv4.2\",\n",
    "            '''##source=BiMamba v1.0.0''',\n",
    "            '''##INFO=<ID=AF,Number=A,Type=Float,Description=\"Estimated Alternate Allele Frequency\">''',\n",
    "            '''##INFO=<ID=MAF,Number=1,Type=Float,Description=\"Estimated Minor Allele Frequency\">''',\n",
    "            '''##INFO=<ID=AVG_CS,Number=1,Type=Float,Description=\"Average Call Score\">''',\n",
    "            '''##INFO=<ID=IMPUTED,Number=0,Type=Flag,Description=\"Marker was imputed\">''',\n",
    "            '''##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">''',\n",
    "        ]\n",
    "        probs_headers = [\n",
    "            '''##FORMAT=<ID=DS,Number=A,Type=Float,Description=\"Estimated Alternate Allele Dosage : [P(0/1)+2*P(1/1)]\">''',\n",
    "            '''##FORMAT=<ID=GP,Number=G,Type=Float,Description=\"Estimated Posterior Probabilities for Genotypes 0/0, 0/1 and 1/1\">'''\n",
    "        ]\n",
    "        if contain_probs:\n",
    "            headers.extend(probs_headers)\n",
    "        return headers\n",
    "\n",
    "    def __convert_genotypes_to_vcf(self, genotypes, pred_format=\"GT:GP:DS\"):\n",
    "        \"\"\"Convert genotypes to VCF format\"\"\"\n",
    "        new_vcf = self.target_set.copy()\n",
    "        new_vcf[new_vcf.columns[self.target_sample_value_index - 1:]] = genotypes\n",
    "        new_vcf[\"FORMAT\"] = pred_format\n",
    "        new_vcf[\"QUAL\"] = \".\"\n",
    "        new_vcf[\"FILTER\"] = \".\"\n",
    "        new_vcf[\"INFO\"] = \"IMPUTED\"\n",
    "        return new_vcf\n",
    "\n",
    "    def preds_to_genotypes(self, predictions: Union[str, np.ndarray]) -> pd.DataFrame:\n",
    "        \"\"\"Convert predictions to genotypes\"\"\"\n",
    "        if isinstance(predictions, str):\n",
    "            preds = np.load(predictions)\n",
    "        else:\n",
    "            preds = predictions\n",
    "\n",
    "        target_df = self.target_set.copy()\n",
    "        if not self.is_phased:\n",
    "            target_df[\n",
    "                target_df.columns[self.target_sample_value_index - 1:]] = self.__convert_unphased_probs_to_genotypes(\n",
    "                preds).T\n",
    "        elif self.target_is_hap:\n",
    "            target_df[\n",
    "                target_df.columns[self.target_sample_value_index - 1:]] = self.__convert_hap_probs_to_hap_genotypes(\n",
    "                preds).T\n",
    "        else:\n",
    "            pred_format = \"GT:GP:DS\" if preds.shape[-1] == 2 else \"GT\"\n",
    "            target_df = self.__convert_genotypes_to_vcf(self.__convert_hap_probs_to_diploid_genotypes(preds).T,\n",
    "                                                        pred_format)\n",
    "        return target_df\n",
    "\n",
    "    def write_ligated_results_to_file(self, df: pd.DataFrame, file_name: str, compress=True) -> str:\n",
    "        \"\"\"Write results to file\"\"\"\n",
    "        to_write_format = self.ref_file_extension\n",
    "        file_path = f\"{file_name}.{to_write_format}.gz\" if compress else f\"{file_name}.{to_write_format}\"\n",
    "\n",
    "        with gzip.open(file_path, 'wt') if compress else open(file_path, 'wt') as f_out:\n",
    "            # Write headers\n",
    "            if self.ref_file_extension == \"vcf\":\n",
    "                f_out.write(\n",
    "                    \"\\n\".join(self.__get_headers_for_output(contain_probs=\"GP\" in df[\"FORMAT\"].values[0])) + \"\\n\")\n",
    "            else:\n",
    "                f_out.write(\"\\n\".join(self.ref_n_header_lines))\n",
    "\n",
    "        # Append data\n",
    "        df.to_csv(file_path, sep=self.ref_separator, mode='a', index=False)\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb96506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicDataset(Dataset):\n",
    "    \"\"\"Dataset class for genomic data with masking for training\"\"\"\n",
    "\n",
    "    def __init__(self, data, targets, seq_depth, gmap,\n",
    "                 offset_before=0, offset_after=0,\n",
    "                 training=True, masking_rates=(0.5, 0.99)):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.seq_depth = seq_depth\n",
    "        self.gmap = gmap \n",
    "        self.offset_before = offset_before\n",
    "        self.offset_after = offset_after\n",
    "        self.training = training\n",
    "        self.masking_rates = masking_rates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx].copy()\n",
    "        y = self.targets[idx]\n",
    "\n",
    "        if self.training:\n",
    "            # Apply masking\n",
    "            seq_len = len(x)\n",
    "            masking_rate = np.random.uniform(*self.masking_rates)\n",
    "            mask_size = int(seq_len * masking_rate)\n",
    "            mask_indices = np.random.choice(seq_len, mask_size, replace=False)\n",
    "            x[mask_indices] = self.seq_depth - 1  # Missing value token\n",
    "\n",
    "        # Convert to one-hot\n",
    "        x_onehot = np.eye(self.seq_depth)[x]\n",
    "        y_onehot = np.eye(self.seq_depth - 1)[y]\n",
    "\n",
    "        # 截取与目标一致的坐标段\n",
    "        coord = self.gmap\n",
    "                        \n",
    "        return (torch.FloatTensor(x_onehot),\n",
    "                torch.FloatTensor(coord),\n",
    "                torch.FloatTensor(y_onehot))\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    \"\"\"Dataset for imputation (no masking needed)\"\"\"\n",
    "\n",
    "    def __init__(self, data, seq_depth):\n",
    "        self.data = data\n",
    "        self.seq_depth = seq_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        # Convert to one-hot without masking\n",
    "        x_onehot = np.eye(self.seq_depth)[x]\n",
    "        return torch.FloatTensor(x_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96407836",
   "metadata": {},
   "source": [
    "uni test for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b8331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DataReader()\n",
    "dr.assign_training_set(\n",
    "    file_path=\"/home/qmtang/GitHub/STICI-HPC/data/training_sets/ALL.chr22.training.samples.100k.any.type.0.01.maf.variants.gmap.vcf.gz\",\n",
    "    target_is_gonna_be_phased_or_haps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8733fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (4808, 10)\n",
      "gmap shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "ref_set = dr.get_ref_set(0, 10).astype(np.int32)\n",
    "print(f\"Data shape: {ref_set.shape}\")\n",
    "gmap_chunk = dr.get_gmap(0, 10)\n",
    "print(f\"gmap shape: {gmap_chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86e007",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac21e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiMambaBlock(nn.Module):\n",
    "    \"\"\"Bidirectional Mamba block for genomic sequence processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Forward and backward Mamba blocks\n",
    "        self.mamba_forward = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "        self.mamba_backward = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        residual = x\n",
    "\n",
    "        # Bidirectional processing\n",
    "        x_norm = self.norm1(x)\n",
    "\n",
    "        # Forward direction\n",
    "        forward_out = self.mamba_forward(x_norm)\n",
    "\n",
    "        # Backward direction (flip sequence)\n",
    "        x_backward = torch.flip(x_norm, dims=[1])\n",
    "        backward_out = self.mamba_backward(x_backward)\n",
    "        backward_out = torch.flip(backward_out, dims=[1])\n",
    "\n",
    "        # Concatenate bidirectional outputs\n",
    "        bi_out = torch.cat([forward_out, backward_out], dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        ffn_out = self.ffn(bi_out)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # Residual connection\n",
    "        out = self.norm2(residual + ffn_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block for local pattern extraction\"\"\"\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.conv1 = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)\n",
    "\n",
    "        self.conv_large1 = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)\n",
    "        self.conv_large2 = nn.Conv1d(d_model, d_model, kernel_size=15, padding=7)\n",
    "\n",
    "        self.conv_final = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.conv_reduce = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(d_model)\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2)  # (batch, d_model, seq_len)\n",
    "\n",
    "        xa = self.gelu(self.conv1(x))\n",
    "\n",
    "        xb = self.gelu(self.conv2(xa))\n",
    "        xb = self.gelu(self.conv3(xb))\n",
    "\n",
    "        xc = self.gelu(self.conv_large1(xa))\n",
    "        xc = self.gelu(self.conv_large2(xc))\n",
    "\n",
    "        xa = xb + xc\n",
    "        xa = self.gelu(self.conv_final(xa))\n",
    "        xa = self.bn1(xa)\n",
    "        xa = self.gelu(self.conv_reduce(xa))\n",
    "        xa = self.bn2(xa)\n",
    "        xa = self.gelu(xa)\n",
    "\n",
    "        return xa.transpose(1, 2)  # (batch, seq_len, d_model)\n",
    "\n",
    "class DistanceEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    把 (B,B) 距离矩阵 -> (B, L*, D) 的时序表征\n",
    "    L* = 1 或 L，这里用 1 个 token 代表整张图，可扩展\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len=1, d_model=256, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embed = nn.Linear(1, d_model)   # 把标量距离映成向量\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_dismat):\n",
    "        \"\"\"\n",
    "        x_dismat: (B,B)  对角=0\n",
    "        返回: (B, max_len, D)  这里 max_len=1\n",
    "        \"\"\"\n",
    "        # 取均值池化后作为全局距离向量 → 也可换成 GCN/Transformer 做更复杂编码\n",
    "        z = x_dismat.mean(dim=1, keepdim=True)            # (B,1)\n",
    "        z = z.unsqueeze(1)                         # (B,1,1)\n",
    "        z = self.embed(z)                          # (B,1,D)\n",
    "        z = self.norm(z)\n",
    "        z = self.dropout(z)\n",
    "        return z\n",
    "\n",
    "class Mamba2CrossBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=64,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        headdim=128,\n",
    "        ngroups=1,\n",
    "        chunk_size=256,\n",
    "        dropout=0.0,\n",
    "        d_embed_dropout=0.0,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 距离矩阵嵌入\n",
    "        self.dist_embed = DistanceEmbed(max_len=1, d_model=d_model, dropout=d_embed_dropout)\n",
    "\n",
    "        # 原归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # SSD 核心\n",
    "        self.ssd = Mamba2Block(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            headdim=headdim,\n",
    "            ngroups=ngroups,\n",
    "            chunk_size=chunk_size,\n",
    "            use_mem_eff_path=True,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, local_repr, global_repr,\n",
    "                start_offset=0, end_offset=0,\n",
    "                x_dismat=None):\n",
    "        \"\"\"\n",
    "        local_repr: (B, L, D)\n",
    "        global_repr: (B, G, D)\n",
    "        D: 可选，(B,B) 距离矩阵，对角=0\n",
    "        \"\"\"\n",
    "        local_norm  = self.norm1(local_repr)\n",
    "        global_norm = self.norm2(global_repr)\n",
    "\n",
    "        # 1. 构造输入序列\n",
    "        tokens = []\n",
    "        if x_dismat is not None:\n",
    "            dist_token = self.dist_embed(self.d_model)        # (B,1,D)\n",
    "            tokens.append(dist_token)\n",
    "        tokens.append(global_norm)\n",
    "        tokens.append(local_norm)\n",
    "        x = torch.cat(tokens, dim=1)               # [B, (1)+G+L, D]\n",
    "\n",
    "        # 2. SSD 扫描\n",
    "        x = self.ssd(x)                            # [B, (1)+G+L, D]\n",
    "\n",
    "        # 3. 只取 local 部分\n",
    "        local_len = local_norm.shape[1]\n",
    "        x = x[:, -local_len:, :]                   # [B, L, D]\n",
    "\n",
    "        # 4. pad 回原始长度\n",
    "        if start_offset or end_offset:\n",
    "            x = F.pad(x, (0, 0, start_offset, end_offset))\n",
    "\n",
    "        # 5. 残差 + FFN\n",
    "        x = x + local_norm\n",
    "        x = self.norm3(x)\n",
    "        x = self.ffn(x) + x\n",
    "        return x\n",
    "\n",
    "class GenoEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    融合改进的遗传距离-重组率自适应 RoPE 嵌入\n",
    "    输入：\n",
    "        x:        (B, L, n_alleles)  one-hot\n",
    "        x_coord:  (L, 2)             [r, d]  重组率(cM/Mb) / 遗传距离(cM)\n",
    "    输出：\n",
    "        (B, L, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_alleles: int, d_model: int,\n",
    "                 base_len: float = 8.0,\n",
    "                 alpha_init: float = 0.2):\n",
    "        super().__init__()\n",
    "        assert d_model % 2 == 0\n",
    "        self.d_model, self.n_alleles = d_model, n_alleles\n",
    "\n",
    "        # ---- 1. allele 嵌入----\n",
    "        emb = torch.randn(n_alleles, d_model) * 0.02\n",
    "        self.allele_real = nn.Parameter(emb[:, :d_model//2])\n",
    "        self.allele_imag = nn.Parameter(emb[:, d_model//2:])\n",
    "\n",
    "        # ---- 2. 基础频率----\n",
    "        inv_freq = base_len / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_parameter(\"inv_freq\", nn.Parameter(inv_freq))  # (d//2,)\n",
    "        # ---- 3. 逐维自适应波长----\n",
    "        self.alpha_vec = nn.Parameter(torch.full([d_model//2], alpha_init))\n",
    "        # ---- 4. 重组率门控----\n",
    "        self.beta = nn.Parameter(torch.tensor(2.0))\n",
    "        # ---- 5. 低频阻尼----\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.1))\n",
    "        # ---- 6. 正余弦混叠相位----\n",
    "        self.phi = nn.Parameter(torch.randn(d_model//2) * 0.1)\n",
    "        # ---- 7. 残差温度----\n",
    "        self.lam = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(self, x: Tensor, x_coord: Tensor) -> Tensor:\n",
    "        # ---- allele 复数嵌入 ----\n",
    "        x_real = torch.einsum(\"bln,nd->bld\", x, self.allele_real)   # (B,L,d//2)\n",
    "        x_imag = torch.einsum(\"bln,nd->bld\", x, self.allele_imag)\n",
    "\n",
    "        # ---- 拆分坐标 ----\n",
    "        r, d = x_coord[:, 0], x_coord[:, 1]  # (L,)\n",
    "\n",
    "        # ---- 重组率门控 + 逐维波长 ----\n",
    "        gate = torch.sigmoid(self.beta * r).unsqueeze(1)           # (L,1)\n",
    "        scale = 1.0 + self.alpha_vec.abs().unsqueeze(0) * r.unsqueeze(1)  # (L,d//2)\n",
    "        eff_freq = self.inv_freq.unsqueeze(0) * scale              # (L,d//2)\n",
    "\n",
    "        # ---- 低频阻尼 ----\n",
    "        damp = (-self.gamma * d).unsqueeze(1).exp()                # (L,1)\n",
    "\n",
    "        # ---- 混叠相位 ----\n",
    "        angle = d.unsqueeze(1) * eff_freq * gate * damp + self.phi.unsqueeze(0)  # (L,d//2)\n",
    "\n",
    "        cos, sin = torch.cos(angle), torch.sin(angle)\n",
    "\n",
    "        # ---- 复数旋转 ----\n",
    "        x_real_rot = x_real * cos - x_imag * sin\n",
    "        x_imag_rot = x_real * sin + x_imag * cos\n",
    "\n",
    "        x_rot = torch.cat([x_real_rot, x_imag_rot], dim=-1)        # (B,L,d)\n",
    "\n",
    "        # ---- 残差温度 ----\n",
    "        x_emb = torch.cat([x_real, x_imag], dim=-1)\n",
    "        out = x_rot + self.lam * x_emb\n",
    "        return out\n",
    "\n",
    "class ChunkModule(nn.Module):\n",
    "    \"\"\"Single chunk processing module with BiMamba\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, start_offset=0, end_offset=0, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.start_offset = start_offset\n",
    "        self.end_offset = end_offset\n",
    "\n",
    "        # BiMamba block\n",
    "        self.bimamba_block = BiMambaBlock(d_model)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv_block1 = ConvBlock(d_model)\n",
    "        self.conv_block2 = ConvBlock(d_model)\n",
    "        self.conv_block3 = ConvBlock(d_model)\n",
    "\n",
    "        # Cross attention\n",
    "        # self.cross_attention = CrossAttentionLayer(d_model, n_heads)\n",
    "        self.cross_attention = Mamba2CrossBlock(\n",
    "            d_model=d_model,\n",
    "            d_state=64,\n",
    "            d_conv=4,\n",
    "            expand=2,\n",
    "            headdim=128,\n",
    "            ngroups=1,\n",
    "            chunk_size=256,\n",
    "        )\n",
    "\n",
    "        # Additional layers\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x, x_dismat=None):\n",
    "        # BiMamba processing\n",
    "        xa0 = self.bimamba_block(x)\n",
    "\n",
    "        # First conv block\n",
    "        xa = self.conv_block1(xa0)\n",
    "        xa_skip = self.conv_block2(xa)\n",
    "\n",
    "        # Dense layer\n",
    "        xa = self.gelu(self.dense(xa))\n",
    "        xa = self.conv_block2(xa)\n",
    "\n",
    "        # Cross attention\n",
    "        xa = self.cross_attention(xa, xa0, self.start_offset, self.end_offset, x_dismat)\n",
    "        xa = self.dropout(xa)\n",
    "\n",
    "        # Final conv block\n",
    "        xa = self.conv_block3(xa)\n",
    "\n",
    "        # Concatenate with skip connection\n",
    "        xa = torch.cat([xa_skip, xa], dim=-1)\n",
    "\n",
    "        return xa\n",
    "\n",
    "class EvoFill(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 n_alleles,\n",
    "                 coord_dim = 1,\n",
    "                 chunk_size=2048,\n",
    "                 chunk_overlap=64,\n",
    "                 offset_before=0,\n",
    "                 offset_after=0,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.offset_before = offset_before\n",
    "        self.offset_after = offset_after\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.n_alleles = n_alleles\n",
    "        self.coord_dim = coord_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = GenoEmbedding(n_alleles, self.d_model, self.coord_dim)\n",
    "\n",
    "        # Create chunk modules\n",
    "        self.chunk_module = ChunkModule(\n",
    "            d_model=self.d_model,\n",
    "            start_offset=0,\n",
    "            end_offset=0,\n",
    "            dropout_rate=self.dropout_rate\n",
    "        )\n",
    "\n",
    "        # Final layers\n",
    "        self.final_conv = nn.Conv1d(self.d_model * 2, self.d_model // 2,\n",
    "                                    kernel_size=5, padding=2)\n",
    "        self.output_conv = nn.Conv1d(self.d_model // 2, n_alleles - 1,\n",
    "                                     kernel_size=5, padding=2)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, x_coord, x_dismat=None):\n",
    "        # x shape: (batch, seq_len, n_alleles)\n",
    "        # Embedding\n",
    "        _, seq_len, n_alleles = x.shape\n",
    "        assert n_alleles == self.n_alleles\n",
    "        x_embedded = self.embedding(x, x_coord)\n",
    "\n",
    "        chunk_starts = list(range(0, seq_len, self.chunk_size))\n",
    "        chunk_ends = [min(cs + self.chunk_size, seq_len) for cs in chunk_starts]\n",
    "        mask_starts = [max(0, cs - self.chunk_overlap) for cs in chunk_starts]\n",
    "        mask_ends = [min(ce + self.chunk_overlap, seq_len) for ce in chunk_ends]\n",
    "\n",
    "        # Process chunks\n",
    "        chunk_outputs = []\n",
    "        for i in range(len(chunk_starts)):\n",
    "            pad_left  = chunk_starts[i] - mask_starts[i]\n",
    "            pad_right = mask_ends[i] - chunk_ends[i]\n",
    "            chunk_input = x_embedded[:, mask_starts[i]:mask_ends[i]]\n",
    "            chunk_output = self.chunk_module(chunk_input, x_dismat)   # 共享权重\n",
    "            if pad_left or pad_right:\n",
    "                chunk_output = F.pad(chunk_output, (0, 0, pad_left, pad_right))\n",
    "            chunk_outputs.append(chunk_output)\n",
    "        # Concatenate chunks along sequence dimension\n",
    "        x_concat = torch.cat(chunk_outputs, dim=1)\n",
    "\n",
    "        # # Final processing\n",
    "        x_concat = x_concat.transpose(1, 2)  # (batch, features, seq_len)\n",
    "        x_final = self.gelu(self.final_conv(x_concat))\n",
    "        x_output = self.output_conv(x_final)\n",
    "        x_output = x_output.transpose(1, 2)  # (batch, seq_len, n_alleles-1)\n",
    "\n",
    "        # Apply offsets\n",
    "        if self.offset_before > 0 or self.offset_after > 0:\n",
    "            x_output = x_output[:, self.offset_before:seq_len - self.offset_after]\n",
    "\n",
    "        x_output = self.softmax(x_output)\n",
    "\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f053a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入x 形状: torch.Size([2, 5120])\n",
      "输入x_coord 形状: torch.Size([5120, 2])\n",
      "前向通过，输出形状: torch.Size([2, 5120, 3])\n"
     ]
    }
   ],
   "source": [
    "n_alleles = 4  # 包含missing\n",
    "model = EvoFill(\n",
    "    d_model=256,\n",
    "    chunk_size=5120,\n",
    "    n_alleles=n_alleles,\n",
    "    chunk_overlap=64, \n",
    "    offset_before=0,\n",
    "    offset_after=0,\n",
    "    dropout_rate=0.1,\n",
    ").cuda()\n",
    "\n",
    "B, L = 2, 5120 \n",
    "\n",
    "# 1. 生成输入\n",
    "x = torch.randint(0, n_alleles, (B, L)).long().cuda()      # {0,1,2,3} 3=missing\n",
    "x_coord = torch.rand(L, 2).cuda()\n",
    "x_coord[:, 0].uniform_(0, 15)   # r  0~15 cM/Mb\n",
    "x_coord[:, 1].uniform_(0, 2)    # d  0~2 cM\n",
    "x_coord = x_coord.cuda()\n",
    "\n",
    "# 2. -1 -> 3，并构造 one-hot（4 维）\n",
    "x_map = x.clone()\n",
    "x_onehot = torch.zeros(B, L, n_alleles, device='cuda')\n",
    "x_onehot.scatter_(2, x_map.unsqueeze(-1), 1)\n",
    "\n",
    "# 3. 前向\n",
    "with torch.no_grad():\n",
    "    probs = model(x_onehot, x_coord)          # shape: (B,L,3)\n",
    "\n",
    "# 4. 简单校验\n",
    "assert torch.allclose(probs.sum(dim=-1), torch.ones(B, L, device='cuda'), atol=1e-5), \\\n",
    "    \"概率未归一\"\n",
    "print(\"输入x 形状:\", x.shape)\n",
    "print(\"输入x_coord 形状:\", x_coord.shape)\n",
    "print(\"前向通过，输出形状:\", probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8c794",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6f2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputationLoss(nn.Module):\n",
    "    \"\"\"Custom loss function for genomic imputation\"\"\"\n",
    "\n",
    "    def __init__(self, use_r2=True, ):\n",
    "        super().__init__()\n",
    "        self.use_r2_loss = use_r2\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "    def calculate_minimac_r2(self, pred_alt_allele_probs, gt_alt_af):\n",
    "        \"\"\"Calculate Minimac-style RÂ² metric\"\"\"\n",
    "        mask = torch.logical_or(torch.eq(gt_alt_af, 0.0), torch.eq(gt_alt_af, 1.0))\n",
    "        gt_alt_af = torch.where(mask, 0.5, gt_alt_af)\n",
    "        denom = gt_alt_af * (1.0 - gt_alt_af)\n",
    "        denom = torch.where(denom < 0.01, 0.01, denom)\n",
    "        r2 = torch.mean(torch.square(pred_alt_allele_probs - gt_alt_af), dim=0) / denom\n",
    "        r2 = torch.where(mask, torch.zeros_like(r2), r2)\n",
    "        return r2\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        # Convert to proper format for losses\n",
    "        y_true_ce = torch.argmax(y_true, dim=-1)  # For CrossEntropy\n",
    "        y_pred_log = torch.log(y_pred + 1e-8)  # For KL divergence\n",
    "\n",
    "        # Basic losses\n",
    "        ce_loss = self.ce_loss(y_pred.view(-1, y_pred.size(-1)), y_true_ce.view(-1))\n",
    "        kl_loss = self.kl_loss(y_pred_log.view(-1, y_pred.size(-1)),\n",
    "                               y_true.view(-1, y_true.size(-1)))\n",
    "\n",
    "        total_loss = ce_loss + kl_loss\n",
    "\n",
    "        if self.use_r2_loss:\n",
    "            batch_size = y_true.size(0)\n",
    "            group_size = 4\n",
    "            num_full_groups = batch_size // group_size\n",
    "\n",
    "            if num_full_groups > 0:\n",
    "                y_true_grouped = y_true[:num_full_groups * group_size].view(\n",
    "                    num_full_groups, group_size, *y_true.shape[1:])\n",
    "                y_pred_grouped = y_pred[:num_full_groups * group_size].view(\n",
    "                    num_full_groups, group_size, *y_pred.shape[1:])\n",
    "\n",
    "                r2_loss = 0.0\n",
    "                for i in range(num_full_groups):\n",
    "                    gt_alt_af = torch.count_nonzero(\n",
    "                        torch.argmax(y_true_grouped[i], dim=-1), dim=0\n",
    "                    ).float() / group_size\n",
    "\n",
    "                    pred_alt_allele_probs = torch.sum(y_pred_grouped[i][:, :, 1:], dim=-1)\n",
    "                    r2_loss += -torch.sum(self.calculate_minimac_r2(\n",
    "                        pred_alt_allele_probs, gt_alt_af)) * group_size\n",
    "\n",
    "                total_loss += r2_loss\n",
    "\n",
    "        return total_loss, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0ff63",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb648d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_similar_rows(array):\n",
    "    \"\"\"Remove duplicate haploids from training set\"\"\"\n",
    "    print(\"Finding any duplicate haploids in training set.\")\n",
    "    unique_array = np.unique(array, axis=0)\n",
    "    print(f\"Removed {len(array) - len(unique_array)} rows. {len(unique_array)} training samples remaining.\")\n",
    "    return unique_array\n",
    "\n",
    "def create_directories(save_dir, models_dir=\"models\", outputs=\"out\") -> None:\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    for dd in [save_dir, f\"{save_dir}/{models_dir}\", f\"{save_dir}/{outputs}\"]:\n",
    "        if not os.path.exists(dd):\n",
    "            os.makedirs(dd)\n",
    "\n",
    "def clear_dir(path) -> None:\n",
    "    \"\"\"Clear directory if it exists\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "def load_chunk_info(save_dir, break_points):\n",
    "    \"\"\"Load chunk training status information\"\"\"\n",
    "    chunk_info = {ww: False for ww in list(range(len(break_points) - 1))}\n",
    "    if os.path.isfile(f\"{save_dir}/models/chunks_info.json\"):\n",
    "        with open(f\"{save_dir}/models/chunks_info.json\", 'r') as f:\n",
    "            loaded_chunks_info = json.load(f)\n",
    "            if isinstance(loaded_chunks_info, dict) and len(loaded_chunks_info) == len(chunk_info):\n",
    "                print(\"Resuming the training...\")\n",
    "                chunk_info = {int(k): v for k, v in loaded_chunks_info.items()}\n",
    "    return chunk_info\n",
    "\n",
    "def save_chunk_status(save_dir, chunk_info) -> None:\n",
    "    \"\"\"Save chunk training status information\"\"\"\n",
    "    with open(f\"{save_dir}/models/chunks_info.json\", \"w\") as outfile:\n",
    "        json.dump(chunk_info, outfile)\n",
    "\n",
    "def create_model(args, n_alleles):\n",
    "    \"\"\"Create BiMamba model\"\"\"\n",
    "    model = EvoFill(\n",
    "        d_model=args.embed_dim,\n",
    "        n_alleles=n_alleles,\n",
    "        chunk_size=args.cs,\n",
    "        chunk_overlap=args.co,\n",
    "        offset_before=getattr(args, 'offset_before', 0),\n",
    "        offset_after=getattr(args, 'offset_after', 0),\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c302bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAF_BINS = [(0.00, 0.05), (0.05, 0.10), (0.10, 0.20),\n",
    "            (0.20, 0.30), (0.30, 0.40), (0.40, 0.50)]\n",
    "\n",
    "def precompute_maf(gts_np, mask_int=-1):\n",
    "    \"\"\"\n",
    "    gts_np: (N, L)  int64\n",
    "    return:\n",
    "        maf: (L,) float32\n",
    "        bin_cnt: list[int] 长度 6，对应 6 个 bin 的位点数量\n",
    "    \"\"\"\n",
    "    L = gts_np.shape[1]\n",
    "    maf = np.zeros(L, dtype=np.float32)\n",
    "    bin_cnt = [0] * 6\n",
    "\n",
    "    for l in range(L):\n",
    "        alleles = gts_np[:, l]\n",
    "        alleles = alleles[alleles != mask_int]   # 去掉缺失\n",
    "        if alleles.size == 0:\n",
    "            maf[l] = 0.0\n",
    "            continue\n",
    "\n",
    "        uniq, cnt = np.unique(alleles, return_counts=True)\n",
    "        total = cnt.sum()\n",
    "        freq = cnt / total\n",
    "        freq[::-1].sort()\n",
    "        maf_val = freq[1] if len(freq) > 1 else 0.0\n",
    "        maf[l] = maf_val\n",
    "\n",
    "        # 统计 bin\n",
    "        for i, (lo, hi) in enumerate(MAF_BINS):\n",
    "            if lo <= maf_val < hi:\n",
    "                bin_cnt[i] += 1\n",
    "                break\n",
    "\n",
    "    return maf, bin_cnt\n",
    "\n",
    "def imputation_maf_accuracy_epoch(all_logits, all_gts, global_maf, mask=None):\n",
    "    \"\"\"\n",
    "    all_logits: (N, L, C)\n",
    "    all_gts:    (N, L, C) one-hot\n",
    "    global_maf: (L,)\n",
    "    mask:       (N, L) 或 None\n",
    "    return:     list[float] 长度 6\n",
    "    \"\"\"\n",
    "    # 1. 预测 vs 真实\n",
    "    all_gts = all_gts.argmax(dim=-1)      # (N, L)\n",
    "    preds   = all_logits.argmax(dim=-1)   # (N, L)\n",
    "\n",
    "    # 2. 如果没有外部 mask，就默认全 1\n",
    "    if mask is None:\n",
    "        mask = torch.ones_like(all_gts, dtype=torch.bool)   # (N, L)\n",
    "    correct = (preds == all_gts) & mask                   # (N, L)\n",
    "\n",
    "    # 3. MAF 条件 -> (1, L) 再广播到 (N, L)\n",
    "    maf = global_maf.unsqueeze(0)                         # (1, L)\n",
    "\n",
    "    # 4. 分 bin 计算\n",
    "    accs = []\n",
    "    for lo, hi in MAF_BINS:\n",
    "        maf_bin = mask & (maf >= lo) & (maf < hi)                # (1, L)\n",
    "        n_cor = (correct & maf_bin).sum()\n",
    "        n_tot = maf_bin.sum()\n",
    "        accs.append(100*(n_cor / n_tot).item() if n_tot > 0 else 0.0)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d21fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DATA: Reading the file...\n",
      "DATA: ref gmap range: rate 0.001-79.577 cM/Mb, cM 0.001-39.984 cM\n",
      "DATA: 2404 diploid samples with 99314 variants found!\n",
      "DATA: Unique genotypes in dataset: ['0|0' '0|1' '1|0' '1|1']...\n",
      "DATA: self.genotype_vals: ['0|0' '0|1' '1|0' '1|1']\n",
      "DATA: self.alleles: ['0' '1']\n",
      "DATA: is_phased: True\n",
      "DATA: hap_map: {'0': 0, '1': 1, '.': 2}\n",
      "DATA: self.SEQ_DEPTH: 3\n",
      "Chunk  1:  [     0,  65536)  len= 65536  |  todo\n",
      "Chunk  2:  [ 33778,  99314)  len= 65536  |  todo\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 以下即命令行参数对应的行内变量 ----------------\n",
    "mode                 = 'train'\n",
    "restart_training     = True          # 对应命令行 1\n",
    "ref                  = \"/home/qmtang/GitHub/STICI-HPC/data/training_sets/ALL.chr22.training.samples.100k.any.type.0.01.maf.variants.gmap.vcf.gz\"\n",
    "tihp                 = True          # 对应命令行 1\n",
    "which_chunk          = -1            # All chunkss\n",
    "save_dir             = '/home/qmtang/mnt_qmtang/EvoFill/data/251017_ver2_chr22'\n",
    "co                   = 64            # 64 in STICI\n",
    "cs                   = 8192          # 2048 in STICI\n",
    "sites_per_model      = 65536         # 10240 in STICI\n",
    "max_mr               = 0.7\n",
    "min_mr               = 0.3\n",
    "epochs               = 100\n",
    "embed_dim            = 128            # 128 in STICI\n",
    "lr                   = 0.001\n",
    "weight_decay         = 1e-5\n",
    "batch_size_per_gpu   = 8\n",
    "use_r2               = True\n",
    "earlystop_patience   = 9\n",
    "verbose              = 1\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 组装成 Namespace\n",
    "args = Namespace(\n",
    "    restart_training=restart_training,\n",
    "    ref=ref,\n",
    "    tihp=tihp,\n",
    "    which_chunk=which_chunk,\n",
    "    save_dir=save_dir,\n",
    "    co=co,\n",
    "    cs=cs,\n",
    "    sites_per_model=sites_per_model,\n",
    "    max_mr=max_mr,\n",
    "    min_mr=min_mr,\n",
    "    epochs=epochs,\n",
    "    embed_dim=embed_dim,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    batch_size_per_gpu=batch_size_per_gpu,\n",
    "    use_r2=use_r2,\n",
    "    earlystop_patience=earlystop_patience,\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "assert args.max_mr > 0\n",
    "assert args.min_mr > 0\n",
    "assert args.max_mr >= args.min_mr\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "create_directories(args.save_dir)\n",
    "with open(f\"{args.save_dir}/commandline_args.json\", 'w') as f:\n",
    "    json.dump(vars(args), f, indent=4)\n",
    "\n",
    "# Load data\n",
    "dr = DataReader()\n",
    "dr.assign_training_set(\n",
    "    file_path=args.ref,\n",
    "    target_is_gonna_be_phased_or_haps=args.tihp,\n",
    "    variants_as_columns=getattr(args, 'ref_vac', False),\n",
    "    delimiter=getattr(args, 'ref_sep', None),\n",
    "    file_format=getattr(args, 'ref_file_format', 'infer'),\n",
    "    first_column_is_index=getattr(args, 'ref_fcai', True),\n",
    "    comments=getattr(args, 'ref_comment', '##')\n",
    ")\n",
    "\n",
    "# Split data for validation\n",
    "n_samples = dr.get_ref_set(0, 1).shape[0]\n",
    "val_n_samples = args.batch_size_per_gpu * getattr(args, 'val_n_batches', 8)\n",
    "x_train_indices, x_valid_indices = train_test_split(\n",
    "    range(n_samples),\n",
    "    test_size=val_n_samples,\n",
    "    random_state=getattr(args, 'random_seed', 3047),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Process chunks\n",
    "def make_chunks(n_tot: int, sites: int, overlap: int):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i + sites < n_tot:\n",
    "        chunks.append((i, i + sites))\n",
    "        i += sites - overlap\n",
    "    # 尾部：以末尾为右端点向左取 sites\n",
    "    last_start = max(0, n_tot - sites)\n",
    "    chunks.append((last_start, n_tot))\n",
    "    return chunks\n",
    "    \n",
    "chunks = make_chunks(dr.VARIANT_COUNT, args.sites_per_model, args.co)\n",
    "break_points = [c[0] for c in chunks] + [chunks[-1][1]]\n",
    "chunks_done = load_chunk_info(args.save_dir, break_points)\n",
    "\n",
    "for w, (s, e) in enumerate(chunks, 1):\n",
    "    status = \"done\" if chunks_done[w-1] else \"todo\"\n",
    "    print(f\"Chunk {w:2d}:  [{s:6d}, {e:6d})  \"\n",
    "          f\"len={e-s:6d}  |  {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f31ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on chunk 1/2\n",
      "Data Chunk Shape: (4808, 65536), gmap shape: (65536, 2)\n",
      "  effective train slice = 0:65408\n",
      "Finding any duplicate haploids in training set.\n",
      "Removed 0 rows. 4744 training samples remaining.\n",
      "Chunk MAF-bin counts: [26813, 9287, 10386, 6884, 6047, 5987]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 1/100, Train Loss: 401716.7062, Val Loss: 407415.7109\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 96.30 96.77\n",
      "(0.05, 0.10)   9287 87.33 88.08\n",
      "(0.10, 0.20)  10386 74.67 75.64\n",
      "(0.20, 0.30)   6884 62.54 63.37\n",
      "(0.30, 0.40)   6047 55.84 56.85\n",
      "(0.40, 0.50)   5987 51.37 52.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 2/100, Train Loss: 359559.3365, Val Loss: 357668.5391\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 96.37 96.61\n",
      "(0.05, 0.10)   9287 88.48 90.26\n",
      "(0.10, 0.20)  10386 77.55 80.59\n",
      "(0.20, 0.30)   6884 65.76 69.29\n",
      "(0.30, 0.40)   6047 59.18 63.63\n",
      "(0.40, 0.50)   5987 55.74 61.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 3/100, Train Loss: 285337.0494, Val Loss: 329532.5156\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 96.91 96.68\n",
      "(0.05, 0.10)   9287 91.08 90.85\n",
      "(0.10, 0.20)  10386 83.44 82.01\n",
      "(0.20, 0.30)   6884 74.02 72.04\n",
      "(0.30, 0.40)   6047 69.93 67.57\n",
      "(0.40, 0.50)   5987 67.94 65.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 4/100, Train Loss: 255041.6776, Val Loss: 317144.1602\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.11 96.44\n",
      "(0.05, 0.10)   9287 91.84 90.73\n",
      "(0.10, 0.20)  10386 85.24 82.34\n",
      "(0.20, 0.30)   6884 76.94 73.38\n",
      "(0.30, 0.40)   6047 74.01 69.86\n",
      "(0.40, 0.50)   5987 72.46 68.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 5/100, Train Loss: 238322.3775, Val Loss: 311378.2656\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.26 96.43\n",
      "(0.05, 0.10)   9287 92.33 90.66\n",
      "(0.10, 0.20)  10386 86.28 82.80\n",
      "(0.20, 0.30)   6884 78.55 73.61\n",
      "(0.30, 0.40)   6047 76.20 70.46\n",
      "(0.40, 0.50)   5987 74.59 69.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 6/100, Train Loss: 227009.5166, Val Loss: 322000.7070\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.32 96.74\n",
      "(0.05, 0.10)   9287 92.59 90.75\n",
      "(0.10, 0.20)  10386 86.96 82.21\n",
      "(0.20, 0.30)   6884 79.66 72.49\n",
      "(0.30, 0.40)   6047 77.58 69.53\n",
      "(0.40, 0.50)   5987 75.93 67.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 7/100, Train Loss: 218845.2755, Val Loss: 316503.3594\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.39 96.72\n",
      "(0.05, 0.10)   9287 92.80 90.60\n",
      "(0.10, 0.20)  10386 87.47 82.39\n",
      "(0.20, 0.30)   6884 80.44 72.94\n",
      "(0.30, 0.40)   6047 78.51 69.85\n",
      "(0.40, 0.50)   5987 76.87 67.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 8/100, Train Loss: 212144.8098, Val Loss: 313085.3125\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.42 96.28\n",
      "(0.05, 0.10)   9287 92.96 90.50\n",
      "(0.10, 0.20)  10386 87.84 82.90\n",
      "(0.20, 0.30)   6884 81.07 73.89\n",
      "(0.30, 0.40)   6047 79.24 71.14\n",
      "(0.40, 0.50)   5987 77.63 68.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 9/100, Train Loss: 207706.4879, Val Loss: 317981.6367\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.43 96.79\n",
      "(0.05, 0.10)   9287 93.05 90.67\n",
      "(0.10, 0.20)  10386 88.09 82.35\n",
      "(0.20, 0.30)   6884 81.55 72.88\n",
      "(0.30, 0.40)   6047 79.76 70.01\n",
      "(0.40, 0.50)   5987 78.16 67.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 10/100, Train Loss: 193908.8194, Val Loss: 315222.9961\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.55 96.66\n",
      "(0.05, 0.10)   9287 93.43 90.54\n",
      "(0.10, 0.20)  10386 88.83 82.56\n",
      "(0.20, 0.30)   6884 82.77 73.38\n",
      "(0.30, 0.40)   6047 81.18 70.68\n",
      "(0.40, 0.50)   5987 79.68 68.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 11/100, Train Loss: 190465.2570, Val Loss: 320192.1406\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.57 96.79\n",
      "(0.05, 0.10)   9287 93.47 90.66\n",
      "(0.10, 0.20)  10386 88.98 82.29\n",
      "(0.20, 0.30)   6884 83.06 73.01\n",
      "(0.30, 0.40)   6047 81.51 69.78\n",
      "(0.40, 0.50)   5987 80.05 67.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 12/100, Train Loss: 187909.2045, Val Loss: 329008.9492\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.58 96.70\n",
      "(0.05, 0.10)   9287 93.52 90.60\n",
      "(0.10, 0.20)  10386 89.11 81.93\n",
      "(0.20, 0.30)   6884 83.28 72.57\n",
      "(0.30, 0.40)   6047 81.75 69.16\n",
      "(0.40, 0.50)   5987 80.38 66.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 13/100, Train Loss: 185562.6412, Val Loss: 310096.9922\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.59 96.51\n",
      "(0.05, 0.10)   9287 93.57 90.54\n",
      "(0.10, 0.20)  10386 89.23 82.78\n",
      "(0.20, 0.30)   6884 83.50 74.00\n",
      "(0.30, 0.40)   6047 81.98 71.27\n",
      "(0.40, 0.50)   5987 80.67 68.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 14/100, Train Loss: 183214.3679, Val Loss: 327149.6562\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.60 96.65\n",
      "(0.05, 0.10)   9287 93.64 90.51\n",
      "(0.10, 0.20)  10386 89.34 82.27\n",
      "(0.20, 0.30)   6884 83.70 72.96\n",
      "(0.30, 0.40)   6047 82.25 69.95\n",
      "(0.40, 0.50)   5987 80.95 66.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 15/100, Train Loss: 181494.8748, Val Loss: 322226.7617\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.61 96.59\n",
      "(0.05, 0.10)   9287 93.68 90.26\n",
      "(0.10, 0.20)  10386 89.43 82.22\n",
      "(0.20, 0.30)   6884 83.86 73.12\n",
      "(0.30, 0.40)   6047 82.39 69.96\n",
      "(0.40, 0.50)   5987 81.13 67.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 16/100, Train Loss: 179726.7330, Val Loss: 319214.5859\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.62 96.49\n",
      "(0.05, 0.10)   9287 93.72 90.51\n",
      "(0.10, 0.20)  10386 89.50 82.47\n",
      "(0.20, 0.30)   6884 84.07 73.32\n",
      "(0.30, 0.40)   6047 82.61 70.65\n",
      "(0.40, 0.50)   5987 81.34 67.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 17/100, Train Loss: 178100.8013, Val Loss: 318138.7539\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.63 96.74\n",
      "(0.05, 0.10)   9287 93.75 90.65\n",
      "(0.10, 0.20)  10386 89.60 82.51\n",
      "(0.20, 0.30)   6884 84.22 73.39\n",
      "(0.30, 0.40)   6047 82.72 70.26\n",
      "(0.40, 0.50)   5987 81.48 67.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 18/100, Train Loss: 170799.2456, Val Loss: 329141.2930\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.68 96.55\n",
      "(0.05, 0.10)   9287 93.93 90.40\n",
      "(0.10, 0.20)  10386 89.94 81.89\n",
      "(0.20, 0.30)   6884 84.81 72.72\n",
      "(0.30, 0.40)   6047 83.45 69.34\n",
      "(0.40, 0.50)   5987 82.18 66.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 19/100, Train Loss: 168872.6794, Val Loss: 326441.8516\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.70 96.55\n",
      "(0.05, 0.10)   9287 93.99 90.36\n",
      "(0.10, 0.20)  10386 90.02 82.15\n",
      "(0.20, 0.30)   6884 84.98 73.12\n",
      "(0.30, 0.40)   6047 83.64 70.07\n",
      "(0.40, 0.50)   5987 82.41 67.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 20/100, Train Loss: 167897.2243, Val Loss: 321475.6211\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.69 96.52\n",
      "(0.05, 0.10)   9287 93.98 90.57\n",
      "(0.10, 0.20)  10386 90.04 82.33\n",
      "(0.20, 0.30)   6884 85.03 73.37\n",
      "(0.30, 0.40)   6047 83.69 70.64\n",
      "(0.40, 0.50)   5987 82.48 67.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Epoch 21/100, Train Loss: 166944.9002, Val Loss: 336305.6328\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)  26813 97.69 96.48\n",
      "(0.05, 0.10)   9287 94.00 90.29\n",
      "(0.10, 0.20)  10386 90.08 81.81\n",
      "(0.20, 0.30)   6884 85.11 72.55\n",
      "(0.30, 0.40)   6047 83.83 69.37\n",
      "(0.40, 0.50)   5987 82.62 66.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[11], line 93\u001b[0m\n",
      "\u001b[1;32m     90\u001b[0m x_onehot, x_coord, target \u001b[38;5;241m=\u001b[39m x_onehot\u001b[38;5;241m.\u001b[39mto(device), x_coord\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;32m---> 93\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_coord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m loss, logs \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "\u001b[1;32m     95\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "Cell \u001b[0;32mIn[4], line 400\u001b[0m, in \u001b[0;36mEvoFill.forward\u001b[0;34m(self, x, x_coord, x_dismat)\u001b[0m\n",
      "\u001b[1;32m    398\u001b[0m pad_right \u001b[38;5;241m=\u001b[39m mask_ends[i] \u001b[38;5;241m-\u001b[39m chunk_ends[i]\n",
      "\u001b[1;32m    399\u001b[0m chunk_input \u001b[38;5;241m=\u001b[39m x_embedded[:, mask_starts[i]:mask_ends[i]]\n",
      "\u001b[0;32m--> 400\u001b[0m chunk_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dismat\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# 共享权重\u001b[39;00m\n",
      "\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad_left \u001b[38;5;129;01mor\u001b[39;00m pad_right:\n",
      "\u001b[1;32m    402\u001b[0m     chunk_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(chunk_output, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, pad_left, pad_right))\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "Cell \u001b[0;32mIn[4], line 320\u001b[0m, in \u001b[0;36mChunkModule.forward\u001b[0;34m(self, x, x_dismat)\u001b[0m\n",
      "\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_dismat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# BiMamba processing\u001b[39;00m\n",
      "\u001b[0;32m--> 320\u001b[0m     xa0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbimamba_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# First conv block\u001b[39;00m\n",
      "\u001b[1;32m    323\u001b[0m     xa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block1(xa0)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mBiMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[1;32m     42\u001b[0m x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Forward direction\u001b[39;00m\n",
      "\u001b[0;32m---> 45\u001b[0m forward_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmamba_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Backward direction (flip sequence)\u001b[39;00m\n",
      "\u001b[1;32m     48\u001b[0m x_backward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflip(x_norm, dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m])\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/modules/mamba2.py:185\u001b[0m, in \u001b[0;36mMamba2.forward\u001b[0;34m(self, u, seqlen, seq_idx, cu_seqlens, inference_params)\u001b[0m\n",
      "\u001b[1;32m    183\u001b[0m dt_limit_kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_limit \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(dt_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_limit)\n",
      "\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_mem_eff_path \u001b[38;5;129;01mand\u001b[39;00m inference_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 185\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_split_conv1d_scan_combined\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzxbcdt\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md 1 w -> d w\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_bias\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(h p) -> h p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaddim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_has_hdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrmsnorm_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmsnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrmsnorm_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmsnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutproj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutproj_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaddim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_has_hdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaddim\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_before_gate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_before_gate\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdt_limit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seqlen_og \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    205\u001b[0m         out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb l d -> (b l) d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:947\u001b[0m, in \u001b[0;36mmamba_split_conv1d_scan_combined\u001b[0;34m(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)\u001b[0m\n",
      "\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmamba_split_conv1d_scan_combined\u001b[39m(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, seq_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dt_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)), return_final_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m, rmsnorm_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rmsnorm_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, outproj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, outproj_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ngroups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, norm_before_gate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;32m    929\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m    Argument:\u001b[39;00m\n",
      "\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m        zxbcdt: (batch, seqlen, 2 * dim + 2 * ngroups * dstate + nheads) where dim == nheads * headdim\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m        out: (batch, seqlen, dim)\u001b[39;00m\n",
      "\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMambaSplitConv1dScanCombinedFn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzxbcdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_final_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrmsnorm_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrmsnorm_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutproj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutproj_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_before_gate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/autograd/function.py:576\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n",
      "\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n",
      "\u001b[1;32m    575\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n",
      "\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n",
      "\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    584\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/amp/autocast_mode.py:517\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    516\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n",
      "\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    519\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:801\u001b[0m, in \u001b[0;36mMambaSplitConv1dScanCombinedFn.forward\u001b[0;34m(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)\u001b[0m\n",
      "\u001b[1;32m    799\u001b[0m C \u001b[38;5;241m=\u001b[39m rearrange(C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb l (g n) -> b l g n\u001b[39m\u001b[38;5;124m\"\u001b[39m, g\u001b[38;5;241m=\u001b[39mngroups)\n",
      "\u001b[1;32m    800\u001b[0m z \u001b[38;5;241m=\u001b[39m rearrange(z, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb l (h p) -> b l h p\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39mnheads) \u001b[38;5;28;01mif\u001b[39;00m z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m--> 801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rmsnorm_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    802\u001b[0m     out, out_x, dt_out, dA_cumsum, states, final_states \u001b[38;5;241m=\u001b[39m _mamba_chunk_scan_combined_fwd(x, dt, A, B, C, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, D\u001b[38;5;241m=\u001b[39mD, z\u001b[38;5;241m=\u001b[39mz, dt_bias\u001b[38;5;241m=\u001b[39mdt_bias, initial_states\u001b[38;5;241m=\u001b[39minitial_states, seq_idx\u001b[38;5;241m=\u001b[39mseq_idx, dt_softplus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dt_limit\u001b[38;5;241m=\u001b[39mdt_limit)\n",
      "\u001b[1;32m    803\u001b[0m     out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s h p -> b s (h p)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for w in range(len(chunks)):\n",
    "    if chunks_done[w]:\n",
    "        print(f\"Skipping chunk {w + 1}/{len(break_points) - 1} due to previous training.\")\n",
    "        continue\n",
    "    if args.which_chunk != -1 and w + 1 != args.which_chunk:\n",
    "        print(f\"Skipping chunk {w + 1}/{len(break_points) - 1} due to your request using --which-chunk.\")\n",
    "        continue\n",
    "    print(f\"Training on chunk {w + 1}/{len(break_points) - 1}\")\n",
    "\n",
    "    # Calculate chunk boundaries\n",
    "    train_start, train_end = chunks[w]                # [train_start, train_end)\n",
    "    # 向外扩 2*co 用于模型输入\n",
    "    final_start_pos = max(0, train_start - 2 * args.co)\n",
    "    final_end_pos   = min(dr.VARIANT_COUNT, train_end + 2 * args.co)\n",
    "\n",
    "    offset_before = train_start - final_start_pos\n",
    "    offset_after  = final_end_pos - train_end\n",
    "\n",
    "    ref_set  = dr.get_ref_set(train_start, train_end).astype(np.int32)\n",
    "    gmap_chunk = dr.get_gmap(train_start, train_end)\n",
    "    print(f\"Data Chunk Shape: {ref_set.shape}, gmap shape: {gmap_chunk.shape}\")\n",
    "    print(f\"  effective train slice = {offset_before}:{ref_set.shape[1] - offset_after}\")\n",
    "\n",
    "    # Remove duplicates from training\n",
    "    ref_set_train = remove_similar_rows(ref_set[x_train_indices])\n",
    "    ref_set_val = ref_set[x_valid_indices]\n",
    "\n",
    "    # MAF bins counts\n",
    "    valid_slice = slice(offset_before,\n",
    "                        ref_set_train.shape[1] - offset_after)\n",
    "    chunk_maf, chunk_bin_cnt = precompute_maf(\n",
    "        ref_set_train[:, valid_slice], \n",
    "        mask_int=-1\n",
    "    )\n",
    "    chunk_maf = torch.from_numpy(chunk_maf).to(device)          # (L_chunk,)\n",
    "    print('Chunk MAF-bin counts:', chunk_bin_cnt)\n",
    "\n",
    "    # Create targets (same as input for reconstruction)\n",
    "    target_train = ref_set_train[:, offset_before:ref_set_train.shape[1] - offset_after]\n",
    "    target_val = ref_set_val[:, offset_before:ref_set_val.shape[1] - offset_after]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = GenomicDataset(\n",
    "        ref_set_train, target_train, dr.SEQ_DEPTH, gmap_chunk,\n",
    "        offset_before, offset_after, training=True,\n",
    "        masking_rates=(args.min_mr, args.max_mr)\n",
    "    )\n",
    "\n",
    "    val_dataset = GenomicDataset(\n",
    "        ref_set_val, target_val, dr.SEQ_DEPTH, gmap_chunk, \n",
    "        offset_before, offset_after, training=False,\n",
    "        masking_rates=(args.min_mr, args.max_mr)\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size_per_gpu,\n",
    "                                shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size_per_gpu,\n",
    "                            shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Create model\n",
    "    # seq_len = ref_set.shape[1]\n",
    "    model = create_model(args, dr.SEQ_DEPTH)\n",
    "    model.offset_before = offset_before\n",
    "    model.offset_after = offset_after\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = ImputationLoss(use_r2=getattr(args, 'use_r2', True))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    # optimizer = Lamb(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    patience = args.earlystop_patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_logits, train_gts, train_mask = [], [], []\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{args.epochs}', leave=False)\n",
    "        for batch_idx, (x_onehot, x_coord, target) in enumerate(train_pbar):\n",
    "            x_coord = x_coord[0] \n",
    "            x_onehot, x_coord, target = x_onehot.to(device), x_coord.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_onehot, x_coord)\n",
    "            loss, logs = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            # === 收集训练结果 ===\n",
    "            mask = x_onehot[..., -1].bool()         # 只关心被 mask 的位点\n",
    "            train_logits.append(output.detach())\n",
    "            train_gts.append(target.detach())\n",
    "            train_mask.append(mask)\n",
    "\n",
    "        # 训练集 MAF-acc\n",
    "        train_logits = torch.cat(train_logits, dim=0)\n",
    "        train_gts    = torch.cat(train_gts,    dim=0)\n",
    "        train_mask   = torch.cat(train_mask,   dim=0)\n",
    "        # 只保留有效位点（去掉 offset  padding）\n",
    "        if model.offset_before > 0 or model.offset_after > 0:\n",
    "            train_mask   = train_mask  [:, model.offset_before : train_mask.shape[1]  -model.offset_after]\n",
    "        train_maf_accs = imputation_maf_accuracy_epoch(train_logits, train_gts, chunk_maf, mask=train_mask)\n",
    "\n",
    "        # ----------- 验证循环同理 ------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_logits, val_gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for (x_onehot, x_coord, target) in val_loader:\n",
    "                x_coord = x_coord[0] \n",
    "                x_onehot, x_coord, target = x_onehot.to(device), x_coord.to(device), target.to(device)\n",
    "                output = model(x_onehot, x_coord)\n",
    "                loss, logs = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                mask = x_onehot[..., -1].bool()\n",
    "                val_logits.append(output)\n",
    "                val_gts.append(target)\n",
    "\n",
    "        val_logits = torch.cat(val_logits, dim=0)\n",
    "        val_gts    = torch.cat(val_gts,    dim=0)\n",
    "        val_maf_accs = imputation_maf_accuracy_epoch(\n",
    "            val_logits, val_gts, chunk_maf,  mask=None,)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss   = val_loss   / len(val_loader)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss   = val_loss   / len(val_loader)\n",
    "        print(f'Chunk {w + 1}, Epoch {epoch + 1}/{args.epochs}, '\n",
    "                f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # 用 DataFrame 打印 MAF-bin 结果\n",
    "        maf_df = pd.DataFrame({\n",
    "            'MAF_bin': ['(0.00, 0.05)', '(0.05, 0.10)', '(0.10, 0.20)',\n",
    "                        '(0.20, 0.30)', '(0.30, 0.40)', '(0.40, 0.50)'],\n",
    "            'Counts':  [f\"{c}\" for c in chunk_bin_cnt],\n",
    "            'Train':   [f\"{acc:.2f}\" for acc in train_maf_accs],\n",
    "            'Val':     [f\"{acc:.2f}\" for acc in val_maf_accs]\n",
    "        })\n",
    "        print(maf_df.to_string(index=False))\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            import os\n",
    "            os.makedirs(f'{args.save_dir}/models', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{args.save_dir}/models/w_{w}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                if args.verbose >= 1:\n",
    "                    print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    # Mark chunk as done\n",
    "    chunks_done[w] = True\n",
    "    save_chunk_status(args.save_dir, chunks_done)\n",
    "\n",
    "    # Clean up\n",
    "    del ref_set, train_dataset, val_dataset, train_loader, val_loader, model\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b1c54",
   "metadata": {},
   "source": [
    "复制以上输出到文件，然后执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e2457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\"\"\"\n",
    "把训练日志中的 chunk/epoch/loss 以及 MAF 六区间统计\n",
    "整理成一张宽表（csv 或 DataFrame）。\n",
    "用法：\n",
    "    python parse_log.py train.log  ->  在当前目录生成 train.log.csv\n",
    "    也可以在脚本里直接改 LOG_PATH。\n",
    "\"\"\"\n",
    "\n",
    "# --------------------- 需要改的地方 ---------------------\n",
    "LOG_PATH = '/home/qmtang/mnt_qmtang/EvoFill/logs/251005.logs'          # 日志路径\n",
    "OUT_PATH = None                 # 输出 csv 路径，None 则自动生成\n",
    "# ------------------------------------------------------\n",
    "\n",
    "if OUT_PATH is None:\n",
    "    OUT_PATH = Path(LOG_PATH).with_suffix('.csv')\n",
    "\n",
    "# 正则：抓取 Chunk x, Epoch y/100, Train Loss: ..., Val Loss: ...\n",
    "epoch_re = re.compile(\n",
    "    r'Chunk\\s+(\\d+),\\s+Epoch\\s+(\\d+)/\\d+,\\s+'\n",
    "    r'Train Loss:\\s+([\\d\\.]+),\\s+Val Loss:\\s+([\\d\\.]+)'\n",
    ")\n",
    "\n",
    "# 正则：抓取 MAF 表格里每一行  (0.00, 0.05)   7421 0.974 0.997\n",
    "maf_re = re.compile(\n",
    "    r'\\(\\d+\\.\\d+,\\s*\\d+\\.\\d+\\)\\s+(\\d+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)'\n",
    ")\n",
    "\n",
    "records = []\n",
    "current = {}\n",
    "\n",
    "with open(LOG_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "\n",
    "        # 遇到新的 epoch 行\n",
    "        m = epoch_re.search(line)\n",
    "        if m:\n",
    "            # 如果上一个 epoch 已经抓完 MAF，就保存\n",
    "            if current and 'maf_0_counts' in current:\n",
    "                records.append(current)\n",
    "\n",
    "            current = {\n",
    "                'chunk': int(m.group(1)),\n",
    "                'epoch': int(m.group(2)),\n",
    "                'train_loss': float(m.group(3)),\n",
    "                'val_loss': float(m.group(4))\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # 在 MAF 表格区域内\n",
    "        if current:\n",
    "            m = maf_re.search(line)\n",
    "            if m:\n",
    "                idx = len([k for k in current.keys() if k.startswith('maf_')]) // 3\n",
    "                current[f'maf_{idx}_counts'] = int(m.group(1))\n",
    "                current[f'maf_{idx}_train'] = float(m.group(2))\n",
    "                current[f'maf_{idx}_val'] = float(m.group(3))\n",
    "\n",
    "# 别忘了最后一条\n",
    "if current and 'maf_0_counts' in current:\n",
    "    records.append(current)\n",
    "\n",
    "# 拼表\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 让列顺序好看一点\n",
    "cols = ['chunk', 'epoch', 'train_loss', 'val_loss']\n",
    "maf_cols = sorted([c for c in df.columns if c.startswith('maf_')])\n",
    "df = df[cols + maf_cols]\n",
    "\n",
    "# 保存 & 打印\n",
    "df.to_csv(OUT_PATH, index=False, float_format='%.6f')\n",
    "print(f'已解析 {len(df)} 条记录，保存为 {OUT_PATH}')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. 读取日志\n",
    "logs_set = {\n",
    "    \"n·(MB+CA)\": \"/home/qmtang/mnt_qmtang/EvoFill/logs/251003.csv\",\n",
    "    \"n·(MB+MB)\": \"/home/qmtang/mnt_qmtang/EvoFill/logs/251004.csv\",\n",
    "    \"1·(MB+MB)\": \"/home/qmtang/mnt_qmtang/EvoFill/logs/251005.csv\"\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "for m, p in logs_set.items():\n",
    "    dfs[m] = pd.read_csv(p)\n",
    "\n",
    "# 2. 统一列名检查（可选）\n",
    "# for m, df in dfs.items():\n",
    "#     print(m, df.columns)\n",
    "\n",
    "# 3. 画图：train_loss\n",
    "n_chunk = 7\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 9))\n",
    "axes = axes.flatten()\n",
    "for idx, chunk in enumerate(range(1, n_chunk + 1)):\n",
    "    ax = axes[idx]\n",
    "    for m in dfs:\n",
    "        df = dfs[m]\n",
    "        df_c = df[df['chunk'] == chunk]\n",
    "        sns.lineplot(data=df_c, x='epoch', y='train_loss', label=m, ax=ax, marker='o')\n",
    "    ax.set_title(f'Chunk {chunk} – train_loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('train_loss')\n",
    "    ax.legend()\n",
    "# 隐藏多余子图\n",
    "for j in range(n_chunk, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "plt.suptitle('Train Loss across Chunks', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. 画图：val_loss\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 9))\n",
    "axes = axes.flatten()\n",
    "for idx, chunk in enumerate(range(1, n_chunk + 1)):\n",
    "    ax = axes[idx]\n",
    "    for m in dfs:\n",
    "        df = dfs[m]\n",
    "        df_c = df[df['chunk'] == chunk]\n",
    "        sns.lineplot(data=df_c, x='epoch', y='val_loss', label=m, ax=ax, marker='o')\n",
    "    ax.set_title(f'Chunk {chunk} – val_loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('val_loss')\n",
    "    ax.legend()\n",
    "for j in range(n_chunk, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "plt.suptitle('Validation Loss across Chunks', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba06a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logs_set = {\n",
    "    \"n·(MB+CA)\": \"/home/qmtang/mnt_qmtang/EvoFill/logs/251003.csv\",\n",
    "    \"n·(MB+MB)\": \"/home/qmtang/mnt_qmtang/EvoFill/logs/251004.csv\",\n",
    "    \"1·(MB+MB)\": \"/home/qmtang/mnt_qmtang/EvoFill/logs/251005.csv\"\n",
    "}\n",
    "\n",
    "dfs = {m: pd.read_csv(p) for m, p in logs_set.items()}\n",
    "\n",
    "# 2. 每个模型每个 chunk 取最大 epoch 的一行\n",
    "best_rows = []\n",
    "for m, df in dfs.items():\n",
    "    best = df.loc[df.groupby('chunk')['epoch'].idxmax()].copy()\n",
    "    best['model'] = m\n",
    "    best_rows.append(best)\n",
    "best_df = pd.concat(best_rows, ignore_index=True)\n",
    "\n",
    "maf_cols = [f'maf_{i}_{s}' for i in range(6) for s in ['train', 'val']]\n",
    "best_df.loc[best_df['model'] == \"1·(MB+MB)\", maf_cols] /= 100\n",
    "\n",
    "maf_labels = [\n",
    "    'MAF(0.00,0.05)',\n",
    "    'MAF(0.05,0.10)',\n",
    "    'MAF(0.10,0.20)',\n",
    "    'MAF(0.20,0.30)',\n",
    "    'MAF(0.30,0.40)',\n",
    "    'MAF(0.40,0.50)'\n",
    "]\n",
    "\n",
    "# ---------- maf_0~5_train ----------\n",
    "train_cols = [f'maf_{i}_train' for i in range(6)]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(train_cols):\n",
    "    sns.barplot(data=best_df, x='chunk', y=col, hue='model', ax=axes[i])\n",
    "    axes[i].set_ylim(0.75, 1.0)\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].set_title(maf_labels[i])\n",
    "plt.suptitle('Best Epoch – Training Accuracy', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- maf_0~5_val ----------\n",
    "val_cols = [f'maf_{i}_val' for i in range(6)]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(val_cols):\n",
    "    sns.barplot(data=best_df, x='chunk', y=col, hue='model', ax=axes[i])\n",
    "    axes[i].set_ylim(0.75, 1.0)\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].set_title(maf_labels[i])\n",
    "plt.suptitle('Best Epoch – Validation Accuracy', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
