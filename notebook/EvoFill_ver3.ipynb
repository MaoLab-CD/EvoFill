{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60ccf3a",
   "metadata": {},
   "source": [
    "ver0: 多 chunk modules 独立权重\n",
    "\n",
    "ver0.1: 加样本特征标签（演化坐标）\n",
    "\n",
    "ver3: chunk-wise 稀疏激活"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bb00d",
   "metadata": {},
   "source": [
    "## Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "757312f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # 设置GPU\n",
    "from cyvcf2 import VCF\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import shutil\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mamba_ssm import Mamba2\n",
    "from mamba_ssm.modules.mamba2_simple import Mamba2Simple as Mamba2Block # 原Mamba2Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975df815",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e956fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenotypeEncoder:\n",
    "    def __init__(self,\n",
    "                 save_dir: str,\n",
    "                 vcf_path: str,\n",
    "                 ref_extra: Optional[str] = None,\n",
    "                 phased: bool = True,\n",
    "                 gts012: bool = False):\n",
    "        self.save_dir = save_dir\n",
    "        self.vcf_path    = vcf_path\n",
    "        self.ref_extra   = ref_extra\n",
    "        self.phased      = phased if ref_extra is None else False # 是否把样本拆成单倍型\n",
    "        self.gts012      = gts012\n",
    "        \n",
    "        # 其余成员先占位\n",
    "        self.hap_map = {}\n",
    "        self.n_samples   = 0\n",
    "        self.n_variants  = 0\n",
    "        self.sample_ids  = []   # 后面读 VCF 时填充\n",
    "        self.variant_ids = []\n",
    "\n",
    "        self.X_gt        = None   # 最终返回的张量\n",
    "        self.X_extra     = None   # extra 信息\n",
    "        self.seq_depth   = None\n",
    "\n",
    "        # 1) 读 VCF\n",
    "        self.X_gt = self.load_gt()\n",
    "        # 2) 读 extra\n",
    "        self.X_extra = self.load_extra() if self.ref_extra else None\n",
    "        # 3) 保存 meta\n",
    "        self.save_meta()\n",
    "\n",
    "    def add_hap_map(self, key, val):\n",
    "        if key in self.hap_map:\n",
    "            if self.hap_map[key] != int(val):\n",
    "                raise(f\"[DATA] hap_map[{key}] inconsistent\")\n",
    "        else:\n",
    "            self.hap_map[key] = int(val)\n",
    "\n",
    "    def encode_gt(self, rec, n_samples, phase=False, gts012=True):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            phase=False  -> (n_samples,)          剂量或基因型\n",
    "            phase=True   -> (2*n_samples,)        单倍型\n",
    "\n",
    "        encoding rule:\n",
    "            gts012=True  -> 0/1/2/3  （3=missing）\n",
    "            gts012=False -> 0/1/2/3/…/-1  （0=REF, 1+=ALT, -1=missing）\n",
    "        \"\"\"\n",
    "        n = n_samples\n",
    "\n",
    "        # ---------- 1. 单倍型模式 ----------\n",
    "        if phase:\n",
    "            out = np.empty(2 * n, dtype=np.int8)\n",
    "            for i, gt in enumerate(rec.genotypes):\n",
    "                a1, a2, _phased = gt\n",
    "                # 缺失\n",
    "                if a1 is None:\n",
    "                    out[2*i]   = 3 if gts012 else -1\n",
    "                    a1 = '.'\n",
    "                else:\n",
    "                    if gts012:                      # 压缩成 0/1/2\n",
    "                        out[2*i] = 0 if a1 == 0 else (2 if a1 >= 2 else 1)\n",
    "                    else:                           # 原值保留\n",
    "                        out[2*i] = a1\n",
    "                self.add_hap_map(str(a1), out[2*i])\n",
    "                if a2 is None:\n",
    "                    out[2*i+1] = 3 if gts012 else -1\n",
    "                    a2 = '.'\n",
    "                else:\n",
    "                    if gts012:\n",
    "                        out[2*i+1] = 0 if a2 == 0 else (2 if a2 >= 2 else 1)\n",
    "                    else:\n",
    "                        out[2*i+1] = a2\n",
    "                self.add_hap_map(str(a2),out[2*i+1])\n",
    "            return out\n",
    "\n",
    "        # ---------- 2. 剂量模式 ----------\n",
    "        else:\n",
    "            out = np.empty(n, dtype=np.int8)\n",
    "            for i, gt in enumerate(rec.genotypes):\n",
    "                a1, a2, _phased = gt\n",
    "                phase = '|' if _phased else '/'\n",
    "                # 缺失\n",
    "                if a1 is None or a2 is None:\n",
    "                    out[i] = 3 if gts012 else -1\n",
    "                else:\n",
    "                    if gts012:\n",
    "                        # 0/1/2 剂量\n",
    "                        out[i] = (1 if a1 > 0 else 0) + (1 if a2 > 0 else 0)\n",
    "                    else:\n",
    "                        # 多等位剂量：把 ALT 编号直接相加\n",
    "                        out[i] = (0 if a1 == 0 else a1) + (0 if a2 == 0 else a2)\n",
    "                a1 ='.' if a1 is None else str(a1)\n",
    "                a2 ='.' if a2 is None else str(a2)\n",
    "                a1, a2 = sorted([a1,a2])\n",
    "                self.add_hap_map(a1+phase+a2, out[i])\n",
    "            return out\n",
    "\n",
    "    def load_extra(self) -> Optional[np.ndarray]:\n",
    "        try:\n",
    "            df = pd.read_csv(self.ref_extra, sep='\\t', index_col=0)\n",
    "            df = df.loc[self.sample_ids]          # 保证与 VCF 样本顺序一致\n",
    "            print(f\"[DATA] Extra dims: {df.shape}\")\n",
    "            return df.values.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"[DATA] Extra features skipped: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_gt(self):\n",
    "        interval = 10000\n",
    "\n",
    "        cols, data, indptr = [], [], [0]\n",
    "\n",
    "        vcf = VCF(self.vcf_path, gts012 = self.gts012)\n",
    "        self.sample_ids = vcf.samples\n",
    "        self.n_samples = len(self.sample_ids)\n",
    "        self.n_variants = 0\n",
    "\n",
    "        for rec in vcf:\n",
    "            vec = self.encode_gt(rec, self.n_samples, phase=self.phased, gts012=self.gts012)\n",
    "            nz_idx = np.flatnonzero(vec)\n",
    "            cols.extend(nz_idx)\n",
    "            data.extend(vec[nz_idx])\n",
    "            indptr.append(indptr[-1] + len(nz_idx))\n",
    "\n",
    "            self.n_variants += 1\n",
    "            self.variant_ids.append(f\"{rec.CHROM}:{rec.POS}_{rec.REF}/{','.join(rec.ALT)}\")\n",
    "            if self.n_variants % interval == 0:\n",
    "                print(f'\\r[DATA] 已编码 {self.n_variants:,} 个位点', end='', flush=True)\n",
    "\n",
    "        print(f'\\r[DATA] 总计 {self.n_variants:,} 个位点  ', flush=True)\n",
    "        vcf.close()\n",
    "\n",
    "        # 根据 phase_mode 决定行数\n",
    "        n_rows = 2 * self.n_samples if self.phased else self.n_samples\n",
    "        M = sp.csc_matrix((data, cols, indptr),\n",
    "                        shape=(n_rows,self.n_variants),\n",
    "                        dtype=np.int8)\n",
    "\n",
    "        print(f'[DATA] 位点矩阵 = {M.shape}，稀疏度 = {M.nnz / (M.shape[0] * M.shape[1]):.2%}')\n",
    "        if self.gts012:\n",
    "            self.seq_depth = M.data.max()+1\n",
    "        else:\n",
    "            self.seq_depth = M.data.max() + 2 \n",
    "            M.data[M.data == -1] = M.data.max() + 1\n",
    "            self.hap_map = {k: self.seq_depth-1 if '.' in str(k) else v for k, v in self.hap_map.items()}\n",
    "        \n",
    "        print(\"[DATA] Hap Map: \",self.hap_map)\n",
    "        print(f'[DATA] gt alleles = [0 - {M.data.max()}], seq_depth = {self.seq_depth} ({self.seq_depth-1} 代表缺失)')\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)          # 1. 不存在就创建\n",
    "\n",
    "        # 2. 保存稀疏矩阵\n",
    "        sp.save_npz(os.path.join(self.save_dir, \"gt_matrix.npz\"), M)\n",
    "\n",
    "        # 3. 保存样本列表（顺序与矩阵行对应）\n",
    "        with open(os.path.join(self.save_dir, \"gt_samples.txt\"), \"w\") as f:\n",
    "            if self.phased:                      # 单倍型模式：写成 sample_A / sample_B\n",
    "                for s in self.sample_ids:\n",
    "                    f.write(f\"{s}_A\\n{s}_B\\n\")\n",
    "            else:                               # 剂量模式\n",
    "                for s in self.sample_ids:\n",
    "                    f.write(f\"{s}\\n\")\n",
    "\n",
    "        # 4. 保存变异位点 ID（chr:pos/ref/alt）\n",
    "        with open(os.path.join(self.save_dir, \"gt_variants.txt\"), \"w\") as f:\n",
    "            for vid in self.variant_ids:\n",
    "                f.write(vid + \"\\n\")\n",
    "\n",
    "        print(f\"[DATA] 结果已写入 {self.save_dir}\")\n",
    "        return M\n",
    "\n",
    "    def save_meta(self):\n",
    "        def _make_json_safe(obj):\n",
    "            \"\"\"递归地把 numpy 数组、tuple、set、bytes 转成 list/str\"\"\"\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: _make_json_safe(v) for k, v in obj.items()}\n",
    "            if isinstance(obj, (list, tuple, set)):\n",
    "                return [_make_json_safe(i) for i in obj]\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return _make_json_safe(obj.tolist())\n",
    "            if isinstance(obj, (np.integer, np.floating)):\n",
    "                return obj.item()\n",
    "            if isinstance(obj, bytes):\n",
    "                return obj.decode(errors='ignore')\n",
    "            return obj\n",
    "        meta = {\n",
    "            \"vcf_path\"   : str(self.vcf_path),\n",
    "            \"ref_extra\"  : str(self.ref_extra),\n",
    "            \"phased\"     : str(self.phased),\n",
    "            \"gts012\"     : str(self.gts012),\n",
    "            \"n_samples\"  : str(self.n_samples),\n",
    "            \"n_variants\" : str(self.n_variants),\n",
    "            \"seq_depth\"  : str(self.seq_depth),\n",
    "            \"hap_map\"    : _make_json_safe(self.hap_map),\n",
    "        }\n",
    "        with open(os.path.join(self.save_dir, \"gt_enc_meta.json\"), \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        # 如果 X_extra 不是 None，也可以落盘\n",
    "        if self.X_extra is not None:\n",
    "            np.save(os.path.join(self.save_dir, \"gt_extra.npy\"), self.X_extra)\n",
    "\n",
    "    @classmethod\n",
    "    def loadfromdisk(cls, work_dir: str):\n",
    "        \"\"\"\n",
    "        反向构造 GenotypeEncoder，要求 work_dir 里必须有：\n",
    "            gt_matrix.npz      -> X_gt  (scipy.sparse.csc_matrix)\n",
    "            gt_samples.txt     -> sample_ids\n",
    "            gt_variants.txt    -> variant_ids\n",
    "            gt_enc_meta.json   -> 其余标量 / 布尔 / 路径信息\n",
    "        \"\"\"\n",
    "        # 1. 读 meta（构造 __init__ 需要的几个“外部”参数）\n",
    "        meta_path = os.path.join(work_dir, \"gt_enc_meta.json\")\n",
    "        if not os.path.exists(meta_path):\n",
    "            raise FileNotFoundError(f\"{meta_path} 不存在，无法反序列化\")\n",
    "        with open(meta_path) as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        # 2. 先“假”构造一个对象（不触发 VCF 扫描）\n",
    "        #    把关键字段先填进去，避免 __init__ 里再去读 VCF\n",
    "        obj = cls.__new__(cls)  # 不调用 __init__\n",
    "        obj.vcf_path    = meta[\"vcf_path\"]\n",
    "        obj.ref_extra   = meta[\"ref_extra\"]\n",
    "        obj.phased      = bool(meta[\"phased\"])\n",
    "        obj.gts012      = bool(meta[\"gts012\"])\n",
    "        obj.n_samples   = int(meta[\"n_samples\"])\n",
    "        obj.n_variants  = int(meta[\"n_variants\"])\n",
    "        obj.seq_depth   = int(meta[\"seq_depth\"])\n",
    "        obj.hap_map     = meta[\"hap_map\"]\n",
    "\n",
    "        # 3. 读样本 & 位点 ID 列表\n",
    "        obj.sample_ids = [\n",
    "            l.rstrip(\"\\n\") for l in open(os.path.join(work_dir, \"gt_samples.txt\"))\n",
    "        ]\n",
    "        obj.variant_ids = [\n",
    "            l.rstrip(\"\\n\") for l in open(os.path.join(work_dir, \"gt_variants.txt\"))\n",
    "        ]\n",
    "\n",
    "        # 4. 读稀疏矩阵\n",
    "        obj.X_gt = sp.load_npz(os.path.join(work_dir, \"gt_matrix.npz\"))\n",
    "\n",
    "        # 5. 读 extra（如果有）\n",
    "        extra_path = os.path.join(work_dir, \"gt_extra.npy\")\n",
    "        if os.path.exists(extra_path):\n",
    "            obj.X_extra = np.load(extra_path)\n",
    "        else:\n",
    "            obj.X_extra = None\n",
    "\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d851db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 99,314 个位点  \n",
      "[DATA] 位点矩阵 = (2404, 99314)，稀疏度 = 28.10%\n",
      "[DATA] {'0|0': 0, '0|1': 1, '1|1': 2}\n",
      "[DATA] gt alleles = [0 - 2], seq_depth = 4 (3 代表缺失)\n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill/data/251027_ver3_chr22_trim\n",
      "[DATA] Extra dims: (2404, 26)\n",
      "[DATA] 2,404 Samples\n",
      "[DATA] 99,314 Variants Sites\n",
      "[DATA] 4 seq_depth\n",
      "[DATA] Hap Map: {'0|0': 0, '0|1': 1, '1|1': 2}\n"
     ]
    }
   ],
   "source": [
    "work_dir = '/mnt/qmtang/EvoFill/data/251027_ver3_chr22_trim'\n",
    "gt_enc = GenotypeEncoder(\n",
    "    save_dir=work_dir,\n",
    "    vcf_path='/home/qmtang/GitHub/STICI-HPC/data/training_sets/ALL.chr22.training.samples.100k.any.type.0.01.maf.variants.vcf.gz',\n",
    "    ref_extra='/mnt/qmtang/EvoFill/data/251020_ver01_chr22/pop_wasserstein.tsv',\n",
    "    phased= True,\n",
    "    gts012= False)\n",
    "\n",
    "print(f\"[DATA] {gt_enc.n_samples:,} Samples\")\n",
    "print(f\"[DATA] {gt_enc.n_variants:,} Variants Sites\")\n",
    "print(f\"[DATA] {gt_enc.seq_depth} seq_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a04e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 1,103,547 个位点  \n",
      "[DATA] 位点矩阵 = (5008, 1103547)，稀疏度 = 3.71%\n",
      "[DATA] gt alleles = [0 - 8], seq_depth = 10 (9 代表缺失)\n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill/data/251023_chr22\n",
      "[DATA] 2,504 Samples\n",
      "[DATA] 1,103,547 Variants Sites\n",
      "[DATA] 10 seq_depth\n"
     ]
    }
   ],
   "source": [
    "work_dir = '/mnt/qmtang/EvoFill/data/251027_ver3_chr22'\n",
    "gt_enc = GenotypeEncoder(\n",
    "    save_dir=work_dir,\n",
    "    vcf_path='/mnt/NAS/Omics/DNA/1kGP/vcf/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz',\n",
    "    ref_extra=None,\n",
    "    phased= True,\n",
    "    gts012= False)\n",
    "\n",
    "print(f\"[DATA] {gt_enc.n_samples:,} Samples\")\n",
    "print(f\"[DATA] {gt_enc.n_variants:,} Variants Sites\")\n",
    "print(f\"[DATA] {gt_enc.seq_depth} seq_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbb96506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicDataset(Dataset):\n",
    "    \"\"\"Dataset class for genomic data with masking for training\"\"\"\n",
    "    def __init__(self, x_gts_sparse, x_extra=None, seq_depth=4,\n",
    "                 mask=True, masking_rates=(0.5, 0.99), indices=None):\n",
    "        \"\"\"\n",
    "        x_gts_sparse: scipy.sparse.csr_matrix or similar\n",
    "        x_extra: numpy array or None\n",
    "        indices: 可选，指定要使用的样本索引（如 train/valid 索引）\n",
    "        \"\"\"\n",
    "        self.gts_sparse = x_gts_sparse\n",
    "        self.x_extra = x_extra\n",
    "        self.seq_depth = seq_depth\n",
    "        self.mask = mask\n",
    "        self.masking_rates = masking_rates\n",
    "        self.indices = indices if indices is not None else np.arange(x_gts_sparse.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        x = self.gts_sparse[real_idx].toarray().squeeze().astype(np.int8)\n",
    "        y = x.copy()\n",
    "\n",
    "        if self.mask:\n",
    "            seq_len = len(x)\n",
    "            masking_rate = np.random.uniform(*self.masking_rates)\n",
    "            mask_size = int(seq_len * masking_rate)\n",
    "            mask_indices = np.random.choice(seq_len, mask_size, replace=False)\n",
    "            x[mask_indices] = self.seq_depth - 1  # missing token\n",
    "\n",
    "        x_onehot = torch.FloatTensor(np.eye(self.seq_depth)[x])\n",
    "        y_onehot = torch.FloatTensor(np.eye(self.seq_depth - 1)[y])\n",
    "\n",
    "        if self.x_extra is not None:\n",
    "            x_extra = torch.FloatTensor(self.x_extra[real_idx])\n",
    "        else:\n",
    "            x_extra = torch.empty(0)\n",
    "\n",
    "        return x_onehot, x_extra, y_onehot\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    \"\"\"Dataset for imputation (no masking needed)\"\"\"\n",
    "\n",
    "    def __init__(self, data, seq_depth):\n",
    "        self.data = data\n",
    "        self.seq_depth = seq_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        # Convert to one-hot without masking\n",
    "        x_onehot = np.eye(self.seq_depth)[x]\n",
    "        return torch.FloatTensor(x_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86e007",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac21e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenoEmbedding(nn.Module):\n",
    "    \"\"\"Genomic embedding layer with positional encoding\"\"\"\n",
    "\n",
    "    def __init__(self, n_alleles, n_snps, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_alleles = n_alleles\n",
    "        self.n_snps = n_snps\n",
    "\n",
    "        # Allele embedding\n",
    "        self.allele_embedding = nn.Parameter(torch.randn(n_alleles, d_model))\n",
    "\n",
    "        # Positional embedding\n",
    "        self.position_embedding = nn.Embedding(n_snps, d_model)\n",
    "\n",
    "        # Initialize parameters\n",
    "        nn.init.xavier_uniform_(self.allele_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, n_alleles) - one-hot encoded\n",
    "        _, seq_len, _ = x.shape\n",
    "\n",
    "        # Allele embedding\n",
    "        embedded = torch.einsum('bsn,nd->bsd', x, self.allele_embedding)\n",
    "\n",
    "        # Positional embedding\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        pos_emb = self.position_embedding(positions).unsqueeze(0)\n",
    "\n",
    "        return embedded + pos_emb\n",
    "\n",
    "class BiMambaBlock(nn.Module):\n",
    "    \"\"\"Bidirectional Mamba block for genomic sequence processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Forward and backward Mamba blocks\n",
    "        self.mamba_forward = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "        self.mamba_backward = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        residual = x\n",
    "\n",
    "        # Bidirectional processing\n",
    "        x_norm = self.norm1(x)\n",
    "\n",
    "        # Forward direction\n",
    "        forward_out = self.mamba_forward(x_norm)\n",
    "\n",
    "        # Backward direction (flip sequence)\n",
    "        x_backward = torch.flip(x_norm, dims=[1])\n",
    "        backward_out = self.mamba_backward(x_backward)\n",
    "        backward_out = torch.flip(backward_out, dims=[1])\n",
    "\n",
    "        # Concatenate bidirectional outputs\n",
    "        bi_out = torch.cat([forward_out, backward_out], dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        ffn_out = self.ffn(bi_out)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # Residual connection\n",
    "        out = self.norm2(residual + ffn_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block for local pattern extraction\"\"\"\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.conv1 = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)\n",
    "\n",
    "        self.conv_large1 = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)\n",
    "        self.conv_large2 = nn.Conv1d(d_model, d_model, kernel_size=15, padding=7)\n",
    "\n",
    "        self.conv_final = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.conv_reduce = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(d_model)\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2)  # (batch, d_model, seq_len)\n",
    "\n",
    "        xa = self.gelu(self.conv1(x))\n",
    "\n",
    "        xb = self.gelu(self.conv2(xa))\n",
    "        xb = self.gelu(self.conv3(xb))\n",
    "\n",
    "        xc = self.gelu(self.conv_large1(xa))\n",
    "        xc = self.gelu(self.conv_large2(xc))\n",
    "\n",
    "        xa = xb + xc\n",
    "        xa = self.gelu(self.conv_final(xa))\n",
    "        xa = self.bn1(xa)\n",
    "        xa = self.gelu(self.conv_reduce(xa))\n",
    "        xa = self.bn2(xa)\n",
    "        xa = self.gelu(xa)\n",
    "\n",
    "        return xa.transpose(1, 2)  # (batch, seq_len, d_model)\n",
    "\n",
    "class ExtraEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    输入:  (B, L)        L == extra_dim\n",
    "    输出: (B, L, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 64,\n",
    "        d_conv: int  = 4,\n",
    "        expand: int  = 2,\n",
    "        headdim: int = 128,\n",
    "        ngroups: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        **mamba_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model   = d_model\n",
    "\n",
    "        # 1. 把 (B, L) 的 1-d 标量升到 d_model\n",
    "        self.in_proj = nn.Linear(1, d_model, bias=False)\n",
    "\n",
    "        # 2. 官方 Mamba2Simple：把 L 当序列长度，建模 L↔L\n",
    "        self.mamba = Mamba2Block(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            headdim=headdim,\n",
    "            ngroups=ngroups,\n",
    "            **mamba_kwargs\n",
    "        )\n",
    "\n",
    "        # 3. Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, L)  连续值或离散索引\n",
    "        \"\"\"\n",
    "        # (B, L) -> (B, L, 1) -> (B, L, d_model)\n",
    "        h = self.in_proj(x.unsqueeze(-1).float())   # 1-d 投影\n",
    "\n",
    "        h = self.norm(h)\n",
    "\n",
    "        # Mamba2Simple 要求输入 (B, L, d_model) 即可\n",
    "        out = self.mamba(h)                           # SSD 全局建模\n",
    "        return out\n",
    "\n",
    "class StackMambaBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=64,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        headdim=128,\n",
    "        ngroups=1,\n",
    "        chunk_size=256,\n",
    "        dropout=0.0,\n",
    "        d_embed_dropout=0.0,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 距离矩阵嵌入\n",
    "        self.extra_embed = ExtraEmbedding(d_model=d_model, dropout=d_embed_dropout)\n",
    "\n",
    "        # 原归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # SSD 核心\n",
    "        self.ssd = Mamba2Block(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            headdim=headdim,\n",
    "            ngroups=ngroups,\n",
    "            chunk_size=chunk_size,\n",
    "            use_mem_eff_path=True,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, local_repr, global_repr, x_extra=None,\n",
    "                start_offset=0, end_offset=0):\n",
    "        \"\"\"\n",
    "        local_repr: (B, L, D)\n",
    "        global_repr: (B, G, D)\n",
    "        x_extra: 可选，(B,E) \n",
    "        \"\"\"\n",
    "        local_norm  = self.norm1(local_repr)\n",
    "        global_norm = self.norm2(global_repr)\n",
    "\n",
    "        # 1. 构造输入序列\n",
    "        tokens = []\n",
    "        if x_extra is not None:\n",
    "            extra_token = self.extra_embed(x_extra)        # (B,E,D)\n",
    "            tokens.append(extra_token)\n",
    "        tokens.append(global_norm)\n",
    "        tokens.append(local_norm)\n",
    "        x = torch.cat(tokens, dim=1)               # [B, (E)+G+L, D]\n",
    "\n",
    "        # 2. SSD 扫描\n",
    "        x = self.ssd(x)                            # [B, (E)+G+L, D]\n",
    "\n",
    "        # 3. 只取 local 部分\n",
    "        local_len = local_norm.shape[1]\n",
    "        x = x[:, -local_len:, :]                   # [B, L, D]\n",
    "\n",
    "        # 4. pad 回原始长度\n",
    "        if start_offset or end_offset:\n",
    "            x = F.pad(x, (0, 0, start_offset, end_offset))\n",
    "\n",
    "        # 5. 残差 + FFN\n",
    "        x = x + local_norm\n",
    "        x = self.norm3(x)\n",
    "        x = self.ffn(x) + x\n",
    "        return x\n",
    "\n",
    "class ChunkModule(nn.Module):\n",
    "    \"\"\"Single chunk processing module with BiMamba\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # BiMamba block\n",
    "        self.bimamba_block = BiMambaBlock(d_model)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv_block1 = ConvBlock(d_model)\n",
    "        self.conv_block2 = ConvBlock(d_model)\n",
    "        self.conv_block3 = ConvBlock(d_model)\n",
    "        self.conv_block4 = ConvBlock(d_model)\n",
    "\n",
    "        # Cross attention\n",
    "        # self.cross_attention = CrossAttentionLayer(d_model, n_heads)\n",
    "        self.cross_attention = StackMambaBlock(\n",
    "            d_model=d_model,\n",
    "            d_state=64,\n",
    "            d_conv=4,\n",
    "            expand=2,\n",
    "            headdim=128,\n",
    "            ngroups=1,\n",
    "            chunk_size=256,\n",
    "        )\n",
    "\n",
    "        # Additional layers\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x, x_extra=None):\n",
    "        # BiMamba processing\n",
    "        xa0 = self.bimamba_block(x)\n",
    "\n",
    "        # First conv block\n",
    "        xa = self.conv_block1(xa0)\n",
    "        xa_skip = self.conv_block2(xa)\n",
    "\n",
    "        # Dense layer\n",
    "        xa = self.gelu(self.dense(xa))\n",
    "        xa = self.conv_block3(xa)\n",
    "\n",
    "        # Cross attention\n",
    "        xa = self.cross_attention(xa, xa0, x_extra)\n",
    "        xa = self.dropout(xa)\n",
    "\n",
    "        # Final conv block\n",
    "        xa = self.conv_block4(xa)\n",
    "\n",
    "        # Concatenate with skip connection\n",
    "        xa = torch.cat([xa_skip, xa], dim=-1)\n",
    "\n",
    "        return xa\n",
    "\n",
    "class UltraLongRangeMamba(nn.Module):\n",
    "    \"\"\"\n",
    "    线性复杂度 O(L) 全局建模，只激活 mask=1 的位点\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_state=64, d_conv=4, expand=2,\n",
    "                 n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_inner = int(d_model * expand)\n",
    "        # 可选：多层 Mamba2\n",
    "        self.layers = nn.ModuleList([\n",
    "            BiMambaBlock(d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, idx):\n",
    "        \"\"\"\n",
    "        x: (B, L_all, d_model)  全局张量，其余位置为 nan\n",
    "        idx: (M,)  当前 mask=1 的坐标\n",
    "        返回: (B, M, d_model//2)\n",
    "        \"\"\"\n",
    "        # 只取有效 token\n",
    "        x_in = x[:, idx] \n",
    "        x_in = self.dropout(x_in)                                   # (B, M, D)\n",
    "        for layer in self.layers:\n",
    "            x_in = layer(x_in)                              # BiMamba2\n",
    "        return self.norm(x_in)\n",
    "\n",
    "class GlobalOut(nn.Module):\n",
    "    def __init__(self, d_model, n_alleles, total_sites, chunk_size,\n",
    "                 kernel=5, pad=2, stripe=4096,\n",
    "                 d_state=64, d_conv=4, expand=2, n_mamba_layers=2):\n",
    "        super().__init__()\n",
    "        self.k, self.p = kernel, pad\n",
    "        self.stripe = stripe\n",
    "        self.total_sites = total_sites\n",
    "        self.n_alleles = n_alleles\n",
    "\n",
    "        # -------------- 1) 局部卷积权重 --------------\n",
    "        # Conv1: 2*d_model -> d_model//2\n",
    "        self.w1 = nn.Parameter(torch.empty(d_model // 2, 2 * d_model, kernel))\n",
    "        self.b1 = nn.Parameter(torch.zeros(d_model // 2))\n",
    "        # Conv2: d_model//2 -> n_alleles-1\n",
    "        self.w2 = nn.Parameter(torch.empty(n_alleles - 1, d_model // 2, kernel))\n",
    "        self.b2 = nn.Parameter(torch.zeros(n_alleles - 1))\n",
    "        nn.init.kaiming_normal_(self.w1)\n",
    "        nn.init.kaiming_normal_(self.w2)\n",
    "\n",
    "        # -------------- 2) ulr 中间件（Mamba2） --------------\n",
    "        self.ulr_mamba = UltraLongRangeMamba(\n",
    "            d_model=d_model//2,          # 与 Conv1 输出同维\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            n_layers=n_mamba_layers,\n",
    "        )\n",
    "        self.gate = nn.Linear(d_model, 2)   # [local; global] -> 2\n",
    "        self.norm = nn.LayerNorm(d_model // 2)\n",
    "\n",
    "        # -------------- 3) 开关 --------------\n",
    "        self.skip_ulr = True\n",
    "        self.set_ulr_enabled(False)\n",
    "\n",
    "    # ============ 两阶段切换 ============\n",
    "    def set_ulr_enabled(self, enabled: bool):\n",
    "        self.skip_ulr = not enabled\n",
    "        for p in self.ulr_mamba.parameters():\n",
    "            p.requires_grad = enabled\n",
    "        for p in self.gate.parameters():\n",
    "            p.requires_grad = enabled\n",
    "\n",
    "    # ============ 前向：ulr 是可插拔中间件 ============\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x:   (B, L,  2*d_model)\n",
    "        mask:(L,) 0/1\n",
    "        return: (B, L, n_alleles-1)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)  # (B, 2*d_model, L)\n",
    "        device = x.device\n",
    "        idx = torch.where(mask)[0]                # 有效坐标 M\n",
    "        n = idx.shape[0]\n",
    "        out = torch.full((x.shape[0], self.w2.shape[0], x.shape[2]), -float('inf'),\n",
    "                         device=device, dtype=x.dtype)\n",
    "\n",
    "        # ---- 1) 统一走 Conv1：2*d_model -> d_model//2 ----\n",
    "        h_local = []                              # (B, d_model//2, M)\n",
    "        for i in range(0, n, self.stripe):\n",
    "            sl = slice(i, i + self.stripe)\n",
    "            idx_i = idx[sl]\n",
    "            x_i = x[..., idx_i].contiguous()      # (B, 2*d_model, stripe)\n",
    "\n",
    "            y1 = checkpoint(self._band_conv1, x_i, self.w1, self.b1, use_reentrant=False)\n",
    "            h_local.append(y1)\n",
    "        h_local = torch.cat(h_local, dim=2).transpose(1, 2)  # (B, M, d_model//2)\n",
    "        # ---- 2) ulr 中间件（可选） ----\n",
    "        if self.skip_ulr:\n",
    "            # 第一阶段：不做任何全局事，h_local 保持原样\n",
    "            fused = h_local\n",
    "        else:\n",
    "            # 第二阶段：Mamba2 全局建模并融合\n",
    "            h_global = self.ulr_mamba(h_local, idx)           # (B, M, d_model//2)\n",
    "            gate_in = torch.cat([h_local, h_global], dim=-1)  # (B, M, d_model)\n",
    "            w = torch.softmax(self.gate(gate_in), dim=-1)     # (B, M, 2)\n",
    "            fused = w[..., 0:1] * h_local + w[..., 1:2] * h_global\n",
    "            fused = self.norm(fused)                          # (B, M, d_model//2)\n",
    "\n",
    "        # ---- 3) 统一走 Conv2：d_model//2 -> n_alleles-1 ----\n",
    "        y_final = F.conv1d(fused.transpose(1, 2), self.w2, self.b2, padding=self.p)\n",
    "        out[..., idx] = y_final\n",
    "        return F.softmax(out.transpose(1, 2), dim=-1)\n",
    "\n",
    "    # ---------- 辅助 ----------\n",
    "    def _band_conv1(self, x, w, b):\n",
    "        return F.gelu(F.conv1d(x, w, b, padding=self.p))\n",
    "\n",
    "class EvoFill(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_alleles: int,\n",
    "        total_sites: int,\n",
    "        chunk_size: int = 8192,\n",
    "        chunk_overlap: int = 64,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_alleles = n_alleles\n",
    "        self.total_sites = total_sites\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "        # 1. chunk 边界\n",
    "        stride = chunk_size - chunk_overlap\n",
    "        starts = [i * stride for i in range((total_sites - 1) // stride + 1)]\n",
    "        ends = [min(s + chunk_size, total_sites) for s in starts]\n",
    "        self.register_buffer(\"starts\", torch.tensor(starts, dtype=torch.long))\n",
    "        self.register_buffer(\"ends\", torch.tensor(ends, dtype=torch.long))\n",
    "        self.n_chunks = len(starts)\n",
    "\n",
    "        # 2. 每 chunk 一份嵌入 & 处理模块（常驻 GPU，但训练时只激活一个）\n",
    "        self.chunk_embeds = nn.ModuleList(\n",
    "            GenoEmbedding(n_alleles, e - s, d_model) for s, e in zip(starts, ends)\n",
    "        )\n",
    "        self.chunk_modules = nn.ModuleList(\n",
    "            ChunkModule(d_model, dropout_rate) for s, e in zip(starts, ends)\n",
    "        )\n",
    "\n",
    "        # 3. 全局输出层\n",
    "        self.global_out = GlobalOut(d_model, n_alleles, total_sites, chunk_size)\n",
    "\n",
    "        # 4. chunk 掩码表  (n_chunks, L)\n",
    "        masks = torch.stack(\n",
    "            [torch.arange(total_sites).ge(s) & torch.arange(total_sites).lt(e)\n",
    "             for s, e in zip(starts, ends)]\n",
    "        ).float()\n",
    "        self.register_buffer(\"chunk_masks\", masks)\n",
    "\n",
    "    def forward(self,\n",
    "            x: torch.Tensor,                 # (B, L, n_alleles) one-hot\n",
    "            chunk_id: Union[int, List[int]],\n",
    "            x_extra: Optional[torch.Tensor] = None\n",
    "            ):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "        if x_extra is not None and x_extra.shape[0] != batch_size:\n",
    "            x_extra = None\n",
    "\n",
    "        # 统一成 list\n",
    "        if isinstance(chunk_id, int):\n",
    "            mask = self.chunk_masks[chunk_id].bool()          # 单 chunk\n",
    "            chunk_id = [chunk_id]\n",
    "        else:\n",
    "            mask = self.chunk_masks[chunk_id].sum(dim=0).bool()  # 多 chunk 并集\n",
    "\n",
    "        z_acc   = torch.zeros(batch_size, self.total_sites, 2 * self.d_model, device=device)\n",
    "        cnt_acc = torch.zeros(self.total_sites, device=device)\n",
    "\n",
    "        # 1. 依次处理每个cid\n",
    "        for cid in chunk_id:\n",
    "            s, e = self.starts[cid].item(), self.ends[cid].item()\n",
    "            x_chunk = x[:, s:e]\n",
    "            z = self.chunk_embeds[cid](x_chunk)                    # (B, len, d_model)\n",
    "            z = self.chunk_modules[cid](z, x_extra)                # (B, len, 2*d_model)\n",
    "            z_acc[:, s:e] += z\n",
    "            cnt_acc[s:e]  += 1\n",
    "\n",
    "        # 2. 重叠平均\n",
    "        cnt_acc = cnt_acc.clamp(min=1)\n",
    "        z_full  = z_acc / cnt_acc.unsqueeze(0).unsqueeze(-1)     # (B, L, 2*d_model)\n",
    "\n",
    "        # 3. 全局输出\n",
    "        out  = self.global_out(z_full, mask)                     # (B, L, n_alleles-1)\n",
    "\n",
    "        # 4. 返回并集区域\n",
    "        # return out[:, torch.where(mask)[0]]\n",
    "        return out, torch.where(mask)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f7cea",
   "metadata": {},
   "source": [
    "假数据测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f053a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: torch.Size([8, 12345, 3])\n",
      "x_extra: torch.Size([8, 10])\n",
      "y_train: torch.Size([8, 12345, 2])\n",
      "\n",
      "model chunks: 4\n",
      "torch.Size([8, 12345, 2])\n",
      "torch.Size([8, 12345, 2])\n"
     ]
    }
   ],
   "source": [
    "B, L, A = 8, 12345, 3\n",
    "d_model = 64\n",
    "chunk_size, overlap = 4096, 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x_train = torch.zeros(B, L, A, device=device)\n",
    "allele = torch.randint(0, A, (B, L), device=device)\n",
    "x_train.scatter_(2, allele.unsqueeze(-1), 1)\n",
    "x_extra = torch.randn(B, 10, device=device)\n",
    "y_train = torch.randn(B, L, A-1, device=device)\n",
    "\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_extra: {x_extra.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(\"\")\n",
    "# ---------- 模型 &损失 ----------\n",
    "model = EvoFill(d_model, A, L, chunk_size, overlap).to(device)\n",
    "print(f\"model chunks: {model.n_chunks}\")\n",
    "\n",
    "print(\"单 chunk 测试\")\n",
    "cid = 0\n",
    "model.global_out.set_ulr_enabled(False)\n",
    "pred, mask_idx = model(x_train, cid, x_extra)\n",
    "print(pred.shape)\n",
    "\n",
    "print(\"多 chunk 测试\")\n",
    "cids= [0,2]\n",
    "model.global_out.set_ulr_enabled(True)\n",
    "pred, mask_idx = model(x_train, cid, x_extra)\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8c794",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f6f2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputationLoss(nn.Module):\n",
    "    \"\"\"Custom loss function for genomic imputation\"\"\"\n",
    "\n",
    "    def __init__(self, use_r2=True):\n",
    "        super().__init__()\n",
    "        self.use_r2_loss = use_r2\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "    def calculate_minimac_r2(self, pred_alt_allele_probs, gt_alt_af):\n",
    "        \"\"\"Calculate Minimac-style RÂ² metric\"\"\"\n",
    "        mask = torch.logical_or(torch.eq(gt_alt_af, 0.0), torch.eq(gt_alt_af, 1.0))\n",
    "        gt_alt_af = torch.where(mask, 0.5, gt_alt_af)\n",
    "        denom = gt_alt_af * (1.0 - gt_alt_af)\n",
    "        denom = torch.where(denom < 0.01, 0.01, denom)\n",
    "        r2 = torch.mean(torch.square(pred_alt_allele_probs - gt_alt_af), dim=0) / denom\n",
    "        r2 = torch.where(mask, torch.zeros_like(r2), r2)\n",
    "        return r2\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        # Convert to proper format for losses\n",
    "        y_true_ce = torch.argmax(y_true, dim=-1)  # For CrossEntropy\n",
    "        y_pred_log = torch.log(y_pred + 1e-8)  # For KL divergence\n",
    "\n",
    "        # Basic losses\n",
    "        ce_loss = self.ce_loss(y_pred.view(-1, y_pred.size(-1)), y_true_ce.view(-1))\n",
    "        kl_loss = self.kl_loss(y_pred_log.view(-1, y_pred.size(-1)),\n",
    "                               y_true.view(-1, y_true.size(-1)))\n",
    "\n",
    "        total_loss = ce_loss + kl_loss\n",
    "\n",
    "        if self.use_r2_loss:\n",
    "            batch_size = y_true.size(0)\n",
    "            group_size = 4\n",
    "            num_full_groups = batch_size // group_size\n",
    "\n",
    "            if num_full_groups > 0:\n",
    "                y_true_grouped = y_true[:num_full_groups * group_size].view(\n",
    "                    num_full_groups, group_size, *y_true.shape[1:])\n",
    "                y_pred_grouped = y_pred[:num_full_groups * group_size].view(\n",
    "                    num_full_groups, group_size, *y_pred.shape[1:])\n",
    "\n",
    "                r2_loss = 0.0\n",
    "                for i in range(num_full_groups):\n",
    "                    gt_alt_af = torch.count_nonzero(\n",
    "                        torch.argmax(y_true_grouped[i], dim=-1), dim=0\n",
    "                    ).float() / group_size\n",
    "\n",
    "                    pred_alt_allele_probs = torch.sum(y_pred_grouped[i][:, :, 1:], dim=-1)\n",
    "                    r2_loss += -torch.sum(self.calculate_minimac_r2(\n",
    "                        pred_alt_allele_probs, gt_alt_af)) * group_size\n",
    "\n",
    "                total_loss += r2_loss\n",
    "\n",
    "        return total_loss, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0ff63",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6b0e7",
   "metadata": {},
   "source": [
    "工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb648d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)                # Python 内置 random 模块\n",
    "    np.random.seed(seed)             # NumPy\n",
    "    torch.manual_seed(seed)          # PyTorch 的 CPU 和 CUDA 的通用随机种子\n",
    "    torch.cuda.manual_seed(seed)     # 当前 GPU\n",
    "    torch.cuda.manual_seed_all(seed) # 所有 GPU（多卡训练时）\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def create_directories(save_dir, models_dir=\"models\", outputs=\"out\") -> None:\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    for dd in [save_dir, f\"{save_dir}/{models_dir}\", f\"{save_dir}/{outputs}\"]:\n",
    "        if not os.path.exists(dd):\n",
    "            os.makedirs(dd)\n",
    "\n",
    "def clear_dir(path) -> None:\n",
    "    \"\"\"Clear directory if it exists\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "def precompute_maf(gts_np, mask_int=-1):\n",
    "    \"\"\"\n",
    "    gts_np: (N, L)  int64\n",
    "    return:\n",
    "        maf: (L,) float32\n",
    "        bin_cnt: list[int] 长度 6，对应 6 个 bin 的位点数量\n",
    "    \"\"\"\n",
    "    L = gts_np.shape[1]\n",
    "    maf = np.zeros(L, dtype=np.float32)\n",
    "    bin_cnt = [0] * 6\n",
    "\n",
    "    for l in range(L):\n",
    "        alleles = gts_np[:, l]\n",
    "        alleles = alleles[alleles != mask_int]   # 去掉缺失\n",
    "        if alleles.size == 0:\n",
    "            maf[l] = 0.0\n",
    "            continue\n",
    "\n",
    "        uniq, cnt = np.unique(alleles, return_counts=True)\n",
    "        total = cnt.sum()\n",
    "        freq = cnt / total\n",
    "        freq[::-1].sort()\n",
    "        maf_val = freq[1] if len(freq) > 1 else 0.0\n",
    "        maf[l] = maf_val\n",
    "\n",
    "        # 统计 bin\n",
    "        for i, (lo, hi) in enumerate(MAF_BINS):\n",
    "            if lo <= maf_val < hi:\n",
    "                bin_cnt[i] += 1\n",
    "                break\n",
    "\n",
    "    return maf, bin_cnt\n",
    "\n",
    "def build_geno3_map_from_hapmap(hap_map: dict) -> np.ndarray:\n",
    "    sorted_items = sorted(hap_map.items(), key=lambda kv: kv[1])\n",
    "    three_class = []\n",
    "    for gt, idx in sorted_items:\n",
    "        if gt in ('.|.', './.'):\n",
    "            continue\n",
    "        sep = '|' if '|' in gt else ('/' if '/' in gt else None)\n",
    "        a, b = (gt.split(sep) if sep else (gt, gt))\n",
    "        try:\n",
    "            ai, bi = int(a), int(b)\n",
    "        except Exception:\n",
    "            three_class.append(1); continue\n",
    "        if ai == bi == 0:\n",
    "            three_class.append(0)\n",
    "        elif ai != bi:\n",
    "            three_class.append(1)\n",
    "        else:\n",
    "            three_class.append(2)\n",
    "    return np.array(three_class, dtype=np.int64)\n",
    "\n",
    "# ---------- 2. 线程安全缓存 ----------\n",
    "MAF_BINS = [(0.00, 0.05), (0.05, 0.10), (0.10, 0.20),\n",
    "            (0.20, 0.30), (0.30, 0.40), (0.40, 0.50)]\n",
    "_GENO3_CACHE: Dict[int, torch.Tensor] = {}\n",
    "_GENO3_LOCK = torch.multiprocessing.Lock()\n",
    "\n",
    "def get_geno3_map_tensor(C_orig: int, hap_map, device: torch.device) -> torch.Tensor:\n",
    "    key = int(C_orig)\n",
    "    with _GENO3_LOCK:\n",
    "        t = _GENO3_CACHE.get(key)\n",
    "        if t is None:\n",
    "            arr = build_geno3_map_from_hapmap(hap_map)  # 假设 gt_enc 已全局可见\n",
    "            if arr.shape[0] != C_orig:\n",
    "                raise RuntimeError(f\"三分类映射长度{arr.shape[0]}与类别数{C_orig}不符\")\n",
    "            t = torch.from_numpy(arr)\n",
    "            _GENO3_CACHE[key] = t\n",
    "    return t.to(device)\n",
    "\n",
    "# ---------- 3. 三分类聚合 ----------\n",
    "def aggregate_three_classes(prob: torch.Tensor, y_true: torch.Tensor, hap_map) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    N, L, C = prob.shape\n",
    "    device = prob.device\n",
    "    gmap = get_geno3_map_tensor(C,hap_map, device)\n",
    "    W = torch.zeros(C, 3, device=device)\n",
    "    W[torch.arange(C, device=device), gmap.long()] = 1.0\n",
    "    prob3 = torch.einsum('nlc,ck->nlk', prob, W)\n",
    "    y3    = torch.einsum('nlc,ck->nlk', y_true, W)\n",
    "    prob3 = prob3 / prob3.sum(-1, keepdim=True).clamp(min=1e-8)\n",
    "    return prob3, y3\n",
    "\n",
    "# ---------- 4. 向量化计算 3 个指标 ----------\n",
    "def _compute_site_metrics(prob3: torch.Tensor,\n",
    "                          y3: torch.Tensor,\n",
    "                          mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    一次性返回 (INFO, MaCH-Rsq, IQS) 三个 (L,) 向量\n",
    "    prob3/y3: (N,L,3)  mask: (N,L)\n",
    "    \"\"\"\n",
    "    # dosage / W / p_alt\n",
    "    p_ref, p_het, p_hom = prob3.unbind(-1)\n",
    "    dosage = p_het + 2*p_hom\n",
    "    W_score = p_het + 4*p_hom\n",
    "\n",
    "    # 按位点求平均\n",
    "    n_valid = mask.sum(0)                        # (L,)\n",
    "    AF = 0.5 * (dosage * mask).sum(0) / n_valid.clamp(min=1)\n",
    "    denom_info = AF * (1 - AF)\n",
    "\n",
    "    # INFO\n",
    "    var_want = ((W_score - dosage.square()) * mask).sum(0) / n_valid.clamp(min=1)\n",
    "    info = 1 - 0.5 * var_want / denom_info.clamp(min=1e-8)\n",
    "    info = info.clamp(0, 1)\n",
    "\n",
    "    # MaCH-Rsq\n",
    "    # 真实剂量\n",
    "    true_dosage = (y3[..., 1] + 2*y3[..., 2]).float()        # (N,L)\n",
    "    # 预测剂量\n",
    "    pred_dosage = dosage                                       # (N,L) 前面已算好\n",
    "    # 有效样本均值\n",
    "    mean_true = (true_dosage * mask).sum(0) / n_valid.clamp(min=1)\n",
    "    mean_pred = (pred_dosage * mask).sum(0) / n_valid.clamp(min=1)\n",
    "\n",
    "    # 分子：协方差（= 预测对真实解释的方差）\n",
    "    num = ((pred_dosage - mean_pred.unsqueeze(0)) *\n",
    "           (true_dosage - mean_true.unsqueeze(0)) * mask).sum(0) / n_valid.clamp(min=1)\n",
    "\n",
    "    # 分母：由真实剂量得到的 AF*(1-AF)\n",
    "    AF = mean_true / 2.0\n",
    "    denom = AF * (1 - AF)\n",
    "    mach = num / denom.clamp(min=1e-8)\n",
    "    mach = mach.clamp(0, 1)\n",
    "\n",
    "    # IQS (Cohen's kappa)\n",
    "    pred_cls = prob3.argmax(-1)                  # (N,L)\n",
    "    true_cls = y3.argmax(-1)\n",
    "    agree = (pred_cls == true_cls) & mask        # (N,L)\n",
    "    Po = (agree.sum(0)).float() / n_valid.clamp(min=1)\n",
    "    Pe = torch.zeros_like(Po)\n",
    "    for c in range(3):\n",
    "        p_c = ((pred_cls == c) & mask).sum(0).float() / n_valid.clamp(min=1)\n",
    "        t_c = ((true_cls == c) & mask).sum(0).float() / n_valid.clamp(min=1)\n",
    "        Pe += p_c * t_c\n",
    "    iqs = (Po - Pe) / (1 - Pe).clamp(min=1e-8)\n",
    "    iqs = iqs.clamp(-1, 1)\n",
    "\n",
    "    # 无效位点填 0\n",
    "    invalid = n_valid == 0\n",
    "    info[invalid] = 0\n",
    "    mach[invalid] = 0\n",
    "    iqs[invalid]  = 0\n",
    "    return info, mach, iqs\n",
    "\n",
    "# ---------- 5. 唯一对外接口 ----------\n",
    "def metrics_by_maf(prob: torch.Tensor,\n",
    "                   y_true: torch.Tensor,\n",
    "                   hap_map,\n",
    "                   maf_vec: torch.Tensor,\n",
    "                   bins: List[Tuple[float, float]] = MAF_BINS,\n",
    "                   mask: Optional[torch.Tensor] = None\n",
    "                   ) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    返回 dict: {'Acc':[...], 'INFO':[...], 'MaCH':[...], 'IQS':[...]}\n",
    "    顺序与 bins 一致\n",
    "    \"\"\"\n",
    "    N, L, _ = prob.shape\n",
    "    device = prob.device\n",
    "    if mask is None:\n",
    "        mask = torch.ones((N, L), dtype=torch.bool, device=device)\n",
    "\n",
    "    # 三分类\n",
    "    prob3, y3 = aggregate_three_classes(prob, y_true, hap_map)\n",
    "\n",
    "    # --- 5.1 accuracy 向量化 ---\n",
    "    preds = prob3.argmax(-1)\n",
    "    gts   = y3.argmax(-1)\n",
    "    correct = (preds == gts) & mask                      # (N,L)\n",
    "    maf_b = maf_vec.unsqueeze(0)                         # (1,L)\n",
    "    acc_bins = []\n",
    "    for lo, hi in bins:\n",
    "        mbin = mask & (maf_b >= lo) & (maf_b < hi)\n",
    "        n_cor = (correct & mbin).sum()\n",
    "        n_tot = mbin.sum()\n",
    "        acc_bins.append((n_cor / n_tot).item() if n_tot > 0 else 0.)\n",
    "\n",
    "    # --- 5.2 其余 3 个指标 ---\n",
    "    info_all, mach_all, iqs_all = _compute_site_metrics(prob3, y3, mask)\n",
    "    info_bins, mach_bins, iqs_bins = [], [], []\n",
    "    for lo, hi in bins:\n",
    "        idx = (maf_vec >= lo) & (maf_vec < hi)\n",
    "        if idx.sum() == 0:\n",
    "            info_bins.append(0.); mach_bins.append(0.); iqs_bins.append(0.)\n",
    "        else:\n",
    "            info_bins.append(info_all[idx].mean().item())\n",
    "            mach_bins.append(mach_all[idx].mean().item())\n",
    "            iqs_bins.append(iqs_all[idx].mean().item())\n",
    "\n",
    "    return {'Acc': acc_bins, 'INFO': info_bins,\n",
    "            'MaCH': mach_bins, 'IQS': iqs_bins}\n",
    "\n",
    "# ---------- 6. 打印 ----------\n",
    "def print_maf_stat_df(chunk_bin_cnt: List[int],\n",
    "                      train_bins_metrics: Dict[str, List[float]],\n",
    "                      val_bins_metrics: Dict[str, List[float]]):\n",
    "    maf_df = pd.DataFrame({\n",
    "        'MAF_bin': ['(0.00, 0.05)', '(0.05, 0.10)', '(0.10, 0.20)',\n",
    "                    '(0.20, 0.30)', '(0.30, 0.40)', '(0.40, 0.50)'],\n",
    "        'Counts':  [f\"{c}\" for c in chunk_bin_cnt],\n",
    "        'Train_Acc':   [f\"{v:.3f}\" for v in train_bins_metrics['Acc']],\n",
    "        'Val_Acc':     [f\"{v:.3f}\" for v in val_bins_metrics['Acc']],\n",
    "        'Train_INFO':  [f\"{v:.3f}\" for v in train_bins_metrics['INFO']],\n",
    "        'Val_INFO':    [f\"{v:.3f}\" for v in val_bins_metrics['INFO']],\n",
    "        'Train_MaCH':  [f\"{v:.3f}\" for v in train_bins_metrics['MaCH']],\n",
    "        'Val_MaCH':    [f\"{v:.3f}\" for v in val_bins_metrics['MaCH']],\n",
    "        'Train_IQS':   [f\"{v:.3f}\" for v in train_bins_metrics['IQS']],\n",
    "        'Val_IQS':     [f\"{v:.3f}\" for v in val_bins_metrics['IQS']],\n",
    "    })\n",
    "    print(maf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac5b35",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d21fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Dir: /mnt/qmtang/EvoFill/data/251027_ver3_chr22_trim\n",
      "Using device: cuda\n",
      "2,404 samples, 99,314 variants, 4 seq-depth.\n",
      "2,276 samples in train\n",
      "128 samples in val\n"
     ]
    }
   ],
   "source": [
    "# work_dir = \"/mnt/qmtang/EvoFill/data/251027_ver3_chr22/\"\n",
    "work_dir = '/mnt/qmtang/EvoFill/data/251027_ver3_chr22_trim'\n",
    "print(f\"Work Dir: {work_dir}\")\n",
    "create_directories(work_dir)\n",
    "\n",
    "val_n_samples = 128\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "gt_enc = GenotypeEncoder.loadfromdisk(work_dir)\n",
    "print(f'{gt_enc.n_samples:,} samples, {gt_enc.n_variants:,} variants, {gt_enc.seq_depth} seq-depth.')\n",
    "\n",
    "x_train_indices, x_valid_indices = train_test_split(\n",
    "    range(gt_enc.n_samples),\n",
    "    test_size=val_n_samples,\n",
    "    random_state=3047,\n",
    "    shuffle=True\n",
    ")\n",
    "print(f\"{len(x_train_indices):,} samples in train\")\n",
    "print(f\"{len(x_valid_indices):,} samples in val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1c35c",
   "metadata": {},
   "source": [
    "init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "097171d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model[hg19_chr22_trim] would have 4 chunks.\n"
     ]
    }
   ],
   "source": [
    "model_name  = 'hg19_chr22_trim'\n",
    "total_sites = gt_enc.n_variants\n",
    "alleles     = gt_enc.seq_depth\n",
    "chunk_size  = 32768\n",
    "overlap     = 1024\n",
    "d_model     = 64\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model = EvoFill(d_model, alleles, total_sites, chunk_size, overlap).to(device)\n",
    "print(f\"model[{model_name}] would have {model.n_chunks} chunks.\")\n",
    "\n",
    "criterion = ImputationLoss(use_r2=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececdaa7",
   "metadata": {},
   "source": [
    "### STAGE 1: Chunk Module Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 1/100, Train Loss: 72410.7, Val Loss: 71993.1, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 2/100, Train Loss: 69847.0, Val Loss: 72826.6, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 3/100, Train Loss: 71025.5, Val Loss: 69634.1, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 4/100, Train Loss: 70020.6, Val Loss: 74399.9, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 5/100, Train Loss: 68704.4, Val Loss: 69484.2, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 6/100, Train Loss: 68544.6, Val Loss: 67193.0, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 7/100, Train Loss: 68802.1, Val Loss: 68758.5, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 8/100, Train Loss: 68900.9, Val Loss: 69451.4, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 9/100, Train Loss: 67918.7, Val Loss: 67256.9, LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 10/100, Train Loss: 67859.7, Val Loss: 68598.2, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 11/100, Train Loss: 65436.2, Val Loss: 64980.8, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 12/100, Train Loss: 64971.0, Val Loss: 64206.6, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 13/100, Train Loss: 64833.6, Val Loss: 64920.5, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 14/100, Train Loss: 64495.2, Val Loss: 66000.7, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 15/100, Train Loss: 64730.5, Val Loss: 64204.8, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 16/100, Train Loss: 64565.5, Val Loss: 62841.1, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 17/100, Train Loss: 64014.0, Val Loss: 64050.9, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 18/100, Train Loss: 64851.4, Val Loss: 63889.1, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 19/100, Train Loss: 64235.0, Val Loss: 65281.4, LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/4, Epoch 20/100:  39%|███▊      | 110/285 [00:16<00:25,  6.93it/s, loss=6.17e+4]"
     ]
    }
   ],
   "source": [
    "model.global_out.set_ulr_enabled(False)\n",
    "\n",
    "batch_size         = 8\n",
    "max_epochs         = 100\n",
    "lr                 = 0.001\n",
    "weight_decay       = 1e-5\n",
    "earlystop_patience = 13\n",
    "max_mr             = 0.7\n",
    "min_mr             = 0.3\n",
    "verbose            = False\n",
    "\n",
    "train_dataset = GenomicDataset(\n",
    "    gt_enc.X_gt,\n",
    "    x_extra=gt_enc.X_extra,\n",
    "    seq_depth=gt_enc.seq_depth,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_train_indices\n",
    ")\n",
    "\n",
    "val_dataset = GenomicDataset(\n",
    "    gt_enc.X_gt,\n",
    "    x_extra=gt_enc.X_extra,\n",
    "    seq_depth=gt_enc.seq_depth,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_valid_indices\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "for cid in range(model.n_chunks):\n",
    "    chunk_mask = model.chunk_masks[cid].cpu()\n",
    "    chunk_maf, chunk_bin_cnt = precompute_maf(gt_enc.X_gt[:,chunk_mask.bool().cpu().numpy()].toarray(),  mask_int=gt_enc.seq_depth)\n",
    "    chunk_maf = torch.from_numpy(chunk_maf).to(device)\n",
    "    if verbose:\n",
    "        print(f\"=== Chunk {cid + 1} STAT ===\")\n",
    "        maf_df = pd.DataFrame({\n",
    "            'MAF_bin': ['(0.00, 0.05)', '(0.05, 0.10)', '(0.10, 0.20)',\n",
    "                        '(0.20, 0.30)', '(0.30, 0.40)', '(0.40, 0.50)'],\n",
    "            'Counts':  [f\"{c}\" for c in chunk_bin_cnt],\n",
    "        })\n",
    "        print(maf_df.to_string(index=False))\n",
    "\n",
    "    # 2. 只给当前chunk专家+GlobalOut局部卷积上优化器\n",
    "    trainable = (list(model.chunk_embeds[cid].parameters()) +\n",
    "                list(model.chunk_modules[cid].parameters()) +\n",
    "                [model.global_out.w1, model.global_out.b1,\n",
    "                model.global_out.w2, model.global_out.b2])\n",
    "    optimizer = AdamW(trainable, lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-8)\n",
    "    best_loss = float('inf')\n",
    "    patience = earlystop_patience\n",
    "    patience_counter = 0\n",
    "    is_early_stopped = False\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_logits, train_gts, train_mask = [], [], []\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f'Chunk {cid + 1}/{model.n_chunks}, Epoch {epoch + 1}/{max_epochs}', leave=False)\n",
    "        for batch_idx, (x, x_extra, target) in enumerate(train_pbar):\n",
    "            x,  target = x.to(device), target.to(device)\n",
    "            if x_extra.numel() == 0:\n",
    "                x_extra = None\n",
    "            else:\n",
    "                x_extra = x_extra.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred, mask_idx = model(x, cid, x_extra)\n",
    "            loss, logs = criterion(pred[:, mask_idx], target[:,mask_idx]) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            # === 收集训练结果 ===\n",
    "            miss_mask = x[:, mask_idx][..., -1].bool()         # 只关心被 mask 的位点\n",
    "            train_logits.append(pred[:, mask_idx].detach())\n",
    "            train_gts.append(target[:,mask_idx].detach())\n",
    "            train_mask.append(miss_mask)\n",
    "\n",
    "        # 训练集 MAF-acc\n",
    "        train_logits = torch.cat(train_logits, dim=0)\n",
    "        train_gts    = torch.cat(train_gts,    dim=0)\n",
    "        train_mask   = torch.cat(train_mask,   dim=0)\n",
    "\n",
    "        # ----------- 验证循环同理 ------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_logits, val_gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, x_extra, target in val_loader:\n",
    "                x,  target = x.to(device), target.to(device)\n",
    "                if x_extra.numel() == 0:\n",
    "                    x_extra = None\n",
    "                else:\n",
    "                    x_extra = x_extra.to(device)\n",
    "                pred, mask_idx = model(x, cid, x_extra)\n",
    "                loss, logs = criterion(pred[:, mask_idx], target[:,mask_idx]) \n",
    "                val_loss += loss.item()\n",
    "                val_logits.append(pred[:, mask_idx].detach())\n",
    "                val_gts.append(target[:,mask_idx].detach())\n",
    "\n",
    "        val_logits = torch.cat(val_logits, dim=0)\n",
    "        val_gts    = torch.cat(val_gts,    dim=0)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss   = val_loss   / len(val_loader)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f'Chunk {cid + 1}/{model.n_chunks}, '\n",
    "            f'Epoch {epoch + 1}/{max_epochs}, '\n",
    "            f'Train Loss: {avg_train_loss:.1f}, '\n",
    "            f'Val Loss: {avg_val_loss:.1f}, '\n",
    "            f'LR: {current_lr:.2e}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # 只存当前 chunk 专家 + 全局层\n",
    "            ckpt = {\n",
    "                'chunk_id': cid,\n",
    "                'chunk_embed_state': model.chunk_embeds[cid].state_dict(),\n",
    "                'chunk_module_state': model.chunk_modules[cid].state_dict(),\n",
    "                'global_out_state': model.global_out.state_dict(),\n",
    "                'best_val_loss': best_loss,\n",
    "            }\n",
    "            torch.save(ckpt, f'{work_dir}/models/{model_name}_chunk{cid}.pth')\n",
    "            predres_with_bestloss = (train_logits, train_gts, val_logits, val_gts)\n",
    "            if verbose:\n",
    "                train_bins_metrics = metrics_by_maf(train_logits, train_gts, gt_enc.hap_map, chunk_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_logits,   val_gts, gt_enc.hap_map, chunk_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,train_bins_metrics,val_bins_metrics)\n",
    "                print(f'  --> updated {model_name}_chunk{cid}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= earlystop_patience:\n",
    "                is_early_stopped = True\n",
    "                print(f'Chunk {cid + 1}/{model.n_chunks}, Early stopping triggered')\n",
    "                train_logits, train_gts, val_logits, val_gts = predres_with_bestloss\n",
    "                train_bins_metrics = metrics_by_maf(train_logits, train_gts, gt_enc.hap_map, chunk_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_logits,   val_gts, gt_enc.hap_map, chunk_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,train_bins_metrics,val_bins_metrics)\n",
    "                break\n",
    "\n",
    "    if not is_early_stopped:\n",
    "        predres_with_bestloss = (train_logits, train_gts, val_logits, val_gts)\n",
    "        train_bins_metrics = metrics_by_maf(train_logits, train_gts, gt_enc.hap_map, chunk_maf, mask=train_mask)\n",
    "        val_bins_metrics   = metrics_by_maf(val_logits,   val_gts, gt_enc.hap_map, chunk_maf, mask=None)\n",
    "        print_maf_stat_df(chunk_bin_cnt,train_bins_metrics,val_bins_metrics)\n",
    "    del optimizer, scheduler\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# ---------------- 全部 chunk 训练完成 -> 保存完整模型 ----------------\n",
    "final_ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'n_chunks': model.n_chunks,\n",
    "    'chunk_size': model.chunk_size,\n",
    "    'chunk_overlap': model.chunk_overlap,\n",
    "}\n",
    "torch.save(final_ckpt, f'{work_dir}/models/{model_name}_stage1.pth')\n",
    "print(f'==> STAGE1 (Chunk Module) training finished: {work_dir}/models/{model_name}_stage1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ee3c7",
   "metadata": {},
   "source": [
    "### STAGE 2: Ultra-Long-Range LD Module Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3924469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'imputation_maf_accuracy_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 108\u001b[0m\n",
      "\u001b[1;32m    106\u001b[0m train_gts    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(train_gts,    dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;32m    107\u001b[0m train_mask   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(train_mask,   dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;32m--> 108\u001b[0m train_maf_accs \u001b[38;5;241m=\u001b[39m \u001b[43mimputation_maf_accuracy_epoch\u001b[49m(train_logits, train_gts,\n",
      "\u001b[1;32m    109\u001b[0m                                                union_maf, mask\u001b[38;5;241m=\u001b[39mtrain_mask)\n",
      "\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# ----------- 验证 -----------\u001b[39;00m\n",
      "\u001b[1;32m    112\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imputation_maf_accuracy_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# ============ 超参 ============\n",
    "max_epochs_per_pair = 10\n",
    "lr                 = 5e-4\n",
    "weight_decay       = 1e-5\n",
    "earlystop_patience = 15\n",
    "batch_size         = 4\n",
    "min_mr, max_mr     = 0.4, 0.6\n",
    "verbose            = True\n",
    "# ==============================\n",
    "\n",
    "train_dataset = GenomicDataset(\n",
    "    gt_enc.X_gt,\n",
    "    x_extra=gt_enc.X_extra,\n",
    "    seq_depth=gt_enc.seq_depth,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_train_indices\n",
    ")\n",
    "\n",
    "val_dataset = GenomicDataset(\n",
    "    gt_enc.X_gt,\n",
    "    x_extra=gt_enc.X_extra,\n",
    "    seq_depth=gt_enc.seq_depth,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_valid_indices\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# ----------- 加载已完成的第一阶段模型 -----------\n",
    "ckpt = torch.load(f'{work_dir}/models/{model_name}_stage1.pth', map_location='cpu')\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "model.global_out.set_ulr_enabled(True)          # 只开 ulr 分支\n",
    "model.eval()        # chunk 专家冻结（requires_grad=False）\n",
    "\n",
    "# 分离优化器\n",
    "ulr_params = list(model.global_out.ulr_mamba.parameters()) + \\\n",
    "             list(model.global_out.gate.parameters()) + \\\n",
    "             list(model.global_out.norm.parameters())          # 如有\n",
    "optimizer = AdamW(ulr_params, lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
    "                              patience=5, min_lr=1e-9)\n",
    "\n",
    "\n",
    "pair_list = list(combinations(range(model.n_chunks), 2))\n",
    "np.random.shuffle(pair_list)          # 打乱\n",
    "total_pairs = len(pair_list)\n",
    "\n",
    "for pair_idx, (cid1, cid2) in enumerate(pair_list, 1):\n",
    "    # ====== 构造并集 mask ======\n",
    "    union_mask = (model.chunk_masks[cid1] + model.chunk_masks[cid2]).clamp(max=1).bool()\n",
    "\n",
    "    # 并集 MAF\n",
    "    union_maf, union_bin_cnt = precompute_maf(\n",
    "        gt_enc.X_gt[:, union_mask.cpu().numpy()].toarray(),\n",
    "        mask_int=gt_enc.seq_depth\n",
    "    )\n",
    "    union_maf = torch.from_numpy(union_maf).to(device)\n",
    "\n",
    "    # ====== 早停变量 ======\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    is_early_stopped = False\n",
    "\n",
    "    # ====== 训练循环 ======\n",
    "    for epoch in range(max_epochs_per_pair):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_logits, train_gts, train_mask = [], [], []\n",
    "\n",
    "        pbar = tqdm(train_loader,\n",
    "                    desc=f'Pair {pair_idx}/{total_pairs}  '\n",
    "                         f'{cid1}-{cid2}  Epoch {epoch+1}/{max_epochs_per_pair}',\n",
    "                    leave=False)\n",
    "        for x, x_extra, target in pbar:\n",
    "            x = x.to(device)\n",
    "            target = target.to(device)\n",
    "            if x_extra.numel() == 0:\n",
    "                x_extra = None\n",
    "            else:\n",
    "                x_extra = x_extra.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred, mask_idx = model(x, [cid1, cid2], x_extra)\n",
    "            loss, _ = criterion(pred[:,mask_idx], target[:, mask_idx])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            # 收集指标\n",
    "            miss_mask = x[:,union_mask][..., -1].bool()\n",
    "            train_logits.append(pred[:, mask_idx].detach())\n",
    "            train_gts.append(target[:,mask_idx].detach())\n",
    "            train_mask.append(miss_mask)\n",
    "\n",
    "        # 训练集 MAF\n",
    "        train_logits = torch.cat(train_logits, dim=0)\n",
    "        train_gts    = torch.cat(train_gts,    dim=0)\n",
    "        train_mask   = torch.cat(train_mask,   dim=0)\n",
    "\n",
    "        # ----------- 验证 -----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_logits, val_gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, x_extra, target in val_loader:\n",
    "                x = x.to(device)\n",
    "                target = target.to(device)\n",
    "                x_extra = x_extra.to(device) if x_extra.numel() else None\n",
    "                pred, mask_idx = model(x, [cid1, cid2], x_extra)\n",
    "                loss, _ = criterion(pred[:,mask_idx], target[:,mask_idx])\n",
    "                val_loss += loss.item()\n",
    "                val_logits.append(pred[:,mask_idx])\n",
    "                val_gts.append(target[:,mask_idx])\n",
    "\n",
    "        val_logits = torch.cat(val_logits, dim=0)\n",
    "        val_gts    = torch.cat(val_gts,    dim=0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss   = val_loss   / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # 早停\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'pair': (cid1, cid2),\n",
    "                'ulr_state': {\n",
    "                    'ulr_mamba': model.global_out.ulr_mamba.state_dict(),\n",
    "                    'gate':      model.global_out.gate.state_dict(),\n",
    "                },\n",
    "                'best_val_loss': best_loss,\n",
    "                'epoch': epoch,\n",
    "            }, f'{work_dir}/models/{model_name}_ulr_ld_{cid1}_{cid2}.pth')\n",
    "            # MAF 表格\n",
    "            predres_with_bestloss = (train_logits, train_gts, val_logits, val_gts)\n",
    "            if verbose:\n",
    "                train_bins_metrics = metrics_by_maf(train_logits, train_gts, gt_enc.hap_map, chunk_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_logits,   val_gts, gt_enc.hap_map, chunk_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,train_bins_metrics,val_bins_metrics)\n",
    "                print(f'  --> updated ulr_ld_{cid1}_{cid2}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= earlystop_patience:\n",
    "                is_early_stopped = True\n",
    "                print(f'Pair {cid1}-{cid2} early stopping')\n",
    "                train_logits, train_gts, val_logits, val_gts = predres_with_bestloss\n",
    "                train_bins_metrics = metrics_by_maf(train_logits, train_gts, gt_enc.hap_map, chunk_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_logits,   val_gts, gt_enc.hap_map, chunk_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,train_bins_metrics,val_bins_metrics)\n",
    "                break\n",
    "            \n",
    "    if not is_early_stopped:\n",
    "        predres_with_bestloss = (train_logits, train_gts, val_logits, val_gts)\n",
    "        train_bins_metrics = metrics_by_maf(train_logits, train_gts, gt_enc.hap_map, chunk_maf, mask=train_mask)\n",
    "        val_bins_metrics   = metrics_by_maf(val_logits,   val_gts, gt_enc.hap_map, chunk_maf, mask=None)\n",
    "        print_maf_stat_df(chunk_bin_cnt,train_bins_metrics,val_bins_metrics)\n",
    "\n",
    "    # del optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ----------- 全部 pair 结束 -> 保存最终模型 -----------\n",
    "torch.save({\n",
    "    'model_state': model.state_dict(),\n",
    "    'ulr_enabled': True,\n",
    "}, f'{work_dir}/models/{model_name}_stage2_final.pth')\n",
    "print(f'==> STAGE2 (Ultra_LR-LD) training finished: {work_dir}/models/{model_name}_stage2_final.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a175c03b",
   "metadata": {},
   "source": [
    "## Inferring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93349a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1. 必须与训练时完全一致 ----------\n",
    "d_model       = 64\n",
    "n_alleles     = 3\n",
    "total_sites   = 12345\n",
    "chunk_size    = 4096\n",
    "chunk_overlap = 64\n",
    "device        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 重建模型\n",
    "model = EvoFill(\n",
    "    d_model=d_model,\n",
    "    n_alleles=n_alleles,\n",
    "    total_sites=total_sites,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ").to(device)\n",
    "\n",
    "# ---------- 2. 加载最终权重 ----------\n",
    "ckpt = torch.load('exp1/models/final_model.pth', map_location=device)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "\n",
    "# ---------- 3. 切换推理模式 ----------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(x, chunk_id=0, x_extra=x_extra)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
