{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60ccf3a",
   "metadata": {},
   "source": [
    "ver0: 多 chunk modules 独立权重\n",
    "\n",
    "ver0.1: 加样本特征标签（演化坐标）\n",
    "\n",
    "ver3: chunk-wise 稀疏激活"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bb00d",
   "metadata": {},
   "source": [
    "## Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757312f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # 设置GPU\n",
    "import gzip\n",
    "import json\n",
    "import shutil\n",
    "from typing import Optional, Tuple, Dict, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mamba_ssm import Mamba2\n",
    "from mamba_ssm.modules.mamba2_simple import Mamba2Simple as Mamba2Block # 原Mamba2Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975df815",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e956fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, ref_vcf: str, ref_extra: Optional[str] = None):\n",
    "        self.ref_vcf   = ref_vcf\n",
    "        self.ref_extra = ref_extra\n",
    "\n",
    "        # 1) 先扫一次 VCF：拿样本 ID 与「所有 GT 字符串」\n",
    "        self.sample_ids, gt_strings = self._scan_all_gt()\n",
    "        # 2) 建立映射\n",
    "        self.hap_map   = self._make_hap_map(gt_strings)\n",
    "        self.seq_depth = len(self.hap_map)        # 已含缺失位\n",
    "\n",
    "        # 3) 正式读矩阵\n",
    "        self.X_gt = self._read_body()\n",
    "        self.total_sites = self.X_gt.shape[1]\n",
    "        print(f'Loaded genotypes info: {self.X_gt.shape}')\n",
    "\n",
    "        # 4) 读 extra\n",
    "        self.X_extra = self._read_extra() if self.ref_extra else None\n",
    "        if self.X_extra is not None:\n",
    "            print(f'Loaded extra info: {self.X_extra.shape}')\n",
    "\n",
    "        # 5) 打印\n",
    "        self._print_summary()\n",
    "\n",
    "    # ---- 工具 ----\n",
    "    def _open(self): return gzip.open(self.ref_vcf, 'rt') if self.ref_vcf.endswith('.gz') else open(self.ref_vcf, 'rt')\n",
    "\n",
    "    def _scan_all_gt(self) -> Tuple[list, Set[str]]:\n",
    "        samples, gt_set = [], set()\n",
    "        with self._open() as f:\n",
    "            for line in f:\n",
    "                if line.startswith('##'): continue\n",
    "                if line.startswith('#CHROM'):\n",
    "                    samples = line.strip().split('\\t')[9:]\n",
    "                    continue\n",
    "                if not samples: continue\n",
    "                parts = line.strip().split('\\t')\n",
    "                for fmt in parts[9:]:\n",
    "                    gt = fmt.split(':')[0]\n",
    "                    gt_set.add(gt)\n",
    "        if not samples: raise RuntimeError('No sample IDs found')\n",
    "        return samples, gt_set\n",
    "\n",
    "    def _make_hap_map(self, gt_strings: Set[str]) -> Dict[str, int]:\n",
    "        \"\"\"给每个真实 GT 字符串一个唯一整数；缺失统一用 '.|.' 并强制占最后一档\"\"\"\n",
    "        cleaned = set()\n",
    "        for gt in gt_strings:\n",
    "            if '.' in gt:\n",
    "                cleaned.add('.|.')\n",
    "            else:\n",
    "                cleaned.add(gt)\n",
    "        # 强制加入缺失项（即使当前没出现）\n",
    "        cleaned.add('.|.')\n",
    "        # 排序保证一致性：缺失放最后\n",
    "        sorted_gts = sorted(cleaned, key=lambda x: (x == '.|.', x))\n",
    "        hap_map = {gt: idx for idx, gt in enumerate(sorted_gts)}\n",
    "        return hap_map\n",
    "\n",
    "    def _read_body(self) -> np.ndarray:\n",
    "        data = []\n",
    "        with self._open() as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#'): continue\n",
    "                parts = line.strip().split('\\t')\n",
    "                row = []\n",
    "                for fmt in parts[9:]:\n",
    "                    gt = fmt.split(':')[0]\n",
    "                    # 缺失处理\n",
    "                    if '.' in gt:\n",
    "                        row.append(self.hap_map['.|.'])\n",
    "                    else:\n",
    "                        row.append(self.hap_map[gt])\n",
    "                data.append(row)\n",
    "        return np.array(data, dtype=np.int32).T   # (n_samp, n_var)\n",
    "\n",
    "    def _read_extra(self) -> Optional[np.ndarray]:\n",
    "        try:\n",
    "            df = pd.read_csv(self.ref_extra, sep='\\t', index_col=0)\n",
    "            df = df.loc[self.sample_ids]\n",
    "            return df.values.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f'Extra features skipped: {e}')\n",
    "            return None\n",
    "\n",
    "    def _print_summary(self):\n",
    "        uniq_codes = sorted(set(self.X_gt.flat))\n",
    "        rev = {v: k for k, v in self.hap_map.items()}\n",
    "        print('Unique genotypes in dataset:', [rev.get(c, f'unknown({c})') for c in uniq_codes])\n",
    "        print('hap_map:')\n",
    "        for k, v in list(self.hap_map.items())[:10]:\n",
    "            print(f'  {k} -> {v}')\n",
    "        print(f'self.seq_depth: {self.seq_depth}')\n",
    "\n",
    "    # ---- 对外 ----\n",
    "    def load(self) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        return self.X_gt, self.X_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27d851db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded genotypes info: (2404, 99314)\n",
      "Loaded extra info: (2404, 26)\n",
      "Unique genotypes in dataset: ['0|0', '0|1', '1|0', '1|1']\n",
      "hap_map:\n",
      "  0|0 -> 0\n",
      "  0|1 -> 1\n",
      "  1|0 -> 2\n",
      "  1|1 -> 3\n",
      "  .|. -> 4\n",
      "self.seq_depth: 5\n"
     ]
    }
   ],
   "source": [
    "dr = DataReader(\n",
    "    ref_vcf='/home/qmtang/GitHub/STICI-HPC/data/training_sets/ALL.chr22.training.samples.100k.any.type.0.01.maf.variants.vcf.gz',\n",
    "    ref_extra='/mnt/qmtang/EvoFill/data/251020_ver01_chr22/pop_wasserstein.tsv'\n",
    ")\n",
    "X_gt, X_extra = dr.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e17b3a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3819261 , 0.6733273 , 2.54835   , ..., 1.8984295 , 1.9717863 , 3.2552247 ],\n",
       "       [0.79339385, 0.6408869 , 1.1961467 , ..., 1.4899036 , 2.3885481 , 2.7226706 ],\n",
       "       [1.1014444 , 2.3403306 , 1.5370792 , ..., 2.2961452 , 2.28726   , 1.3032436 ],\n",
       "       ...,\n",
       "       [2.9782143 , 3.4962935 , 2.4113977 , ..., 1.9876934 , 1.383686  , 1.4305575 ],\n",
       "       [1.2182752 , 1.1490815 , 1.7263185 , ..., 1.3928081 , 3.5453155 , 3.036527  ],\n",
       "       [2.8082004 , 2.8113415 , 2.583803  , ..., 2.4467747 , 1.1540115 , 1.2323742 ]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "849bffe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 2, 0, 0],\n",
       "       [2, 0, 0, ..., 0, 0, 3],\n",
       "       [0, 0, 0, ..., 0, 0, 3],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 0, 0, 2],\n",
       "       [3, 2, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 3]], dtype=int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbb96506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicDataset(Dataset):\n",
    "    \"\"\"Dataset class for genomic data with masking for training\"\"\"\n",
    "\n",
    "    def __init__(self, x_gts, x_extra, seq_depth,\n",
    "                 mask=True, masking_rates=(0.5, 0.99)):\n",
    "        self.gts = x_gts\n",
    "        self.x_extra = x_extra\n",
    "        self.seq_depth = seq_depth\n",
    "        self.mask = mask\n",
    "        self.masking_rates = masking_rates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x       = self.gts[idx].copy()\n",
    "        x_extra = self.x_extra[idx]\n",
    "        y       = self.gts[idx]\n",
    "\n",
    "        if self.mask:\n",
    "            # Apply masking\n",
    "            seq_len = len(x)\n",
    "            masking_rate = np.random.uniform(*self.masking_rates)\n",
    "            mask_size = int(seq_len * masking_rate)\n",
    "            mask_indices = np.random.choice(seq_len, mask_size, replace=False)\n",
    "            x[mask_indices] = self.seq_depth - 1  # Missing value token\n",
    "\n",
    "        # Convert to one-hot\n",
    "        x_onehot = np.eye(self.seq_depth)[x]\n",
    "        y_onehot = np.eye(self.seq_depth - 1)[y]\n",
    "\n",
    "        return torch.FloatTensor(x_onehot),torch.FloatTensor(x_extra), torch.FloatTensor(y_onehot)\n",
    "\n",
    "class ImputationDataset(Dataset):\n",
    "    \"\"\"Dataset for imputation (no masking needed)\"\"\"\n",
    "\n",
    "    def __init__(self, data, seq_depth):\n",
    "        self.data = data\n",
    "        self.seq_depth = seq_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        # Convert to one-hot without masking\n",
    "        x_onehot = np.eye(self.seq_depth)[x]\n",
    "        return torch.FloatTensor(x_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86e007",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac21e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenoEmbedding(nn.Module):\n",
    "    \"\"\"Genomic embedding layer with positional encoding\"\"\"\n",
    "\n",
    "    def __init__(self, n_alleles, n_snps, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_alleles = n_alleles\n",
    "        self.n_snps = n_snps\n",
    "\n",
    "        # Allele embedding\n",
    "        self.allele_embedding = nn.Parameter(torch.randn(n_alleles, d_model))\n",
    "\n",
    "        # Positional embedding\n",
    "        self.position_embedding = nn.Embedding(n_snps, d_model)\n",
    "\n",
    "        # Initialize parameters\n",
    "        nn.init.xavier_uniform_(self.allele_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, n_alleles) - one-hot encoded\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Allele embedding\n",
    "        embedded = torch.einsum('bsn,nd->bsd', x, self.allele_embedding)\n",
    "\n",
    "        # Positional embedding\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        pos_emb = self.position_embedding(positions).unsqueeze(0)\n",
    "\n",
    "        return embedded + pos_emb\n",
    "\n",
    "class BiMambaBlock(nn.Module):\n",
    "    \"\"\"Bidirectional Mamba block for genomic sequence processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Forward and backward Mamba blocks\n",
    "        self.mamba_forward = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "        self.mamba_backward = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        residual = x\n",
    "\n",
    "        # Bidirectional processing\n",
    "        x_norm = self.norm1(x)\n",
    "\n",
    "        # Forward direction\n",
    "        forward_out = self.mamba_forward(x_norm)\n",
    "\n",
    "        # Backward direction (flip sequence)\n",
    "        x_backward = torch.flip(x_norm, dims=[1])\n",
    "        backward_out = self.mamba_backward(x_backward)\n",
    "        backward_out = torch.flip(backward_out, dims=[1])\n",
    "\n",
    "        # Concatenate bidirectional outputs\n",
    "        bi_out = torch.cat([forward_out, backward_out], dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        ffn_out = self.ffn(bi_out)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # Residual connection\n",
    "        out = self.norm2(residual + ffn_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block for local pattern extraction\"\"\"\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.conv1 = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)\n",
    "\n",
    "        self.conv_large1 = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)\n",
    "        self.conv_large2 = nn.Conv1d(d_model, d_model, kernel_size=15, padding=7)\n",
    "\n",
    "        self.conv_final = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.conv_reduce = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(d_model)\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2)  # (batch, d_model, seq_len)\n",
    "\n",
    "        xa = self.gelu(self.conv1(x))\n",
    "\n",
    "        xb = self.gelu(self.conv2(xa))\n",
    "        xb = self.gelu(self.conv3(xb))\n",
    "\n",
    "        xc = self.gelu(self.conv_large1(xa))\n",
    "        xc = self.gelu(self.conv_large2(xc))\n",
    "\n",
    "        xa = xb + xc\n",
    "        xa = self.gelu(self.conv_final(xa))\n",
    "        xa = self.bn1(xa)\n",
    "        xa = self.gelu(self.conv_reduce(xa))\n",
    "        xa = self.bn2(xa)\n",
    "        xa = self.gelu(xa)\n",
    "\n",
    "        return xa.transpose(1, 2)  # (batch, seq_len, d_model)\n",
    "\n",
    "class EvoEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    输入:  (B, L)        L == extra_dim\n",
    "    输出: (B, L, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 64,\n",
    "        d_conv: int  = 4,\n",
    "        expand: int  = 2,\n",
    "        headdim: int = 128,\n",
    "        ngroups: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        **mamba_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model   = d_model\n",
    "\n",
    "        # 1. 把 (B, L) 的 1-d 标量升到 d_model\n",
    "        self.in_proj = nn.Linear(1, d_model, bias=False)\n",
    "\n",
    "        # 2. 官方 Mamba2Simple：把 L 当序列长度，建模 L↔L\n",
    "        self.mamba = Mamba2Block(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            headdim=headdim,\n",
    "            ngroups=ngroups,\n",
    "            **mamba_kwargs\n",
    "        )\n",
    "\n",
    "        # 3. 残差 + Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, L)  连续值或离散索引\n",
    "        \"\"\"\n",
    "        # (B, L) -> (B, L, 1) -> (B, L, d_model)\n",
    "        h = self.in_proj(x.unsqueeze(-1).float())   # 1-d 投影\n",
    "\n",
    "        # Mamba2Simple 要求输入 (B, L, d_model) 即可\n",
    "        h = self.mamba(h)                           # SSD 全局建模\n",
    "\n",
    "        # 残差 + Norm\n",
    "        out = self.norm(h + self.dropout(h))\n",
    "        return out\n",
    "\n",
    "class Mamba2CrossBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=64,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        headdim=128,\n",
    "        ngroups=1,\n",
    "        chunk_size=256,\n",
    "        dropout=0.0,\n",
    "        d_embed_dropout=0.0,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 距离矩阵嵌入\n",
    "        self.extra_embed = EvoEmbedding(d_model=d_model, dropout=d_embed_dropout)\n",
    "\n",
    "        # 原归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # SSD 核心\n",
    "        self.ssd = Mamba2Block(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            headdim=headdim,\n",
    "            ngroups=ngroups,\n",
    "            chunk_size=chunk_size,\n",
    "            use_mem_eff_path=True,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, local_repr, global_repr, x_extra=None,\n",
    "                start_offset=0, end_offset=0):\n",
    "        \"\"\"\n",
    "        local_repr: (B, L, D)\n",
    "        global_repr: (B, G, D)\n",
    "        x_extra: 可选，(B,E) \n",
    "        \"\"\"\n",
    "        local_norm  = self.norm1(local_repr)\n",
    "        global_norm = self.norm2(global_repr)\n",
    "\n",
    "        # 1. 构造输入序列\n",
    "        tokens = []\n",
    "        if x_extra is not None:\n",
    "            extra_token = self.extra_embed(x_extra)        # (B,E,D)\n",
    "            tokens.append(extra_token)\n",
    "        tokens.append(global_norm)\n",
    "        tokens.append(local_norm)\n",
    "        x = torch.cat(tokens, dim=1)               # [B, (E)+G+L, D]\n",
    "\n",
    "        # 2. SSD 扫描\n",
    "        x = self.ssd(x)                            # [B, (E)+G+L, D]\n",
    "\n",
    "        # 3. 只取 local 部分\n",
    "        local_len = local_norm.shape[1]\n",
    "        x = x[:, -local_len:, :]                   # [B, L, D]\n",
    "\n",
    "        # 4. pad 回原始长度\n",
    "        if start_offset or end_offset:\n",
    "            x = F.pad(x, (0, 0, start_offset, end_offset))\n",
    "\n",
    "        # 5. 残差 + FFN\n",
    "        x = x + local_norm\n",
    "        x = self.norm3(x)\n",
    "        x = self.ffn(x) + x\n",
    "        return x\n",
    "        return x\n",
    "\n",
    "class ChunkModule(nn.Module):\n",
    "    \"\"\"Single chunk processing module with BiMamba\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # BiMamba block\n",
    "        self.bimamba_block = BiMambaBlock(d_model)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv_block1 = ConvBlock(d_model)\n",
    "        self.conv_block2 = ConvBlock(d_model)\n",
    "        self.conv_block3 = ConvBlock(d_model)\n",
    "\n",
    "        # Cross attention\n",
    "        # self.cross_attention = CrossAttentionLayer(d_model, n_heads)\n",
    "        self.cross_attention = Mamba2CrossBlock(\n",
    "            d_model=d_model,\n",
    "            d_state=64,\n",
    "            d_conv=4,\n",
    "            expand=2,\n",
    "            headdim=128,\n",
    "            ngroups=1,\n",
    "            chunk_size=256,\n",
    "        )\n",
    "\n",
    "        # Additional layers\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x, x_extra=None):\n",
    "        # BiMamba processing\n",
    "        xa0 = self.bimamba_block(x)\n",
    "\n",
    "        # First conv block\n",
    "        xa = self.conv_block1(xa0)\n",
    "        xa_skip = self.conv_block2(xa)\n",
    "\n",
    "        # Dense layer\n",
    "        xa = self.gelu(self.dense(xa))\n",
    "        xa = self.conv_block2(xa)\n",
    "\n",
    "        # Cross attention\n",
    "        xa = self.cross_attention(xa, xa0, x_extra)\n",
    "        xa = self.dropout(xa)\n",
    "\n",
    "        # Final conv block\n",
    "        xa = self.conv_block3(xa)\n",
    "\n",
    "        # Concatenate with skip connection\n",
    "        xa = torch.cat([xa_skip, xa], dim=-1)\n",
    "\n",
    "        return xa\n",
    "\n",
    "class BandConv1d(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight, bias, mask, kernel, pad):\n",
    "        # x: (B, C, L)   mask: (L,)  0/1\n",
    "        ctx.save_for_backward(x, weight, bias, mask)\n",
    "        ctx.kernel, ctx.pad = kernel, pad\n",
    "        L = x.shape[-1]\n",
    "        # 1. 把有效区抽出来（连续内存）\n",
    "        idx = torch.where(mask)[0]\n",
    "        x_band = x[..., idx]                      # (B, C, len_band)\n",
    "        # 2. 标准 conv1d\n",
    "        y_band = F.conv1d(x_band, weight, bias, padding=pad)\n",
    "        # 3. 建全长度空张量，再把结果写回去\n",
    "        y = torch.full((x.shape[0], weight.shape[0], L),\n",
    "                       float('nan'), device=x.device)\n",
    "        y[..., idx] = y_band\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, weight, bias, mask = ctx.saved_tensors\n",
    "        kernel, pad = ctx.kernel, ctx.pad\n",
    "        idx = torch.where(mask)[0]\n",
    "        # 只把带状区梯度拿出来\n",
    "        grad_band = grad_output[..., idx]\n",
    "        x_band = x[..., idx]\n",
    "        # 计算输入/权重梯度\n",
    "        grad_x_band = torch.nn.grad.conv1d_input(\n",
    "            x_band.shape, weight, grad_band, padding=pad)\n",
    "        grad_weight = torch.nn.grad.conv1d_weight(\n",
    "            x_band, weight.shape, grad_band, padding=pad)\n",
    "        grad_bias   = grad_band.sum(dim=[0,2]) if bias is not None else None\n",
    "        # 把输入梯度写回原位，其余区域无梯度（=None）\n",
    "        grad_x = torch.full_like(x, float('nan'))\n",
    "        grad_x[..., idx] = grad_x_band\n",
    "        return grad_x, grad_weight, grad_bias, None, None, None\n",
    "\n",
    "class SparseGlobalConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, kernel=5, pad=2):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(c_out, c_in, kernel))\n",
    "        self.bias   = nn.Parameter(torch.zeros(c_out))\n",
    "        self.kernel, self.pad = kernel, pad\n",
    "        nn.init.kaiming_normal_(self.weight)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        return BandConv1d.apply(x, self.weight, self.bias, mask,\n",
    "                                self.kernel, self.pad)\n",
    "\n",
    "class GlobalOut(nn.Module):\n",
    "    def __init__(self, d_model, n_alleles):\n",
    "        super().__init__()\n",
    "        self.final_conv  = SparseGlobalConv(2*d_model, d_model//2)\n",
    "        self.output_conv = SparseGlobalConv(d_model//2, n_alleles-1)  # no missing\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # mask: (L,)  0/1  当前 chunk 要激活的位点\n",
    "        x = self.gelu(self.final_conv(x, mask))\n",
    "        x = self.output_conv(x, mask)        # (B, n_alleles-1, L)\n",
    "        # 无效区填 -inf，后面 CrossEntropy 会自动忽略\n",
    "        x = torch.where(mask.unsqueeze(0).unsqueeze(0).bool(),\n",
    "                        x, torch.tensor(-float('inf'), device=x.device))\n",
    "        return F.softmax(x.transpose(1,2), dim=-1)\n",
    "\n",
    "class EvoFill(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_alleles: int,\n",
    "        total_sites: int,\n",
    "        chunk_size: int = 8192,\n",
    "        chunk_overlap: int = 64,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_alleles = n_alleles\n",
    "        self.total_sites = total_sites\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "        # 1. chunk 边界\n",
    "        stride = chunk_size - chunk_overlap\n",
    "        starts = [i * stride for i in range((total_sites - 1) // stride + 1)]\n",
    "        ends = [min(s + chunk_size, total_sites) for s in starts]\n",
    "        self.register_buffer(\"starts\", torch.tensor(starts, dtype=torch.long))\n",
    "        self.register_buffer(\"ends\", torch.tensor(ends, dtype=torch.long))\n",
    "        self.n_chunks = len(starts)\n",
    "\n",
    "        # 2. 每 chunk 一份嵌入 & 处理模块（常驻 GPU，但训练时只激活一个）\n",
    "        self.chunk_embeds = nn.ModuleList(\n",
    "            GenoEmbedding(n_alleles, e - s, d_model) for s, e in zip(starts, ends)\n",
    "        )\n",
    "        self.chunk_modules = nn.ModuleList(\n",
    "            ChunkModule(d_model, dropout_rate) for s, e in zip(starts, ends)\n",
    "        )\n",
    "\n",
    "        # 3. 全局输出层（始终 GPU）\n",
    "        self.global_out = GlobalOut(d_model, n_alleles)\n",
    "\n",
    "        # 4. chunk 掩码表  (n_chunks, L)\n",
    "        masks = torch.stack(\n",
    "            [torch.arange(total_sites).ge(s) & torch.arange(total_sites).lt(e)\n",
    "             for s, e in zip(starts, ends)]\n",
    "        ).float()\n",
    "        self.register_buffer(\"chunk_masks\", masks)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, chunk_id: int,\n",
    "                x_extra: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        x:       (B, L, n_alleles)  完整序列 one-hot\n",
    "        chunk_id: 0..n_chunks-1\n",
    "        x_extra:  (B, extra_dim)\n",
    "        return:   (B, L, n_alleles-1)  softmax 概率\n",
    "        \"\"\"\n",
    "        B, L, _ = x.shape\n",
    "        device = x.device\n",
    "        s, e = self.starts[chunk_id].item(), self.ends[chunk_id].item()\n",
    "        mask = self.chunk_masks[chunk_id]          # (L,)  当前 chunk 覆盖区\n",
    "\n",
    "        # 1. 只取当前 chunk 输入切片\n",
    "        x_slice = x[:, s:e]                        # (B, len, n_alleles)\n",
    "\n",
    "        # 2. 当前 chunk 嵌入 & 处理\n",
    "        z = self.chunk_embeds[chunk_id](x_slice)   # (B, len, d_model)\n",
    "        z = self.chunk_modules[chunk_id](z, x_extra)  # (B, len, 2*d_model)\n",
    "\n",
    "        # 3. 拼回全长度，其余 nan\n",
    "        z_full = torch.full((B, L, 2 * self.d_model), float('nan'), device=device)\n",
    "        z_full[:, s:e] = z\n",
    "        z_full = z_full.transpose(1, 2)            # (B, 2*d_model, L)\n",
    "\n",
    "        # 4. 全局卷积只激活带状区\n",
    "        return self.global_out(z_full, mask)       # (B, L, n_alleles-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f053a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "y_train: torch.Size([4, 12345, 2])\n",
      "\n",
      "model chunks: 4\n",
      "\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 0/3 | epoch 1/3 | loss 1.3536\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 0/3 | epoch 2/3 | loss 1.2774\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 0/3 | epoch 3/3 | loss 1.2199\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 1/3 | epoch 1/3 | loss 1.3241\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 1/3 | epoch 2/3 | loss 1.2441\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 1/3 | epoch 3/3 | loss 1.1981\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 2/3 | epoch 1/3 | loss 1.3014\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 2/3 | epoch 2/3 | loss 1.2271\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 4096, 2])\n",
      "y_band: torch.Size([4, 4096, 2])\n",
      "chunk 2/3 | epoch 3/3 | loss 1.1826\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 249, 2])\n",
      "y_band: torch.Size([4, 249, 2])\n",
      "chunk 3/3 | epoch 1/3 | loss 1.2414\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 249, 2])\n",
      "y_band: torch.Size([4, 249, 2])\n",
      "chunk 3/3 | epoch 2/3 | loss 1.0831\n",
      "\n",
      "x_train: torch.Size([4, 12345, 3])\n",
      "x_extra: torch.Size([4, 10])\n",
      "pred: torch.Size([4, 12345, 2])\n",
      "pred_band: torch.Size([4, 249, 2])\n",
      "y_band: torch.Size([4, 249, 2])\n",
      "chunk 3/3 | epoch 3/3 | loss 1.0537\n"
     ]
    }
   ],
   "source": [
    "# ---------- 假数据 ----------\n",
    "B, L, A = 4, 12345, 3\n",
    "d_model = 64\n",
    "chunk_size, overlap = 4096, 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x_train = torch.zeros(B, L, A, device=device)\n",
    "allele = torch.randint(0, A, (B, L), device=device)\n",
    "x_train.scatter_(2, allele.unsqueeze(-1), 1)\n",
    "x_extra = torch.randn(B, 10, device=device)\n",
    "y_train = torch.randn(B, L, A-1, device=device)\n",
    "\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_extra: {x_extra.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(\"\")\n",
    "# ---------- 模型 &损失 ----------\n",
    "model = EvoFill(d_model, A, L, chunk_size, overlap).to(device)\n",
    "print(f\"model chunks: {model.n_chunks}\")\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# ---------- 训练循环 ----------\n",
    "epochs_per_chunk = 3\n",
    "for cid in range(model.n_chunks):\n",
    "    mask = model.chunk_masks[cid]\n",
    "    opt = torch.optim.AdamW(\n",
    "          list(model.chunk_embeds[cid].parameters()) +\n",
    "          list(model.chunk_modules[cid].parameters()) +\n",
    "          list(model.global_out.parameters()), lr=3e-4)\n",
    "    for epoch in range(epochs_per_chunk):\n",
    "        opt.zero_grad()\n",
    "        pred = model(x_train, cid, x_extra)\n",
    "\n",
    "        idx = torch.where(mask)[0]                 # (n_active,)\n",
    "        pred_band  = pred[:, idx]                  # (B, n_active, n_alleles-1)\n",
    "        y_band     = y_train[:, idx]               # (B, n_active, n_alleles-1)\n",
    "\n",
    "        loss = criterion(pred_band, y_band)        # 默认 reduction='mean'\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(\"\")\n",
    "        print(f\"x_train: {x_train.shape}\")\n",
    "        print(f\"x_extra: {x_extra.shape}\")\n",
    "        print(f\"pred: {pred.shape}\")\n",
    "\n",
    "        print(f\"pred_band: {pred_band.shape}\")\n",
    "        print(f\"y_band: {y_band.shape}\")\n",
    "\n",
    "        print(f'chunk {cid}/{model.n_chunks-1} | epoch {epoch+1}/{epochs_per_chunk} | loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8c794",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6f2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputationLoss(nn.Module):\n",
    "    \"\"\"Custom loss function for genomic imputation\"\"\"\n",
    "\n",
    "    def __init__(self, use_r2=True, \n",
    "                 use_focal=False, #  all dummy \n",
    "                 group_size=None,\n",
    "                 gamma=None,\n",
    "                 alpha=None,\n",
    "                 eps=None,\n",
    "                 use_gradnorm=None,\n",
    "                 gn_alpha=None,\n",
    "                 gn_lr_w=None,):\n",
    "        super().__init__()\n",
    "        self.use_r2_loss = use_r2\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "    def calculate_minimac_r2(self, pred_alt_allele_probs, gt_alt_af):\n",
    "        \"\"\"Calculate Minimac-style RÂ² metric\"\"\"\n",
    "        mask = torch.logical_or(torch.eq(gt_alt_af, 0.0), torch.eq(gt_alt_af, 1.0))\n",
    "        gt_alt_af = torch.where(mask, 0.5, gt_alt_af)\n",
    "        denom = gt_alt_af * (1.0 - gt_alt_af)\n",
    "        denom = torch.where(denom < 0.01, 0.01, denom)\n",
    "        r2 = torch.mean(torch.square(pred_alt_allele_probs - gt_alt_af), dim=0) / denom\n",
    "        r2 = torch.where(mask, torch.zeros_like(r2), r2)\n",
    "        return r2\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        # Convert to proper format for losses\n",
    "        y_true_ce = torch.argmax(y_true, dim=-1)  # For CrossEntropy\n",
    "        y_pred_log = torch.log(y_pred + 1e-8)  # For KL divergence\n",
    "\n",
    "        # Basic losses\n",
    "        ce_loss = self.ce_loss(y_pred.view(-1, y_pred.size(-1)), y_true_ce.view(-1))\n",
    "        kl_loss = self.kl_loss(y_pred_log.view(-1, y_pred.size(-1)),\n",
    "                               y_true.view(-1, y_true.size(-1)))\n",
    "\n",
    "        total_loss = ce_loss + kl_loss\n",
    "\n",
    "        if self.use_r2_loss:\n",
    "            batch_size = y_true.size(0)\n",
    "            group_size = 4\n",
    "            num_full_groups = batch_size // group_size\n",
    "\n",
    "            if num_full_groups > 0:\n",
    "                y_true_grouped = y_true[:num_full_groups * group_size].view(\n",
    "                    num_full_groups, group_size, *y_true.shape[1:])\n",
    "                y_pred_grouped = y_pred[:num_full_groups * group_size].view(\n",
    "                    num_full_groups, group_size, *y_pred.shape[1:])\n",
    "\n",
    "                r2_loss = 0.0\n",
    "                for i in range(num_full_groups):\n",
    "                    gt_alt_af = torch.count_nonzero(\n",
    "                        torch.argmax(y_true_grouped[i], dim=-1), dim=0\n",
    "                    ).float() / group_size\n",
    "\n",
    "                    pred_alt_allele_probs = torch.sum(y_pred_grouped[i][:, :, 1:], dim=-1)\n",
    "                    r2_loss += -torch.sum(self.calculate_minimac_r2(\n",
    "                        pred_alt_allele_probs, gt_alt_af)) * group_size\n",
    "\n",
    "                total_loss += r2_loss\n",
    "\n",
    "        return total_loss, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0ff63",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb648d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(save_dir, models_dir=\"models\", outputs=\"out\") -> None:\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    for dd in [save_dir, f\"{save_dir}/{models_dir}\", f\"{save_dir}/{outputs}\"]:\n",
    "        if not os.path.exists(dd):\n",
    "            os.makedirs(dd)\n",
    "\n",
    "def clear_dir(path) -> None:\n",
    "    \"\"\"Clear directory if it exists\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c302bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAF_BINS = [(0.00, 0.05), (0.05, 0.10), (0.10, 0.20),\n",
    "            (0.20, 0.30), (0.30, 0.40), (0.40, 0.50)]\n",
    "\n",
    "def precompute_maf(gts_np, mask_int=-1):\n",
    "    \"\"\"\n",
    "    gts_np: (N, L)  int64\n",
    "    return:\n",
    "        maf: (L,) float32\n",
    "        bin_cnt: list[int] 长度 6，对应 6 个 bin 的位点数量\n",
    "    \"\"\"\n",
    "    L = gts_np.shape[1]\n",
    "    maf = np.zeros(L, dtype=np.float32)\n",
    "    bin_cnt = [0] * 6\n",
    "\n",
    "    for l in range(L):\n",
    "        alleles = gts_np[:, l]\n",
    "        alleles = alleles[alleles != mask_int]   # 去掉缺失\n",
    "        if alleles.size == 0:\n",
    "            maf[l] = 0.0\n",
    "            continue\n",
    "\n",
    "        uniq, cnt = np.unique(alleles, return_counts=True)\n",
    "        total = cnt.sum()\n",
    "        freq = cnt / total\n",
    "        freq[::-1].sort()\n",
    "        maf_val = freq[1] if len(freq) > 1 else 0.0\n",
    "        maf[l] = maf_val\n",
    "\n",
    "        # 统计 bin\n",
    "        for i, (lo, hi) in enumerate(MAF_BINS):\n",
    "            if lo <= maf_val < hi:\n",
    "                bin_cnt[i] += 1\n",
    "                break\n",
    "\n",
    "    return maf, bin_cnt\n",
    "\n",
    "def imputation_maf_accuracy_epoch(all_logits, all_gts, global_maf, mask=None):\n",
    "    \"\"\"\n",
    "    all_logits: (N, L, C)\n",
    "    all_gts:    (N, L, C) one-hot\n",
    "    global_maf: (L,)\n",
    "    mask:       (N, L) 或 None\n",
    "    return:     list[float] 长度 6\n",
    "    \"\"\"\n",
    "    # 1. 预测 vs 真实\n",
    "    all_gts = all_gts.argmax(dim=-1)      # (N, L)\n",
    "    preds   = all_logits.argmax(dim=-1)   # (N, L)\n",
    "\n",
    "    # 2. 如果没有外部 mask，就默认全 1\n",
    "    if mask is None:\n",
    "        mask = torch.ones_like(all_gts, dtype=torch.bool)   # (N, L)\n",
    "    correct = (preds == all_gts) & mask                   # (N, L)\n",
    "\n",
    "    # 3. MAF 条件 -> (1, L) 再广播到 (N, L)\n",
    "    maf = global_maf.unsqueeze(0)                         # (1, L)\n",
    "\n",
    "    # 4. 分 bin 计算\n",
    "    accs = []\n",
    "    for lo, hi in MAF_BINS:\n",
    "        maf_bin = mask & (maf >= lo) & (maf < hi)                # (1, L)\n",
    "        n_cor = (correct & maf_bin).sum()\n",
    "        n_tot = maf_bin.sum()\n",
    "        accs.append(100*(n_cor / n_tot).item() if n_tot > 0 else 0.0)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d21fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded genotypes info: (2404, 99314)\n",
      "Loaded extra info: (2404, 26)\n",
      "Unique genotypes in dataset: ['0|0', '0|1', '1|0', '1|1']\n",
      "hap_map:\n",
      "  0|0 -> 0\n",
      "  0|1 -> 1\n",
      "  1|0 -> 2\n",
      "  1|1 -> 3\n",
      "  .|. -> 4\n",
      "self.seq_depth: 5\n",
      "2340 samples in train\n",
      "64 samples in val\n"
     ]
    }
   ],
   "source": [
    "work_dir      = '/home/qmtang/mnt_qmtang/EvoFill/data/251021_ver01_chr22'\n",
    "ref_vcf       = \"/home/qmtang/GitHub/STICI-HPC/data/training_sets/ALL.chr22.training.samples.100k.any.type.0.01.maf.variants.vcf.gz\"\n",
    "ref_extra     = \"/mnt/qmtang/EvoFill/data/251020_ver01_chr22/pop_wasserstein.tsv\"\n",
    "\n",
    "val_n_samples = 64\n",
    "max_mr             = 0.7\n",
    "min_mr             = 0.3\n",
    "batch_size_per_gpu = 8\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "create_directories(work_dir)\n",
    "\n",
    "# Load data\n",
    "dr = DataReader(\n",
    "    ref_vcf=ref_vcf,\n",
    "    ref_extra=ref_extra\n",
    ")\n",
    "X_gt, X_extra = dr.load()\n",
    "\n",
    "# # Split data for validation\n",
    "n_samples, total_sites = X_gt.shape\n",
    "x_train_indices, x_valid_indices = train_test_split(\n",
    "    range(n_samples),\n",
    "    test_size=val_n_samples,\n",
    "    random_state=3047,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_gt = X_gt[x_train_indices,:]\n",
    "train_extra = X_extra[x_train_indices,:]\n",
    "\n",
    "val_gt = X_gt[x_valid_indices,:]\n",
    "val_extra = X_extra[x_valid_indices,:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GenomicDataset(\n",
    "    train_gt, train_extra, dr.seq_depth,\n",
    "    mask=True, masking_rates=(min_mr, max_mr)\n",
    ")\n",
    "val_dataset = GenomicDataset(\n",
    "    val_gt, val_extra, dr.seq_depth,\n",
    "    mask=True, masking_rates=(min_mr, max_mr)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_per_gpu,\n",
    "                        shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_per_gpu,\n",
    "                        shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"{len(x_train_indices)} samples in train\")\n",
    "print(f\"{len(x_valid_indices)} samples in val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "097171d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model[hg19_chr22] chunks=13\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hg19_chr22'\n",
    "chunk_size = 8192\n",
    "overlap    = 64\n",
    "d_model    = 64\n",
    "alleles    = dr.seq_depth\n",
    "\n",
    "model = EvoFill(d_model, alleles, total_sites, chunk_size, overlap).to(device)\n",
    "print(f\"model[{model_name}] chunks={model.n_chunks}\")\n",
    "\n",
    "criterion = ImputationLoss(use_r2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MAF_bin Counts\n",
      "(0.00, 0.05)   3145\n",
      "(0.05, 0.10)   1552\n",
      "(0.10, 0.20)   1942\n",
      "(0.20, 0.30)   1446\n",
      "(0.30, 0.40)     85\n",
      "(0.40, 0.50)     22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 1/10, Train Loss: 69292.5245, Val Loss: 66277.8574\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 93.78 95.63\n",
      "(0.05, 0.10)   1552 83.49 90.09\n",
      "(0.10, 0.20)   1942 69.64 81.87\n",
      "(0.20, 0.30)   1446 58.54 76.44\n",
      "(0.30, 0.40)     85 43.98 69.36\n",
      "(0.40, 0.50)     22 44.68 68.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 2/10, Train Loss: 49928.0296, Val Loss: 65222.2310\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 94.95 96.78\n",
      "(0.05, 0.10)   1552 86.59 91.92\n",
      "(0.10, 0.20)   1942 76.90 84.44\n",
      "(0.20, 0.30)   1446 70.70 78.97\n",
      "(0.30, 0.40)     85 51.11 71.67\n",
      "(0.40, 0.50)     22 51.75 69.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 3/10, Train Loss: 46159.0682, Val Loss: 70372.1309\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 95.18 96.41\n",
      "(0.05, 0.10)   1552 87.93 90.93\n",
      "(0.10, 0.20)   1942 79.53 82.92\n",
      "(0.20, 0.30)   1446 74.14 77.15\n",
      "(0.30, 0.40)     85 53.12 70.17\n",
      "(0.40, 0.50)     22 52.96 67.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 4/10, Train Loss: 44259.7210, Val Loss: 65322.7339\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 95.35 96.87\n",
      "(0.05, 0.10)   1552 88.73 92.66\n",
      "(0.10, 0.20)   1942 81.03 85.40\n",
      "(0.20, 0.30)   1446 76.16 79.95\n",
      "(0.30, 0.40)     85 54.22 72.89\n",
      "(0.40, 0.50)     22 53.58 67.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 5/10, Train Loss: 42616.2359, Val Loss: 67731.4341\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 95.45 96.73\n",
      "(0.05, 0.10)   1552 89.32 91.81\n",
      "(0.10, 0.20)   1942 82.24 84.13\n",
      "(0.20, 0.30)   1446 77.71 78.22\n",
      "(0.30, 0.40)     85 55.34 72.72\n",
      "(0.40, 0.50)     22 54.88 66.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 6/10, Train Loss: 41701.9468, Val Loss: 74443.4268\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 95.56 96.34\n",
      "(0.05, 0.10)   1552 89.75 91.25\n",
      "(0.10, 0.20)   1942 82.98 82.87\n",
      "(0.20, 0.30)   1446 78.76 76.13\n",
      "(0.30, 0.40)     85 56.15 69.52\n",
      "(0.40, 0.50)     22 55.61 67.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 7/10, Train Loss: 40288.1210, Val Loss: 74371.6104\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 95.73 96.34\n",
      "(0.05, 0.10)   1552 90.51 91.27\n",
      "(0.10, 0.20)   1942 84.38 82.91\n",
      "(0.20, 0.30)   1446 80.32 76.03\n",
      "(0.30, 0.40)     85 57.38 69.65\n",
      "(0.40, 0.50)     22 55.97 67.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0, Epoch 8/10, Train Loss: 39683.8185, Val Loss: 67533.2114\n",
      "     MAF_bin Counts Train   Val\n",
      "(0.00, 0.05)   3145 95.77 96.94\n",
      "(0.05, 0.10)   1552 90.71 92.63\n",
      "(0.10, 0.20)   1942 84.87 84.80\n",
      "(0.20, 0.30)   1446 80.97 79.17\n",
      "(0.30, 0.40)     85 57.71 72.17\n",
      "(0.40, 0.50)     22 55.86 68.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs_per_chunk   = 10\n",
    "lr                 = 0.001\n",
    "weight_decay       = 1e-5\n",
    "earlystop_patience = 9\n",
    "\n",
    "for cid in range(model.n_chunks):\n",
    "    chunk_mask = model.chunk_masks[cid].cpu()\n",
    "\n",
    "    chunk_maf, chunk_bin_cnt = precompute_maf(X_gt[:, torch.where(chunk_mask)[0]],  mask_int=dr.seq_depth)\n",
    "    chunk_maf = torch.from_numpy(chunk_maf).to(device)\n",
    "    maf_df = pd.DataFrame({\n",
    "        'MAF_bin': ['(0.00, 0.05)', '(0.05, 0.10)', '(0.10, 0.20)',\n",
    "                    '(0.20, 0.30)', '(0.30, 0.40)', '(0.40, 0.50)'],\n",
    "        'Counts':  [f\"{c}\" for c in chunk_bin_cnt],\n",
    "    })\n",
    "    print(maf_df.to_string(index=False))\n",
    "\n",
    "    optimizer = AdamW(list(model.chunk_embeds[cid].parameters()) +\n",
    "                      list(model.chunk_modules[cid].parameters()) +\n",
    "                      list(model.global_out.parameters()), \n",
    "                      lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-7)\n",
    "    best_loss = float('inf')\n",
    "    patience = earlystop_patience\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs_per_chunk):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_logits, train_gts, train_mask = [], [], []\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f'Chunk {cid}/{model.n_chunks}, Epoch {epoch + 1}/{epochs_per_chunk}', leave=False)\n",
    "        for batch_idx, (x, x_extra, target) in enumerate(train_pbar):\n",
    "            x, x_extra, target = x.to(device), x_extra.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x, cid, x_extra)\n",
    "\n",
    "            idx       = torch.where(chunk_mask)[0]          # (n_active,)\n",
    "            pred_band = pred[:, idx]                  # (B, n_active, n_alleles-1)\n",
    "            y_band    = target[:, idx]               # (B, n_active, n_alleles-1)\n",
    "\n",
    "            loss, logs = criterion(pred_band, y_band)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            # === 收集训练结果 ===\n",
    "            miss_mask = x[:, idx][..., -1].bool()         # 只关心被 mask 的位点\n",
    "            train_logits.append(pred_band.detach())\n",
    "            train_gts.append(y_band.detach())\n",
    "            train_mask.append(miss_mask)\n",
    "\n",
    "        # 训练集 MAF-acc\n",
    "        train_logits = torch.cat(train_logits, dim=0)\n",
    "        train_gts    = torch.cat(train_gts,    dim=0)\n",
    "        train_mask   = torch.cat(train_mask,   dim=0)\n",
    "        train_maf_accs = imputation_maf_accuracy_epoch(train_logits, train_gts, chunk_maf, mask=train_mask)\n",
    "\n",
    "        # ----------- 验证循环同理 ------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_logits, val_gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, x_extra, target in val_loader:\n",
    "                x, x_extra, target = x.to(device), x_extra.to(device), target.to(device)\n",
    "                pred = model(x, cid, x_extra)\n",
    "\n",
    "                idx       = torch.where(chunk_mask)[0]          # (n_active,)\n",
    "                pred_band = pred[:, idx]                  # (B, n_active, n_alleles-1)\n",
    "                y_band    = target[:, idx]               # (B, n_active, n_alleles-1)\n",
    "\n",
    "                loss, logs = criterion(pred_band, y_band)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_logits.append(pred_band)\n",
    "                val_gts.append(y_band)\n",
    "\n",
    "        val_logits = torch.cat(val_logits, dim=0)\n",
    "        val_gts    = torch.cat(val_gts,    dim=0)\n",
    "        val_maf_accs = imputation_maf_accuracy_epoch(\n",
    "            val_logits, val_gts, chunk_maf,  mask=None,) # 计算所有位点\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss   = val_loss   / len(val_loader)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f'Chunk {cid}/{model.n_chunks}, Epoch {epoch + 1}/{epochs_per_chunk}, '\n",
    "                f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # 用 DataFrame 打印 MAF-bin 结果\n",
    "        maf_df = pd.DataFrame({\n",
    "            'MAF_bin': ['(0.00, 0.05)', '(0.05, 0.10)', '(0.10, 0.20)',\n",
    "                        '(0.20, 0.30)', '(0.30, 0.40)', '(0.40, 0.50)'],\n",
    "            'Counts':  [f\"{c}\" for c in chunk_bin_cnt],\n",
    "            'Train':   [f\"{acc:.2f}\" for acc in train_maf_accs],\n",
    "            'Val':     [f\"{acc:.2f}\" for acc in val_maf_accs]\n",
    "        })\n",
    "        print(maf_df.to_string(index=False))\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            import os\n",
    "            os.makedirs(f'{work_dir}/models', exist_ok=True)\n",
    "            # torch.save(model.state_dict(), f'{work_dir}/models/weights.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    del optimizer, scheduler\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2059f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_maf.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
