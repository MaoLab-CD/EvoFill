{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c4a14f",
   "metadata": {},
   "source": [
    "# EvoFill all-in-one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef99aa",
   "metadata": {},
   "source": [
    "本Notebook打包了项目涉及的所有模块与函数，依赖环境: `/home/qmtang/miniconda3/envs/mamba`，可参考通过以下命令克隆：\n",
    "```bash\n",
    "conda create env -f mamba.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a35a2",
   "metadata": {},
   "source": [
    "## 1. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31acbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from types import SimpleNamespace\n",
    "def load_config(path: str) -> SimpleNamespace:\n",
    "    def hook(d):\n",
    "        return SimpleNamespace(**{k: hook(v) if isinstance(v, dict) else v\n",
    "                                  for k, v in d.items()})\n",
    "    with open(path) as f:\n",
    "        return json.load(f, object_hook=hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b7ac0",
   "metadata": {},
   "source": [
    "注：在针对变异位点坐标的嵌入上，这里的处理方式和STICI原版稍有不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadcb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from cyvcf2 import VCF\n",
    "\n",
    "\n",
    "def build_quaternion(chrom, pos, chrom_len_dict, chrom_start_dict, genome_len):\n",
    "    \"\"\"\n",
    "    返回 list[float32] 长度 4\n",
    "    \"\"\"\n",
    "    def _log4(x):\n",
    "        return math.log(x) / math.log(4)\n",
    "\n",
    "    chrom = str(chrom).strip('chr')\n",
    "    pos = int(pos)\n",
    "    c_len = chrom_len_dict[chrom]\n",
    "    c_start = chrom_start_dict[chrom]\n",
    "    abs_pos = c_start + pos\n",
    "    return [\n",
    "        _log4(pos),\n",
    "        _log4(c_len),\n",
    "        _log4(abs_pos),\n",
    "        _log4(genome_len),\n",
    "    ]\n",
    "\n",
    "\n",
    "def read_vcf(path: str, phased: bool, genome_json: str):\n",
    "    \"\"\"\n",
    "    返回\n",
    "        gts: np.ndarray (n_samples, n_snps)  int32\n",
    "        samples: list[str]\n",
    "        var_index: torch.Tensor (n_snps,)  int8\n",
    "        depth: int\n",
    "        pos_tensor: torch.Tensor (n_snps, 2)  str  // 染色体+坐标\n",
    "        quat_tensor: torch.Tensor (n_snps, 4)  float32\n",
    "    同时保存 var_index.pt\n",
    "    \"\"\"\n",
    "    # ---- 0. 读基因组元信息 ----\n",
    "    with open(genome_json) as f:\n",
    "        gmeta = json.load(f)\n",
    "    chrom_len = gmeta[\"chrom_len\"]        # dict[str, int]\n",
    "    chrom_start = gmeta[\"chrom_start\"]    # dict[str, int]\n",
    "    genome_len = gmeta[\"genome_len\"]      # int\n",
    "\n",
    "    vcf = VCF(path)\n",
    "    samples = list(vcf.samples)\n",
    "\n",
    "    gts_list = []\n",
    "    var_index_list = []\n",
    "    quat_list = []\n",
    "\n",
    "    total = sum(1 for _ in VCF(path))\n",
    "    for var in tqdm(vcf, total=total, desc=\"Parsing VCF\"):\n",
    "        alleles = [var.REF] + var.ALT\n",
    "        allele2idx = {a: i for i, a in enumerate(alleles)}\n",
    "\n",
    "        row = []\n",
    "        for gt_str in var.gt_bases:\n",
    "            if gt_str in ['.|.', './.']:\n",
    "                row.append(-1)\n",
    "            else:\n",
    "                sep = '|' if phased else '/'\n",
    "                a1, a2 = gt_str.split(sep)\n",
    "                row.append(allele2idx[a1] + allele2idx[a2])\n",
    "        row = np.array(row, dtype=np.int32)\n",
    "        gts_list.append(row)\n",
    "\n",
    "        # 离散值个数\n",
    "        observed = row[row >= 0]\n",
    "        var_index_list.append(int(np.unique(observed).size))\n",
    "\n",
    "        # 变异位点位置坐标\n",
    "        quat = build_quaternion(var.CHROM, var.POS, chrom_len, chrom_start, genome_len)\n",
    "        quat_list.append(quat)\n",
    "\n",
    "    gts = np.vstack(gts_list).T.astype(np.int32)\n",
    "    flat = gts[gts >= 0]\n",
    "    global_depth = int(flat.max()) + 2\n",
    "\n",
    "    gts = torch.tensor(gts, dtype=torch.int8)\n",
    "    var_depth_index = torch.tensor(var_index_list, dtype=torch.int8)\n",
    "    quat_tensor = torch.tensor(quat_list, dtype=torch.float32)\n",
    "\n",
    "    return gts, samples, var_depth_index, global_depth, quat_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"/home/qmtang/mnt_qmtang/EvoFill/config/config.json\")\n",
    "os.makedirs(cfg.data.path, exist_ok=True)\n",
    "\n",
    "phased = bool(cfg.data.tihp)\n",
    "genome_json = cfg.data.genome_json\n",
    "\n",
    "# ---------- 训练集 ----------\n",
    "train_gts, train_samples, var_depth_index, global_depth, quat_train = read_vcf(\n",
    "    cfg.data.train_vcf, phased, genome_json)\n",
    "print(f\"Inferred unified depth = {global_depth}\")\n",
    "\n",
    "torch.save({'gts': train_gts, 'coords':quat_train, 'var_depths':var_depth_index},\n",
    "            os.path.join(cfg.data.path, \"train.pt\"))\n",
    "\n",
    "print(f\"Saved train.pt | gts={tuple(train_gts.shape)} | coords={tuple(quat_train.shape)}\")\n",
    "\n",
    "\n",
    "# ---------- 验证集 ----------\n",
    "val_gts, val_samples, _, _, quat_val = read_vcf(\n",
    "    cfg.data.val_vcf, phased, genome_json)\n",
    "\n",
    "torch.save({'gts': val_gts, 'coords':quat_val, 'var_depths':var_depth_index},\n",
    "            os.path.join(cfg.data.path, \"val.pt\"))\n",
    "\n",
    "print(f\"Saved val.pt   | gts={tuple(val_gts.shape)} | coords={tuple(quat_val.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa44d13",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f2e2c",
   "metadata": {},
   "source": [
    "将STICI中原有的 `chunk_module` 内的注意力、全连接层等模块统一替换为 `BiMamba2Block`\n",
    "\n",
    "以下几个地方需要注意和STICI的差别：\n",
    "\n",
    "1. chunk 划分标准：按 chunk_size 分割？按 n_chunks 分割？尾部 chunk 位点数不足的部分如何处理？ \n",
    "2. 分 chunk 后的 concat 部分，如何处理 chunk 与 chunk 之间 overlap 的位点？\n",
    "3. STICI 在 concat chunk 后，又经过了两层 Conv1D，文章示意图上未标明。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "from mamba_ssm import Mamba2  # 官方实现\n",
    "\n",
    "\n",
    "class CatEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    等位基因 + 坐标 嵌入\n",
    "    -1 -> padding_idx (n_cats) -> 零向量\n",
    "    \"\"\"\n",
    "    def __init__(self, n_cats: int, d_model: int, coord_dim: int = 4):\n",
    "        super().__init__()\n",
    "        self.allele_embed = nn.Embedding(n_cats + 1, d_model, padding_idx=n_cats)\n",
    "        self.coord_proj = nn.Linear(coord_dim, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_coord: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x:       (B, L)        long，-1 会被当成 padding_idx n_cats\n",
    "        x_coord: (L, 4)        float\n",
    "        return:  (B, L, d_model)\n",
    "        \"\"\"\n",
    "        x = x.masked_fill(x == -1, self.allele_embed.padding_idx)\n",
    "        e1 = self.allele_embed(x)                       # (B,L,d)\n",
    "        e2 = self.coord_proj(x_coord).unsqueeze(0)      # (1,L,d)\n",
    "        return self.norm(e1 + e2)\n",
    "\n",
    "class BiMamba2Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        bidirectional: bool = True,\n",
    "        bidirectional_strategy: str = \"add\",      # \"add\" | \"ew_multiply\"\n",
    "        bidirectional_weight_tie: bool = True,\n",
    "        # ---- 以下透传给 Mamba2 ----\n",
    "        d_state: int = 128,\n",
    "        expand: int = 2,\n",
    "        d_conv: int = 4,\n",
    "        conv_bias: bool = True,\n",
    "        bias: bool = False,\n",
    "        headdim: int = 64,\n",
    "        ngroups: int = 1,\n",
    "        **mamba2_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if bidirectional and bidirectional_strategy not in {\"add\", \"ew_multiply\"}:\n",
    "            raise NotImplementedError(bidirectional_strategy)\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.strategy = bidirectional_strategy\n",
    "\n",
    "        # 前向 SSM\n",
    "        self.mamba_fwd = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            expand=expand,\n",
    "            d_conv=d_conv,\n",
    "            conv_bias=conv_bias,\n",
    "            bias=bias,\n",
    "            headdim=headdim,\n",
    "            ngroups=ngroups,\n",
    "            **mamba2_kwargs,\n",
    "        )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.mamba_rev = Mamba2(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                expand=expand,\n",
    "                d_conv=d_conv,\n",
    "                conv_bias=conv_bias,\n",
    "                bias=bias,\n",
    "                headdim=headdim,\n",
    "                ngroups=ngroups,\n",
    "                **mamba2_kwargs,\n",
    "            )\n",
    "            if bidirectional_weight_tie:\n",
    "                self.mamba_rev.in_proj.weight = self.mamba_fwd.in_proj.weight\n",
    "                self.mamba_rev.in_proj.bias   = self.mamba_fwd.in_proj.bias\n",
    "                self.mamba_rev.out_proj.weight = self.mamba_fwd.out_proj.weight\n",
    "                self.mamba_rev.out_proj.bias   = self.mamba_fwd.out_proj.bias\n",
    "        else:\n",
    "            self.mamba_rev = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        out = self.mamba_fwd(x)\n",
    "        if self.bidirectional:\n",
    "            x_rev = x.flip(dims=[1])\n",
    "            out_rev = self.mamba_rev(x_rev).flip(dims=[1])\n",
    "            if self.strategy == \"add\":\n",
    "                out = out + out_rev\n",
    "            elif self.strategy == \"ew_multiply\":\n",
    "                out = out * out_rev\n",
    "            else:\n",
    "                raise RuntimeError(self.strategy)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ChunkModule(nn.Module):\n",
    "    def __init__(self, d_model: int, n_layers: int, **mamba_kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            BiMamba2Block(d_model=d_model, **mamba_kwargs)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x_chunk):\n",
    "        for blk in self.blocks:\n",
    "            x_chunk = blk(x_chunk)\n",
    "        return x_chunk\n",
    "\n",
    "class EvoFill(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cats: int,\n",
    "        n_chunks: int,\n",
    "        d_model: int = 256,\n",
    "        n_layers: int = 4,\n",
    "        chunk_overlap_ratio: float = 0.1,\n",
    "        **mamba_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cats = n_cats\n",
    "        self.n_chunks = n_chunks\n",
    "        self.chunk_overlap_ratio = chunk_overlap_ratio\n",
    "\n",
    "        self.embed = CatEmbeddings(n_cats, d_model)\n",
    "        self.chunk_modules = nn.ModuleList([\n",
    "            ChunkModule(d_model, n_layers, **mamba_kwargs)\n",
    "            for _ in range(n_chunks)\n",
    "        ])\n",
    "        self.length_proj = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.out_conv = nn.Conv1d(d_model, n_cats, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_coord: torch.Tensor):\n",
    "        B, L_orig = x.shape\n",
    "\n",
    "        # 1. 动态计算 chunk_size\n",
    "        chunk_size = math.ceil(L_orig / self.n_chunks)\n",
    "        overlap = int(chunk_size * self.chunk_overlap_ratio)\n",
    "        step = chunk_size - overlap\n",
    "\n",
    "        # 2. 补 -1 到能被 step 整除\n",
    "        pad_len = (step - L_orig % step) % step\n",
    "        if pad_len > 0:\n",
    "            x = F.pad(x, (0, pad_len), value=-1)\n",
    "            x_coord = F.pad(x_coord, (0, 0, 0, pad_len), value=0.0)\n",
    "        L_pad = x.shape[-1]\n",
    "\n",
    "        # 3. 嵌入\n",
    "        h = self.embed(x, x_coord)  # (B, L_pad, d_model)\n",
    "\n",
    "        # 4. 滑窗切 chunk -> 独立模块\n",
    "        outs = []\n",
    "        start = 0\n",
    "        for i in range(self.n_chunks):\n",
    "            end = min(start + chunk_size, L_pad)\n",
    "            chunk = h[:, start:end, :]\n",
    "            outs.append(self.chunk_modules[i](chunk))  # (B, chunk_len, d)\n",
    "            if end == L_pad:\n",
    "                break\n",
    "            start += step\n",
    "\n",
    "        # 5. concat (axis=-2) -> 投影回原始长度\n",
    "        h_seq = torch.cat(outs, dim=-2)  # (B, total_len, d)\n",
    "        h_seq = self.length_proj(h_seq.transpose(1, 2))  # (B, d, total_len)\n",
    "        h_seq = F.interpolate(h_seq, size=L_orig, mode='linear', align_corners=False)  # (B, d, L_orig)\n",
    "\n",
    "        # 6. 输出 logits\n",
    "        logits = self.out_conv(h_seq).transpose(1, 2)  # (B, L_orig, n_cats)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0c02b",
   "metadata": {},
   "source": [
    "模型单测，检查输出张量形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada08775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test\n",
    "model = EvoFill(n_cats=4, n_chunks=4, d_model=256, n_layers=4,\n",
    "                chunk_size=512, chunk_overlap_ratio=0.1,\n",
    "                d_state=128, expand=2).cuda()\n",
    "\n",
    "x       = torch.randint(-1, 4, (2, 1800)).cuda()\n",
    "x_coord = torch.randn(1800, 4).cuda()\n",
    "logits  = model(x, x_coord) \n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93def35e",
   "metadata": {},
   "source": [
    "## 3. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629d868",
   "metadata": {},
   "source": [
    "STICI 的 `ImputationLoss` 计算本身以及Pytorch复现存在问题：\n",
    "\n",
    "`tf.keras.losses.KLDivergence` 计算要求输入logits，会在其内部做一遍`softmax`。\n",
    "\n",
    "然而 STICI 最后一层（`STICI_V1.1.py`, L387）：\n",
    "\n",
    "`self.last_conv = layers.Conv1D(self.in_channel - 1, 5, padding='same', activation=tf.nn.softmax)`\n",
    "\n",
    "输出值已经做过一次`softmax`，如此一来会导致计算的KL散度偏大。\n",
    "\n",
    "同时，Pytorch 中的`nn.KLDivLoss`输入要求为`softmax`处理后的`log-probabilities`，其余差异见下表：\n",
    "\n",
    "| 特性                | `tf.keras.losses.KLDivergence` | `nn.KLDivLoss`                        |\n",
    "| ----------------- | ------------------------------ | ------------------------------------- |\n",
    "| 输入格式              | 概率                             | 输入：log-probabilities，目标：probabilities |\n",
    "| 是否需手动取 log        | ❌                              | ✅                                     |\n",
    "| 是否自动裁剪输入          | ✅                              | ❌                                     |\n",
    "| 默认归约方式            | `sum_over_batch_size`          | `batchmean`                           |\n",
    "| 是否支持 `log_target` | ❌                              | ✅（可选）                                 |\n",
    "\n",
    "在 `y_true` 为 one-hot 编码时，真实分布 `y_true` 的熵为0，交叉熵和KL散度应该相等，两者累加无意义。\n",
    "\n",
    "在以下代码中采取了和STICI不同的处理方法：保留MCE，删除KL，R2取1/log（而非负数），用GradNorm平衡MCE和R2损失。实际性能表现待评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "\n",
    "class GradNormLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    GradNorm: Gradient Normalization for Adaptive Loss Balancing\n",
    "    参考原始论文 Chen et al. 2018 实现，适配 2 任务（CE + R²）\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tasks=2, alpha=1.5, lr_w=1e-3, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "        self.alpha   = alpha          # 恢复速度偏好，论文默认 1.5\n",
    "        self.lr_w    = lr_w           # 权重学习率，比模型 lr 小 1~2 量级\n",
    "        self.eps     = eps\n",
    "        self.w       = nn.Parameter(torch.ones(num_tasks))   # 可训练权重\n",
    "        self.register_buffer('L0', torch.zeros(num_tasks))   # 初始损失\n",
    "        self.initialized = False\n",
    "\n",
    "    def forward(self, losses: torch.Tensor):\n",
    "        # losses: [ce, r2]  已经 detach-free\n",
    "        if not self.initialized:\n",
    "            self.L0 = losses.detach().clone()\n",
    "            self.initialized = True\n",
    "\n",
    "        self.L_t = losses\n",
    "        weighted = self.w * losses          # w_i * L_i\n",
    "        return weighted.sum()               # 返回给主优化器\n",
    "\n",
    "    def gradnorm_step(self, shared_params, retain_graph=False):\n",
    "        \"\"\"\n",
    "        在 model.loss_backward() 之后、optimizer.step() 之前调用一次\n",
    "        shared_params:  ***共享部分*** 的参数（例如 encoder 最后一层）\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "\n",
    "        # 1. 清零 w 的 grad\n",
    "        if self.w.grad is not None:\n",
    "            self.w.grad.zero_()\n",
    "\n",
    "        # 2. 计算每个任务对 shared 的梯度范数  G_i(t)\n",
    "        G_t = []\n",
    "        for i in range(self.num_tasks):\n",
    "            g = grad(self.L_t[i], shared_params, retain_graph=True,\n",
    "                     create_graph=True)[0]          # 返回 tuple\n",
    "            G_t.append(torch.norm(g * self.w[i]) + self.eps)\n",
    "        G_t = torch.stack(G_t)                      # [T]\n",
    "\n",
    "        # 3. 相对逆训练速率  r_i(t)\n",
    "        tilde_L_t = (self.L_t / self.L0).detach()\n",
    "        r_t       = tilde_L_t / tilde_L_t.mean()\n",
    "\n",
    "        # 4. 期望梯度范数\n",
    "        bar_G_t = G_t.mean()\n",
    "\n",
    "        # 5. GradNorm 损失：L_grad = sum|G_i(t) - bar_G_t * r_i(t)^α|\n",
    "        l_grad = F.l1_loss(G_t, bar_G_t * (r_t ** self.alpha))\n",
    "\n",
    "        # 6. 只更新 w\n",
    "        self.w.grad = torch.autograd.grad(l_grad, self.w)[0]\n",
    "        with torch.no_grad():\n",
    "            new_w = self.w - self.lr_w * self.w.grad\n",
    "            new_w = new_w * (self.num_tasks / new_w.sum())\n",
    "            self.w.data = new_w  # ✅ 替换 copy_\n",
    "\n",
    "\n",
    "class ImputationLoss(nn.Module):\n",
    "    def __init__(self, use_r2_loss=True, group_size=4, eps=1e-8,\n",
    "                 use_grad_norm=False, gn_alpha=1.5, gn_lr_w=1e-3):\n",
    "        super().__init__()\n",
    "        self.use_r2_loss = use_r2_loss\n",
    "        self.group_size  = group_size\n",
    "        self.eps         = eps\n",
    "        self.use_gn      = use_grad_norm\n",
    "        if self.use_gn:\n",
    "            self.gn_loss = GradNormLoss(num_tasks=2, alpha=gn_alpha, lr_w=gn_lr_w)\n",
    "\n",
    "    # ---------- 工具函数 ---------- #\n",
    "    def _calc_r2(self, pred_alt_prob: torch.Tensor, gt_alt_af: torch.Tensor):\n",
    "        mask = ((gt_alt_af == 0.0) | (gt_alt_af == 1.0))\n",
    "        gt_alt_af = torch.where(mask, 0.5, gt_alt_af)\n",
    "        denom = gt_alt_af * (1.0 - gt_alt_af)\n",
    "        denom = torch.clamp(denom, min=0.01)\n",
    "        r2 = ((pred_alt_prob - gt_alt_af) ** 2) / denom\n",
    "        r2 = torch.where(mask, 0.0, r2)\n",
    "        return r2\n",
    "\n",
    "    # ---------- R2 loss（改为 -1/log(r2)） ---------- #\n",
    "    def _r2_loss(self, y_pred: torch.Tensor, y_true: torch.Tensor, mask_valid: torch.Tensor):\n",
    "        B, V, C = y_pred.shape\n",
    "        G = self.group_size\n",
    "        num_full = B // G\n",
    "        rem = B % G\n",
    "\n",
    "        prob = F.softmax(y_pred, dim=-1)\n",
    "        alt_prob = prob[..., 1] + 2.0 * prob[..., 2]\n",
    "\n",
    "        r2_penalty = 0.0\n",
    "\n",
    "        def one_group(sl):\n",
    "            gt_sl   = y_true[sl]                     # (g_size, V)\n",
    "            mask_sl = mask_valid[sl]                 # (g_size, V)\n",
    "            alt_sl  = alt_prob[sl]                   # (g_size, V)\n",
    "\n",
    "            gt_alt_cnt = (gt_sl * mask_sl).sum(dim=0)\n",
    "            gt_alt_af  = gt_alt_cnt / (mask_sl.sum(dim=0) + self.eps)\n",
    "\n",
    "            pred_alt_af = (alt_sl * mask_sl).sum(dim=0) / (mask_sl.sum(dim=0) + self.eps)\n",
    "\n",
    "            r2 = self._calc_r2(pred_alt_af, gt_alt_af)          # (V,)\n",
    "            return r2.sum() * (sl.stop - sl.start)      # 保持与原来相同的加权方式\n",
    "\n",
    "        # 完整组\n",
    "        for g in range(num_full):\n",
    "            r2_penalty += one_group(slice(g * G, (g + 1) * G))\n",
    "\n",
    "        # 剩余样本\n",
    "        if rem:\n",
    "            r2_penalty += one_group(slice(num_full * G, B))\n",
    "\n",
    "        return 1.0 / torch.log(r2_penalty + 1e-6)\n",
    "\n",
    "    # ---------- 前向 ---------- #\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mask_valid = (y_true != -1)\n",
    "        y_true_m   = y_true.clone()\n",
    "        y_true_m[~mask_valid] = 0\n",
    "\n",
    "        # 1. MCE 改为 mean\n",
    "        log_p = F.log_softmax(y_pred, dim=-1)\n",
    "        ce = -log_p.gather(dim=-1, index=y_true_m.long().unsqueeze(-1)).squeeze(-1)\n",
    "        ce = (ce * mask_valid).sum() / (mask_valid.sum() + self.eps)   # ← 关键改动\n",
    "        # 2. R²\n",
    "        r2 = 0.\n",
    "        if self.use_r2_loss:\n",
    "            r2 = self._r2_loss(y_pred, y_true, mask_valid)\n",
    "\n",
    "        # 3. GradNorm 或固定系数\n",
    "        if self.use_gn:\n",
    "            losses = torch.stack([ce, r2])\n",
    "            return self.gn_loss(losses)\n",
    "        else:\n",
    "            return ce + 10 * r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cade7b9",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e871913",
   "metadata": {},
   "source": [
    "单卡，早停，AdamW优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class GenotypeDataset(Dataset):\n",
    "    def __init__(self, gts, coords, mask_ratio=0.0):\n",
    "        self.gt_true = gts.long()          # 原始完整标签\n",
    "        self.coords = coords.float()\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.gt_true.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gt_true = self.gt_true[idx]        # 完整标签\n",
    "        coords = self.coords                # (L, 4)\n",
    "\n",
    "        # 训练时额外随机遮掩\n",
    "        gt_mask = gt_true.clone()\n",
    "        if self.mask_ratio > 0:\n",
    "            mask = torch.rand_like(gt_mask.float()) < self.mask_ratio\n",
    "            gt_mask[mask] = -1             # 仅输入被遮掩\n",
    "\n",
    "        # 返回：输入（含缺失）、原始标签、坐标\n",
    "        return gt_mask, gt_true, coords \n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: List[(gt_mask, gt_true, coords)] 每个 coords 形状相同\n",
    "    返回：gt_mask, gt_true, coords（二维，直接取第 0 个即可\n",
    "    \"\"\"\n",
    "    gt_mask  = torch.stack([b[0] for b in batch], 0)\n",
    "    gt_true  = torch.stack([b[1] for b in batch], 0)\n",
    "    coords   = batch[0][2]          # 全局共享\n",
    "    return gt_mask, gt_true, coords\n",
    "\n",
    "def build_loader(pt_path, batch_size, shuffle, mask_ratio):\n",
    "    data = torch.load(pt_path)\n",
    "    dataset = GenotypeDataset(data['gts'], data['coords'], mask_ratio=mask_ratio)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=shuffle,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "def imputation_accuracy(logits, gts, mask):\n",
    "    \"\"\"仅在被 mask 位点计算 accuracy\"\"\"\n",
    "    preds = torch.argmax(logits, dim=-1)  # (B, L)\n",
    "    correct = (preds == gts) & mask\n",
    "    return correct.sum().float() / mask.sum().float()\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_mask = 0.0, 0.0, 0\n",
    "\n",
    "    shared_params = list(model.length_proj.parameters()) + list(model.out_conv.parameters())\n",
    "    assert len(shared_params) > 0\n",
    "    assert all(p.requires_grad for p in shared_params)\n",
    "\n",
    "    pbar = tqdm(loader, leave=False)\n",
    "    for gt_mask, gt_true, coords in pbar:\n",
    "        gt_mask, gt_true, coords = gt_mask.to(device), gt_true.to(device), \\\n",
    "                                               coords.to(device)\n",
    "        \n",
    "        logits = model(gt_mask, coords)  # (B, L, n_cats)\n",
    "        loss = criterion(logits, gt_true) \n",
    "\n",
    "        if criterion.use_gn:\n",
    "            criterion.gn_loss.gradnorm_step(shared_params, retain_graph=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy：只算被 mask 的位点\n",
    "        mask = gt_mask == -1\n",
    "        acc = imputation_accuracy(logits, gt_true, mask)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item() * mask.sum().item()\n",
    "        total_mask += mask.sum().item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{acc.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(loader), total_acc / total_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_mask = 0.0, 0.0, 0\n",
    "\n",
    "    pbar = tqdm(loader, leave=False, desc='validate')\n",
    "    for gt_mask, gt_true, coords in pbar:\n",
    "        gt_mask, gt_true, coords = gt_mask.to(device), \\\n",
    "                                     gt_true.to(device), \\\n",
    "                                     coords.to(device)\n",
    "\n",
    "        logits = model(gt_mask, coords)          # (B, L, n_cats)\n",
    "        loss   = criterion(logits, gt_true)      # 计算与真值差异\n",
    "\n",
    "        # 只统计被 mask 的位点\n",
    "        mask = gt_mask == -1\n",
    "        acc  = imputation_accuracy(logits, gt_true, mask)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc  += acc.item() * mask.sum().item()\n",
    "        total_mask += mask.sum().item()\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{acc.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(loader), total_acc / (total_mask + 1e-8)\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.0, mode='min'):\n",
    "        assert mode in {'min', 'max'}\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best = None\n",
    "        self.best_state = None\n",
    "\n",
    "    def __call__(self, metric, model):\n",
    "        if self.best is None:\n",
    "            self.best = metric\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "\n",
    "        better = (metric < self.best - self.min_delta) if self.mode == 'min' else \\\n",
    "                 (metric > self.best + self.min_delta)\n",
    "\n",
    "        if better:\n",
    "            self.best = metric\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c49f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"/home/qmtang/mnt_qmtang/EvoFill/config/config.json\")\n",
    "device = torch.device(cfg.train.device)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 数据\n",
    "train_loader = build_loader(\n",
    "    Path(cfg.data.path) / \"train.pt\",\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    shuffle=True,\n",
    "    mask_ratio=cfg.train.mask_ratio,\n",
    ")\n",
    "val_loader = build_loader(\n",
    "    Path(cfg.data.path) / \"val.pt\",\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    shuffle=False,\n",
    "    mask_ratio=cfg.train.mask_ratio,\n",
    ")\n",
    "\n",
    "# 模型 & 优化器\n",
    "model = EvoFill(**vars(cfg.model)).to(device)\n",
    "\n",
    "criterion = ImputationLoss(use_r2_loss=True,\n",
    "                        use_grad_norm=True,\n",
    "                        gn_alpha=1.5,\n",
    "                        gn_lr_w=cfg.train.lr/10).to(device) #权重学习率，比模型 lr 小 1~2 量级\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=cfg.train.patience,\n",
    "                                min_delta=cfg.train.min_delta,\n",
    "                                mode='min')\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(1, cfg.train.num_epochs + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:03d} | train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "            f\"val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
    "    if early_stopper(val_loss, model):\n",
    "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# 保存最优模型\n",
    "save_dir = Path(cfg.train.save)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(early_stopper.best_state, save_dir / \"evofill_best.pt\")\n",
    "print(f\"Best model saved to {save_dir / 'evofill_best.pt'} (epoch {epoch - early_stopper.counter})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02dd78d",
   "metadata": {},
   "source": [
    "## 5. Imuptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0b933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "002ac335",
   "metadata": {},
   "source": [
    "## 6. Evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80fb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
