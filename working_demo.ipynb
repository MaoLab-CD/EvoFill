{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928e399c",
   "metadata": {},
   "source": [
    "# EvoFill working demo\n",
    "\n",
    "ver 4.   new imputation loss with evo loss\n",
    "\n",
    "ver 4.1  long range modules integrated in stage1 training\n",
    "\n",
    "ver 4.2  stage 3 fine tuning with under-reprensted population samples.\n",
    "\n",
    "last update: 2025/11/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9efe343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "os.chdir('/mnt/qmtang/EvoFill/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed77817",
   "metadata": {},
   "source": [
    "## 0. Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23548ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver:   3.10.19 | packaged by conda-forge | (main, Oct 13 2025, 14:08:27) [GCC 14.3.0]\n",
      "Pytorch ver:  2.8.0+cu129\n",
      "Mamba ver:    2.2.5\n",
      "GPU in use:   NVIDIA H100 PCIe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mamba_ssm\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW, SparseAdam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Python ver:  ', sys.version)\n",
    "print('Pytorch ver: ', torch.__version__)\n",
    "print('Mamba ver:   ', mamba_ssm.__version__)\n",
    "print('GPU in use:  ', torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca64e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import GenotypeEncoder, GenomicDataset, GenomicDataset_Missing, ImputationDataset\n",
    "from src.model import EvoFill\n",
    "from src.loss import ImputationLoss, ImputationLoss_Missing\n",
    "from src.utils import setup_workdir, set_seed, precompute_maf, metrics_by_maf, print_maf_stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path('/mnt/qmtang/EvoFill_data/1kGP_chr22')\n",
    "setup_workdir(work_dir)\n",
    "os.chdir(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7064af",
   "metadata": {},
   "source": [
    "## 1. Pretraining (1kGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ef302",
   "metadata": {},
   "source": [
    "### 1.1 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d332f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 15,792 个位点  \n",
      "[DATA] EvoMat shape: (2222, 2222)\n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill_data/20251118_v4.3/pretrain\n",
      "[DATA] 位点矩阵 = (2222, 15792)，稀疏度 = 44.01%\n",
      "[DATA] 位点字典 = {'0|1': 1, '1|1': 2, '0|0': 0, '.|.': 3}，字典深度 = 4\n",
      "[DATA] 2,222 Samples\n",
      "[DATA] 15,792 Variants Sites\n",
      "[DATA] 4 seq_depth\n"
     ]
    }
   ],
   "source": [
    "gt_enc = GenotypeEncoder(phased = False, gts012 = False, save2disk = True, save_dir = Path(work_dir / \"pretrain\"))\n",
    "gt_enc = gt_enc.encode_new(vcf_path   = \"/mnt/qmtang/EvoFill_data/1kGP_chr22/pretrain/major_pops.vcf.gz\" ,\n",
    "                           default_gt = 'ref',\n",
    "                           evo_mat    = \"/mnt/qmtang/EvoFill_data/1kGP_chr22/pretrain/evo_mat_major_pops.tsv\")\n",
    "\n",
    "print(f\"[DATA] {gt_enc.n_samples:,} Samples\")\n",
    "print(f\"[DATA] {gt_enc.n_variants:,} Variants Sites\")\n",
    "print(f\"[DATA] {gt_enc.seq_depth} seq_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dda895",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_enc = GenotypeEncoder.loadfromdisk(Path(work_dir / \"pretrain\"))\n",
    "print(f'{gt_enc.n_samples:,} samples, {gt_enc.n_variants:,} variants, {gt_enc.seq_depth} seq-depth.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4404ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,094 samples in train\n",
      "128 samples in test\n"
     ]
    }
   ],
   "source": [
    "test_n_samples = 128\n",
    "batch_size    = 16\n",
    "max_mr        = 0.7\n",
    "min_mr        = 0.3\n",
    "\n",
    "x_train_indices, x_test_indices = train_test_split(\n",
    "    range(gt_enc.n_samples),\n",
    "    test_size=test_n_samples,\n",
    "    random_state=3047,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"{len(x_train_indices):,} samples in train\")\n",
    "print(f\"{len(x_test_indices):,} samples in test\")\n",
    "# print(f\"evo_mat: {gt_enc.evo_mat.shape}\")\n",
    "\n",
    "train_dataset = GenomicDataset(\n",
    "    gt_enc,\n",
    "    evo_mat=gt_enc.evo_mat,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_train_indices\n",
    ")\n",
    "\n",
    "test_dataset = GenomicDataset(\n",
    "    gt_enc,\n",
    "    evo_mat=gt_enc.evo_mat,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_test_indices\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_onehot = torch.stack([item[0] for item in batch])\n",
    "    y_onehot = torch.stack([item[1] for item in batch])\n",
    "    real_idx_list = [item[2] for item in batch]\n",
    "    # 提取 evo_mat 子矩阵\n",
    "    if train_dataset.evo_mat is not None:\n",
    "        evo_mat_batch = train_dataset.evo_mat[np.ix_(real_idx_list, real_idx_list)]\n",
    "        evo_mat_batch = torch.FloatTensor(evo_mat_batch)\n",
    "    else:\n",
    "        evo_mat_batch = torch.empty(0)\n",
    "\n",
    "    return x_onehot, y_onehot, evo_mat_batch\n",
    "    \n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c3f8a",
   "metadata": {},
   "source": [
    "### 1.2 Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd90485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "model[aadr_hg19_chr22] would have 1 chunks.\n"
     ]
    }
   ],
   "source": [
    "model_name  = 'chr22'\n",
    "total_sites = gt_enc.n_variants\n",
    "alleles     = gt_enc.seq_depth\n",
    "chunk_size  = 32768\n",
    "overlap     = 1024\n",
    "d_model     = 64\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = EvoFill(d_model, alleles, total_sites, chunk_size, overlap).to(device)\n",
    "print(f\"model[{model_name}] would have {model.n_chunks} chunks.\")\n",
    "\n",
    "criterion = ImputationLoss(use_r2=True, use_evo=True, r2_weight=1, evo_weight=4, evo_lambda=10)\n",
    "# criterion = ImputationLoss(use_r2=True, use_evo=False)\n",
    "\n",
    "meta = {\n",
    "    \"model_name\": str(model_name),\n",
    "    \"total_sites\": int(total_sites),\n",
    "    \"alleles\": int(alleles),\n",
    "    \"chunk_size\": int(chunk_size),\n",
    "    \"overlap\": int(overlap),\n",
    "    \"d_model\": int(d_model)\n",
    "}\n",
    "save_path = os.path.join(Path(work_dir / \"models\"), \"model_meta.json\")\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233acee1",
   "metadata": {},
   "source": [
    "### 1.3 Chunk Module Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bba6dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 1/100: 100%|██████████| 131/131 [01:15<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 1/100, Total Loss, Train = 120527.3, Test = 86382.3, LR: 1.00e-03\n",
      "        Train, ce: 120527.3, r2: -4290.3, evo: 10501.1\n",
      "        Test , ce: 86382.3, r2: -5580.7, evo: 11290.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 2/100: 100%|██████████| 131/131 [00:42<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 2/100, Total Loss, Train = 80379.1, Test = 82244.8, LR: 1.00e-03\n",
      "        Train, ce: 80379.1, r2: -5744.3, evo: 12102.5\n",
      "        Test , ce: 82244.8, r2: -6151.1, evo: 11976.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 3/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 3/100, Total Loss, Train = 72650.1, Test = 66853.7, LR: 1.00e-03\n",
      "        Train, ce: 72650.1, r2: -6083.0, evo: 11815.3\n",
      "        Test , ce: 66853.7, r2: -6482.9, evo: 11368.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 4/100: 100%|██████████| 131/131 [00:42<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 4/100, Total Loss, Train = 67181.4, Test = 63620.5, LR: 1.00e-03\n",
      "        Train, ce: 67181.4, r2: -6318.6, evo: 11502.5\n",
      "        Test , ce: 63620.5, r2: -6640.0, evo: 10723.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 5/100: 100%|██████████| 131/131 [00:42<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 5/100, Total Loss, Train = 64276.2, Test = 62745.3, LR: 1.00e-03\n",
      "        Train, ce: 64276.2, r2: -6439.3, evo: 11257.0\n",
      "        Test , ce: 62745.3, r2: -6604.5, evo: 11142.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 6/100: 100%|██████████| 131/131 [00:42<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 6/100, Total Loss, Train = 61431.3, Test = 61017.3, LR: 1.00e-03\n",
      "        Train, ce: 61431.3, r2: -6542.6, evo: 11031.0\n",
      "        Test , ce: 61017.3, r2: -6890.3, evo: 10676.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 7/100: 100%|██████████| 131/131 [00:41<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 7/100, Total Loss, Train = 59598.6, Test = 56682.3, LR: 1.00e-03\n",
      "        Train, ce: 59598.6, r2: -6630.2, evo: 10858.6\n",
      "        Test , ce: 56682.3, r2: -7028.2, evo: 10335.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 8/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 8/100, Total Loss, Train = 56972.5, Test = 59138.2, LR: 1.00e-03\n",
      "        Train, ce: 56972.5, r2: -6723.1, evo: 10612.3\n",
      "        Test , ce: 59138.2, r2: -6898.9, evo: 10494.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 9/100: 100%|██████████| 131/131 [00:41<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 9/100, Total Loss, Train = 55705.4, Test = 51417.8, LR: 1.00e-03\n",
      "        Train, ce: 55705.4, r2: -6766.3, evo: 10448.8\n",
      "        Test , ce: 51417.8, r2: -7120.3, evo: 9721.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 10/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 10/100, Total Loss, Train = 54229.2, Test = 52766.5, LR: 1.00e-03\n",
      "        Train, ce: 54229.2, r2: -6806.8, evo: 10299.0\n",
      "        Test , ce: 52766.5, r2: -7003.8, evo: 9664.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 11/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 11/100, Total Loss, Train = 53164.3, Test = 48600.6, LR: 1.00e-03\n",
      "        Train, ce: 53164.3, r2: -6843.9, evo: 10193.7\n",
      "        Test , ce: 48600.6, r2: -7187.2, evo: 9356.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 12/100: 100%|██████████| 131/131 [00:43<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 12/100, Total Loss, Train = 51827.3, Test = 48895.9, LR: 1.00e-03\n",
      "        Train, ce: 51827.3, r2: -6908.9, evo: 10055.7\n",
      "        Test , ce: 48895.9, r2: -7162.1, evo: 9568.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 13/100: 100%|██████████| 131/131 [00:42<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 13/100, Total Loss, Train = 50985.4, Test = 48473.5, LR: 1.00e-03\n",
      "        Train, ce: 50985.4, r2: -6935.7, evo: 9939.9\n",
      "        Test , ce: 48473.5, r2: -7174.6, evo: 9675.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 14/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 14/100, Total Loss, Train = 50100.8, Test = 47107.0, LR: 1.00e-03\n",
      "        Train, ce: 50100.8, r2: -6976.6, evo: 9851.0\n",
      "        Test , ce: 47107.0, r2: -7248.4, evo: 9169.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 15/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 15/100, Total Loss, Train = 48772.9, Test = 48043.3, LR: 1.00e-03\n",
      "        Train, ce: 48772.9, r2: -7012.0, evo: 9706.4\n",
      "        Test , ce: 48043.3, r2: -7262.7, evo: 9467.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 16/100: 100%|██████████| 131/131 [00:42<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 16/100, Total Loss, Train = 48313.0, Test = 47164.7, LR: 1.00e-03\n",
      "        Train, ce: 48313.0, r2: -7021.8, evo: 9658.4\n",
      "        Test , ce: 47164.7, r2: -7169.9, evo: 9427.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 17/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 17/100, Total Loss, Train = 47312.7, Test = 45793.8, LR: 1.00e-03\n",
      "        Train, ce: 47312.7, r2: -7078.7, evo: 9527.7\n",
      "        Test , ce: 45793.8, r2: -7262.7, evo: 9469.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 18/100: 100%|██████████| 131/131 [00:42<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 18/100, Total Loss, Train = 46800.2, Test = 43136.7, LR: 1.00e-03\n",
      "        Train, ce: 46800.2, r2: -7073.8, evo: 9472.4\n",
      "        Test , ce: 43136.7, r2: -7341.1, evo: 8847.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 19/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 19/100, Total Loss, Train = 46111.3, Test = 48983.7, LR: 1.00e-03\n",
      "        Train, ce: 46111.3, r2: -7112.4, evo: 9383.6\n",
      "        Test , ce: 48983.7, r2: -7181.4, evo: 9780.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 20/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 20/100, Total Loss, Train = 45790.9, Test = 45293.1, LR: 1.00e-03\n",
      "        Train, ce: 45790.9, r2: -7143.5, evo: 9325.8\n",
      "        Test , ce: 45293.1, r2: -7306.2, evo: 9444.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 21/100: 100%|██████████| 131/131 [00:42<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 21/100, Total Loss, Train = 45617.1, Test = 43222.5, LR: 1.00e-03\n",
      "        Train, ce: 45617.1, r2: -7127.6, evo: 9328.7\n",
      "        Test , ce: 43222.5, r2: -7344.9, evo: 8679.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 22/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 22/100, Total Loss, Train = 44325.7, Test = 45835.8, LR: 1.00e-03\n",
      "        Train, ce: 44325.7, r2: -7156.8, evo: 9175.4\n",
      "        Test , ce: 45835.8, r2: -7233.5, evo: 9138.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 23/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 23/100, Total Loss, Train = 44742.5, Test = 44976.3, LR: 1.00e-03\n",
      "        Train, ce: 44742.5, r2: -7169.5, evo: 9238.6\n",
      "        Test , ce: 44976.3, r2: -7405.2, evo: 8671.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 24/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 24/100, Total Loss, Train = 44035.7, Test = 41529.0, LR: 1.00e-03\n",
      "        Train, ce: 44035.7, r2: -7195.5, evo: 9134.3\n",
      "        Test , ce: 41529.0, r2: -7438.4, evo: 8464.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 25/100: 100%|██████████| 131/131 [00:42<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 25/100, Total Loss, Train = 43619.6, Test = 44251.7, LR: 1.00e-03\n",
      "        Train, ce: 43619.6, r2: -7192.3, evo: 9064.3\n",
      "        Test , ce: 44251.7, r2: -7460.1, evo: 9258.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 26/100: 100%|██████████| 131/131 [00:42<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 26/100, Total Loss, Train = 43063.8, Test = 41002.3, LR: 1.00e-03\n",
      "        Train, ce: 43063.8, r2: -7250.0, evo: 9009.4\n",
      "        Test , ce: 41002.3, r2: -7485.5, evo: 8721.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 27/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 27/100, Total Loss, Train = 42776.3, Test = 41515.5, LR: 1.00e-03\n",
      "        Train, ce: 42776.3, r2: -7231.9, evo: 8983.4\n",
      "        Test , ce: 41515.5, r2: -7459.5, evo: 8947.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 28/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 28/100, Total Loss, Train = 42459.9, Test = 41863.1, LR: 1.00e-03\n",
      "        Train, ce: 42459.9, r2: -7244.3, evo: 8943.7\n",
      "        Test , ce: 41863.1, r2: -7488.1, evo: 8590.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 29/100: 100%|██████████| 131/131 [00:41<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 29/100, Total Loss, Train = 43047.2, Test = 39950.6, LR: 1.00e-03\n",
      "        Train, ce: 43047.2, r2: -7240.0, evo: 9020.2\n",
      "        Test , ce: 39950.6, r2: -7493.7, evo: 8583.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 30/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 30/100, Total Loss, Train = 42192.5, Test = 41836.6, LR: 1.00e-03\n",
      "        Train, ce: 42192.5, r2: -7258.6, evo: 8919.1\n",
      "        Test , ce: 41836.6, r2: -7491.0, evo: 8744.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 31/100: 100%|██████████| 131/131 [00:41<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 31/100, Total Loss, Train = 42435.7, Test = 42564.0, LR: 1.00e-03\n",
      "        Train, ce: 42435.7, r2: -7242.5, evo: 8909.4\n",
      "        Test , ce: 42564.0, r2: -7447.3, evo: 8703.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 32/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 32/100, Total Loss, Train = 42050.2, Test = 40922.7, LR: 1.00e-03\n",
      "        Train, ce: 42050.2, r2: -7257.7, evo: 8893.6\n",
      "        Test , ce: 40922.7, r2: -7464.1, evo: 8997.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 33/100: 100%|██████████| 131/131 [00:42<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 33/100, Total Loss, Train = 41554.0, Test = 42766.6, LR: 1.00e-03\n",
      "        Train, ce: 41554.0, r2: -7275.4, evo: 8804.4\n",
      "        Test , ce: 42766.6, r2: -7528.9, evo: 9241.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 34/100: 100%|██████████| 131/131 [00:42<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 34/100, Total Loss, Train = 41025.5, Test = 37915.8, LR: 1.00e-03\n",
      "        Train, ce: 41025.5, r2: -7305.5, evo: 8763.4\n",
      "        Test , ce: 37915.8, r2: -7565.8, evo: 8264.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 35/100: 100%|██████████| 131/131 [00:42<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 35/100, Total Loss, Train = 40916.4, Test = 37253.3, LR: 1.00e-03\n",
      "        Train, ce: 40916.4, r2: -7317.5, evo: 8753.9\n",
      "        Test , ce: 37253.3, r2: -7601.1, evo: 8159.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 36/100: 100%|██████████| 131/131 [00:41<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 36/100, Total Loss, Train = 40746.4, Test = 39772.3, LR: 1.00e-03\n",
      "        Train, ce: 40746.4, r2: -7318.6, evo: 8724.9\n",
      "        Test , ce: 39772.3, r2: -7492.0, evo: 8441.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 37/100: 100%|██████████| 131/131 [00:42<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 37/100, Total Loss, Train = 40883.3, Test = 39794.6, LR: 1.00e-03\n",
      "        Train, ce: 40883.3, r2: -7295.1, evo: 8745.9\n",
      "        Test , ce: 39794.6, r2: -7506.7, evo: 8314.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 38/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 38/100, Total Loss, Train = 39733.3, Test = 39021.6, LR: 1.00e-03\n",
      "        Train, ce: 39733.3, r2: -7352.9, evo: 8581.8\n",
      "        Test , ce: 39021.6, r2: -7546.8, evo: 8091.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 39/100: 100%|██████████| 131/131 [00:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 39/100, Total Loss, Train = 40391.1, Test = 42246.7, LR: 1.00e-03\n",
      "        Train, ce: 40391.1, r2: -7313.8, evo: 8674.6\n",
      "        Test , ce: 42246.7, r2: -7439.2, evo: 9076.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 40/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 40/100, Total Loss, Train = 39596.4, Test = 38623.2, LR: 1.00e-03\n",
      "        Train, ce: 39596.4, r2: -7356.5, evo: 8594.6\n",
      "        Test , ce: 38623.2, r2: -7596.0, evo: 8050.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 41/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 41/100, Total Loss, Train = 39994.4, Test = 39691.2, LR: 5.00e-04\n",
      "        Train, ce: 39994.4, r2: -7331.2, evo: 8623.3\n",
      "        Test , ce: 39691.2, r2: -7621.0, evo: 8542.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 42/100: 100%|██████████| 131/131 [00:42<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 42/100, Total Loss, Train = 38070.2, Test = 35743.7, LR: 5.00e-04\n",
      "        Train, ce: 38070.2, r2: -7393.2, evo: 8359.7\n",
      "        Test , ce: 35743.7, r2: -7675.8, evo: 7955.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 43/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 43/100, Total Loss, Train = 37159.6, Test = 35162.7, LR: 5.00e-04\n",
      "        Train, ce: 37159.6, r2: -7432.9, evo: 8249.3\n",
      "        Test , ce: 35162.7, r2: -7639.5, evo: 7796.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 44/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 44/100, Total Loss, Train = 36851.9, Test = 34954.7, LR: 5.00e-04\n",
      "        Train, ce: 36851.9, r2: -7430.8, evo: 8215.2\n",
      "        Test , ce: 34954.7, r2: -7686.0, evo: 7982.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 45/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 45/100, Total Loss, Train = 36771.3, Test = 36510.4, LR: 5.00e-04\n",
      "        Train, ce: 36771.3, r2: -7437.9, evo: 8202.5\n",
      "        Test , ce: 36510.4, r2: -7632.2, evo: 8242.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 46/100: 100%|██████████| 131/131 [00:42<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 46/100, Total Loss, Train = 36930.4, Test = 34632.0, LR: 5.00e-04\n",
      "        Train, ce: 36930.4, r2: -7428.4, evo: 8233.4\n",
      "        Test , ce: 34632.0, r2: -7684.4, evo: 8149.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 47/100: 100%|██████████| 131/131 [00:42<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 47/100, Total Loss, Train = 37327.2, Test = 34197.5, LR: 5.00e-04\n",
      "        Train, ce: 37327.2, r2: -7440.8, evo: 8291.4\n",
      "        Test , ce: 34197.5, r2: -7692.5, evo: 7943.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 48/100: 100%|██████████| 131/131 [00:44<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 48/100, Total Loss, Train = 36613.4, Test = 34131.7, LR: 5.00e-04\n",
      "        Train, ce: 36613.4, r2: -7450.5, evo: 8190.8\n",
      "        Test , ce: 34131.7, r2: -7726.4, evo: 7737.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 49/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 49/100, Total Loss, Train = 36668.1, Test = 38048.2, LR: 5.00e-04\n",
      "        Train, ce: 36668.1, r2: -7445.3, evo: 8178.3\n",
      "        Test , ce: 38048.2, r2: -7621.6, evo: 8487.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 50/100: 100%|██████████| 131/131 [00:43<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 50/100, Total Loss, Train = 36906.8, Test = 34559.1, LR: 5.00e-04\n",
      "        Train, ce: 36906.8, r2: -7437.2, evo: 8237.3\n",
      "        Test , ce: 34559.1, r2: -7677.8, evo: 7903.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 51/100: 100%|██████████| 131/131 [00:41<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 51/100, Total Loss, Train = 36295.9, Test = 35303.5, LR: 5.00e-04\n",
      "        Train, ce: 36295.9, r2: -7481.5, evo: 8146.6\n",
      "        Test , ce: 35303.5, r2: -7699.7, evo: 8018.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 52/100: 100%|██████████| 131/131 [00:43<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 52/100, Total Loss, Train = 35961.0, Test = 34936.7, LR: 5.00e-04\n",
      "        Train, ce: 35961.0, r2: -7444.9, evo: 8090.2\n",
      "        Test , ce: 34936.7, r2: -7702.9, evo: 7713.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 53/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 53/100, Total Loss, Train = 35734.9, Test = 36771.2, LR: 5.00e-04\n",
      "        Train, ce: 35734.9, r2: -7479.0, evo: 8064.4\n",
      "        Test , ce: 36771.2, r2: -7665.1, evo: 8597.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 54/100: 100%|██████████| 131/131 [00:43<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 54/100, Total Loss, Train = 36059.7, Test = 34472.1, LR: 2.50e-04\n",
      "        Train, ce: 36059.7, r2: -7443.5, evo: 8114.8\n",
      "        Test , ce: 34472.1, r2: -7707.1, evo: 7735.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 55/100: 100%|██████████| 131/131 [00:41<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 55/100, Total Loss, Train = 34868.5, Test = 32262.8, LR: 2.50e-04\n",
      "        Train, ce: 34868.5, r2: -7499.2, evo: 7933.6\n",
      "        Test , ce: 32262.8, r2: -7784.7, evo: 7495.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 56/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 56/100, Total Loss, Train = 34689.1, Test = 35879.6, LR: 2.50e-04\n",
      "        Train, ce: 34689.1, r2: -7512.1, evo: 7935.2\n",
      "        Test , ce: 35879.6, r2: -7649.0, evo: 7896.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 57/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 57/100, Total Loss, Train = 34342.1, Test = 35891.9, LR: 2.50e-04\n",
      "        Train, ce: 34342.1, r2: -7507.4, evo: 7880.8\n",
      "        Test , ce: 35891.9, r2: -7681.7, evo: 8124.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 58/100: 100%|██████████| 131/131 [00:42<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 58/100, Total Loss, Train = 35033.4, Test = 32273.6, LR: 2.50e-04\n",
      "        Train, ce: 35033.4, r2: -7471.6, evo: 7989.3\n",
      "        Test , ce: 32273.6, r2: -7755.4, evo: 7564.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 59/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 59/100, Total Loss, Train = 34459.3, Test = 33960.1, LR: 2.50e-04\n",
      "        Train, ce: 34459.3, r2: -7496.0, evo: 7894.5\n",
      "        Test , ce: 33960.1, r2: -7701.0, evo: 7824.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 60/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 60/100, Total Loss, Train = 34318.5, Test = 33634.1, LR: 2.50e-04\n",
      "        Train, ce: 34318.5, r2: -7530.6, evo: 7866.5\n",
      "        Test , ce: 33634.1, r2: -7725.9, evo: 7785.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 61/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 61/100, Total Loss, Train = 33913.6, Test = 33244.3, LR: 1.25e-04\n",
      "        Train, ce: 33913.6, r2: -7547.3, evo: 7820.5\n",
      "        Test , ce: 33244.3, r2: -7725.5, evo: 7587.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 62/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 62/100, Total Loss, Train = 33160.0, Test = 31765.4, LR: 1.25e-04\n",
      "        Train, ce: 33160.0, r2: -7545.9, evo: 7690.3\n",
      "        Test , ce: 31765.4, r2: -7781.6, evo: 7466.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 63/100: 100%|██████████| 131/131 [00:42<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 63/100, Total Loss, Train = 33544.6, Test = 32283.7, LR: 1.25e-04\n",
      "        Train, ce: 33544.6, r2: -7551.9, evo: 7771.4\n",
      "        Test , ce: 32283.7, r2: -7780.8, evo: 7523.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 64/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 64/100, Total Loss, Train = 33077.5, Test = 31741.3, LR: 1.25e-04\n",
      "        Train, ce: 33077.5, r2: -7571.9, evo: 7693.1\n",
      "        Test , ce: 31741.3, r2: -7798.5, evo: 7424.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 65/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 65/100, Total Loss, Train = 32999.2, Test = 32322.9, LR: 1.25e-04\n",
      "        Train, ce: 32999.2, r2: -7564.0, evo: 7689.9\n",
      "        Test , ce: 32322.9, r2: -7762.1, evo: 7605.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 66/100: 100%|██████████| 131/131 [00:41<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 66/100, Total Loss, Train = 33209.1, Test = 31356.4, LR: 1.25e-04\n",
      "        Train, ce: 33209.1, r2: -7541.2, evo: 7708.7\n",
      "        Test , ce: 31356.4, r2: -7798.6, evo: 7451.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 67/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 67/100, Total Loss, Train = 33691.5, Test = 30723.8, LR: 1.25e-04\n",
      "        Train, ce: 33691.5, r2: -7532.6, evo: 7787.5\n",
      "        Test , ce: 30723.8, r2: -7808.6, evo: 7357.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 68/100: 100%|██████████| 131/131 [00:43<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 68/100, Total Loss, Train = 32678.0, Test = 31461.9, LR: 1.25e-04\n",
      "        Train, ce: 32678.0, r2: -7582.2, evo: 7639.0\n",
      "        Test , ce: 31461.9, r2: -7806.5, evo: 7453.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 69/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 69/100, Total Loss, Train = 33415.8, Test = 30071.4, LR: 1.25e-04\n",
      "        Train, ce: 33415.8, r2: -7522.7, evo: 7760.1\n",
      "        Test , ce: 30071.4, r2: -7839.7, evo: 7221.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 70/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 70/100, Total Loss, Train = 33030.5, Test = 33451.7, LR: 1.25e-04\n",
      "        Train, ce: 33030.5, r2: -7556.3, evo: 7701.5\n",
      "        Test , ce: 33451.7, r2: -7744.0, evo: 7679.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 71/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 71/100, Total Loss, Train = 33194.8, Test = 31211.2, LR: 1.25e-04\n",
      "        Train, ce: 33194.8, r2: -7544.6, evo: 7719.7\n",
      "        Test , ce: 31211.2, r2: -7797.2, evo: 7492.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 72/100: 100%|██████████| 131/131 [00:43<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 72/100, Total Loss, Train = 33246.5, Test = 30846.3, LR: 1.25e-04\n",
      "        Train, ce: 33246.5, r2: -7553.5, evo: 7725.6\n",
      "        Test , ce: 30846.3, r2: -7825.7, evo: 7381.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 73/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 73/100, Total Loss, Train = 33571.9, Test = 33594.1, LR: 1.25e-04\n",
      "        Train, ce: 33571.9, r2: -7543.5, evo: 7787.6\n",
      "        Test , ce: 33594.1, r2: -7733.7, evo: 7738.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 74/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 74/100, Total Loss, Train = 33439.4, Test = 31939.1, LR: 1.25e-04\n",
      "        Train, ce: 33439.4, r2: -7555.4, evo: 7763.4\n",
      "        Test , ce: 31939.1, r2: -7782.9, evo: 7534.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 75/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 75/100, Total Loss, Train = 33327.2, Test = 31401.9, LR: 6.25e-05\n",
      "        Train, ce: 33327.2, r2: -7539.9, evo: 7746.4\n",
      "        Test , ce: 31401.9, r2: -7821.9, evo: 7494.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 76/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 76/100, Total Loss, Train = 32506.5, Test = 31173.4, LR: 6.25e-05\n",
      "        Train, ce: 32506.5, r2: -7585.8, evo: 7609.0\n",
      "        Test , ce: 31173.4, r2: -7797.2, evo: 7447.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 77/100: 100%|██████████| 131/131 [00:42<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 77/100, Total Loss, Train = 32810.4, Test = 32009.5, LR: 6.25e-05\n",
      "        Train, ce: 32810.4, r2: -7553.5, evo: 7661.5\n",
      "        Test , ce: 32009.5, r2: -7777.5, evo: 7408.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 78/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 78/100, Total Loss, Train = 32676.1, Test = 29179.0, LR: 6.25e-05\n",
      "        Train, ce: 32676.1, r2: -7579.2, evo: 7655.6\n",
      "        Test , ce: 29179.0, r2: -7876.7, evo: 7117.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 79/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 79/100, Total Loss, Train = 32706.7, Test = 31547.8, LR: 6.25e-05\n",
      "        Train, ce: 32706.7, r2: -7567.8, evo: 7647.7\n",
      "        Test , ce: 31547.8, r2: -7815.8, evo: 7482.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 80/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 80/100, Total Loss, Train = 32409.4, Test = 31407.7, LR: 6.25e-05\n",
      "        Train, ce: 32409.4, r2: -7608.2, evo: 7605.9\n",
      "        Test , ce: 31407.7, r2: -7780.4, evo: 7424.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 81/100: 100%|██████████| 131/131 [00:42<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 81/100, Total Loss, Train = 32352.2, Test = 30379.8, LR: 6.25e-05\n",
      "        Train, ce: 32352.2, r2: -7565.2, evo: 7603.3\n",
      "        Test , ce: 30379.8, r2: -7818.7, evo: 7275.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 82/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 82/100, Total Loss, Train = 32285.4, Test = 30226.7, LR: 6.25e-05\n",
      "        Train, ce: 32285.4, r2: -7585.1, evo: 7574.2\n",
      "        Test , ce: 30226.7, r2: -7830.6, evo: 7190.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 83/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 83/100, Total Loss, Train = 32446.1, Test = 27905.1, LR: 6.25e-05\n",
      "        Train, ce: 32446.1, r2: -7580.7, evo: 7613.5\n",
      "        Test , ce: 27905.1, r2: -7894.7, evo: 6873.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 84/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 84/100, Total Loss, Train = 32827.8, Test = 32299.2, LR: 6.25e-05\n",
      "        Train, ce: 32827.8, r2: -7578.1, evo: 7662.0\n",
      "        Test , ce: 32299.2, r2: -7773.2, evo: 7578.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 85/100: 100%|██████████| 131/131 [00:42<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 85/100, Total Loss, Train = 32238.5, Test = 33403.9, LR: 6.25e-05\n",
      "        Train, ce: 32238.5, r2: -7563.6, evo: 7594.6\n",
      "        Test , ce: 33403.9, r2: -7737.2, evo: 7646.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 86/100: 100%|██████████| 131/131 [00:41<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 86/100, Total Loss, Train = 32854.7, Test = 33008.6, LR: 6.25e-05\n",
      "        Train, ce: 32854.7, r2: -7567.7, evo: 7670.9\n",
      "        Test , ce: 33008.6, r2: -7748.7, evo: 7538.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 87/100: 100%|██████████| 131/131 [00:41<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 87/100, Total Loss, Train = 32237.4, Test = 32442.1, LR: 6.25e-05\n",
      "        Train, ce: 32237.4, r2: -7579.5, evo: 7577.1\n",
      "        Test , ce: 32442.1, r2: -7767.7, evo: 7636.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 88/100: 100%|██████████| 131/131 [00:41<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 88/100, Total Loss, Train = 32939.8, Test = 31055.6, LR: 6.25e-05\n",
      "        Train, ce: 32939.8, r2: -7569.0, evo: 7679.6\n",
      "        Test , ce: 31055.6, r2: -7805.2, evo: 7344.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 89/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 89/100, Total Loss, Train = 32006.5, Test = 29926.6, LR: 3.13e-05\n",
      "        Train, ce: 32006.5, r2: -7597.4, evo: 7547.9\n",
      "        Test , ce: 29926.6, r2: -7860.4, evo: 7205.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 90/100: 100%|██████████| 131/131 [00:43<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 90/100, Total Loss, Train = 32643.8, Test = 29333.0, LR: 3.13e-05\n",
      "        Train, ce: 32643.8, r2: -7563.1, evo: 7639.0\n",
      "        Test , ce: 29333.0, r2: -7844.0, evo: 7062.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 91/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 91/100, Total Loss, Train = 32523.3, Test = 32343.8, LR: 3.13e-05\n",
      "        Train, ce: 32523.3, r2: -7582.1, evo: 7615.6\n",
      "        Test , ce: 32343.8, r2: -7775.8, evo: 7553.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 92/100: 100%|██████████| 131/131 [00:42<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 92/100, Total Loss, Train = 32620.8, Test = 29271.6, LR: 3.13e-05\n",
      "        Train, ce: 32620.8, r2: -7585.9, evo: 7636.7\n",
      "        Test , ce: 29271.6, r2: -7855.9, evo: 7090.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 93/100: 100%|██████████| 131/131 [00:42<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 93/100, Total Loss, Train = 31704.5, Test = 33162.3, LR: 3.13e-05\n",
      "        Train, ce: 31704.5, r2: -7603.4, evo: 7506.9\n",
      "        Test , ce: 33162.3, r2: -7752.1, evo: 7754.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 94/100: 100%|██████████| 131/131 [00:42<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 94/100, Total Loss, Train = 32679.5, Test = 30799.4, LR: 3.13e-05\n",
      "        Train, ce: 32679.5, r2: -7558.9, evo: 7636.7\n",
      "        Test , ce: 30799.4, r2: -7826.6, evo: 7377.7\n",
      "Chunk 1/1, Early stopping triggered\n",
      "     MAF_bin Counts test_Acc test_INFO test_IQS test_MaCH train_Acc train_INFO train_IQS train_MaCH\n",
      "(0.00, 0.05)   1573    0.990     0.486    0.513     0.857     0.990      0.557     0.613      0.871\n",
      "(0.05, 0.10)   1310    0.975     0.754    0.793     0.977     0.972      0.732     0.773      0.996\n",
      "(0.10, 0.20)   2533    0.965     0.837    0.864     0.997     0.961      0.807     0.842      1.000\n",
      "(0.20, 0.30)   3400    0.949     0.876    0.888     1.000     0.943      0.850     0.873      1.000\n",
      "(0.30, 0.40)   5601    0.942     0.895    0.899     1.000     0.933      0.871     0.884      1.000\n",
      "(0.40, 0.50)   1375    0.940     0.896    0.899     1.000     0.931      0.875     0.885      1.000\n",
      "  --> Chunk 1 loaded best weights (test_loss=27905.074)\n",
      "==> STAGE1 (Chunk Module) training finished: /mnt/qmtang/EvoFill_data/20251118_v4.3/models/aadr_hg19_chr22_stage1.pth\n"
     ]
    }
   ],
   "source": [
    "verbose            = False\n",
    "max_epochs         = 100\n",
    "lr                 = 0.001\n",
    "weight_decay       = 1e-5\n",
    "earlystop_patience = 11\n",
    "scheduler_factor   = 0.5\n",
    "scheduler_patience = 5\n",
    "scheduler_min_lr   = 1e-8\n",
    "\n",
    "\n",
    "for cid in range(model.n_chunks):\n",
    "    chunk_mask = model.chunk_masks[cid].cpu()\n",
    "    chunk_maf, chunk_bin_cnt = precompute_maf(gt_enc.X_gt[:,chunk_mask.bool().cpu().numpy()].toarray(),  mask_int=gt_enc.seq_depth)\n",
    "    if verbose:\n",
    "        print(f\"=== Chunk {cid + 1} STAT ===\")\n",
    "        maf_df = pd.DataFrame({\n",
    "            'MAF_bin': ['(0.00, 0.05)', '(0.05, 0.10)', '(0.10, 0.20)',\n",
    "                        '(0.20, 0.30)', '(0.30, 0.40)', '(0.40, 0.50)'],\n",
    "            'Counts':  [f\"{c}\" for c in chunk_bin_cnt],\n",
    "        })\n",
    "        print(maf_df.to_string(index=False))\n",
    "\n",
    "    # 收集所有稀疏参数（主要是嵌入层）\n",
    "    sparse_params = []\n",
    "    dense_params = []\n",
    "    # 当前chunk的模块参数（密集）\n",
    "    dense_params.extend(model.chunk_modules[cid].parameters())\n",
    "    \n",
    "    # 全局输出层的卷积参数（密集）\n",
    "    dense_params.extend([model.global_out.w1, model.global_out.b1])\n",
    "    dense_params.extend([model.global_out.w2, model.global_out.b2])\n",
    "    # ULR默认启用\n",
    "    if hasattr(model.global_out, 'ulr_mamba'):\n",
    "        for name, param in model.global_out.ulr_mamba.named_parameters():\n",
    "            if 'idx_embed' in name:\n",
    "                sparse_params.append(param)\n",
    "            else:\n",
    "                dense_params.append(param)\n",
    "    \n",
    "    # 创建分离优化器\n",
    "    optim_sparse = SparseAdam(sparse_params, lr=lr) if sparse_params else None\n",
    "    optim_dense = AdamW(dense_params, lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler_sparse = ReduceLROnPlateau(optim_sparse, mode='min', factor=scheduler_factor, \n",
    "                        patience=scheduler_patience, min_lr=scheduler_min_lr) if optim_sparse else None\n",
    "    scheduler_dense = ReduceLROnPlateau(optim_dense, mode='min', factor=scheduler_factor, \n",
    "                        patience=scheduler_patience, min_lr=scheduler_min_lr)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = earlystop_patience\n",
    "    patience_counter = 0\n",
    "    is_early_stopped = False\n",
    "    train_logs_sum = None\n",
    "    test_logs_sum   = None\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_prob, train_gts, train_mask = [], [], []\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f'Chunk {cid + 1}/{model.n_chunks}, Epoch {epoch + 1}/{max_epochs}',) # leave=False\n",
    "        for batch_idx, (x, target, evo_mat) in enumerate(train_pbar):\n",
    "            x,  target = x.to(device), target.to(device)\n",
    "            if evo_mat.numel() == 0:\n",
    "                evo_mat = None\n",
    "            else:\n",
    "                evo_mat = evo_mat.to(device)\n",
    "\n",
    "            # 清零梯度\n",
    "            if optim_sparse:\n",
    "                optim_sparse.zero_grad()\n",
    "            optim_dense.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            logits, prob, mask_idx = model(x, cid)\n",
    "            loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:, mask_idx], evo_mat)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 更新参数\n",
    "            if optim_sparse:\n",
    "                optim_sparse.step()\n",
    "            optim_dense.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if train_logs_sum is None:          # 第一次初始化\n",
    "                train_logs_sum = {k: 0.0 for k in logs}\n",
    "            for k, v in logs.items():\n",
    "                train_logs_sum[k] += v\n",
    "            # train_pbar.set_postfix({'loss': loss.item(), 'ce':logs['ce'], 'r2':logs['r2'], 'evo':logs['evo']})\n",
    "\n",
    "            # === 收集训练结果 ===\n",
    "            miss_mask = x[:, mask_idx][..., -1].bool()         # 只关心被 mask 的位点\n",
    "            train_prob.append(prob[:, mask_idx].detach())\n",
    "            train_gts.append(target[:,mask_idx].detach())\n",
    "            train_mask.append(miss_mask)\n",
    "\n",
    "        # 训练集 MAF-acc\n",
    "        train_prob = torch.cat(train_prob, dim=0)\n",
    "        train_gts  = torch.cat(train_gts,    dim=0)\n",
    "        train_mask = torch.cat(train_mask,   dim=0)\n",
    "\n",
    "        # ----------- 验证循环同理 ------------\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_prob, test_gts = [], []\n",
    "        if test_logs_sum is None:\n",
    "            test_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "        with torch.no_grad():\n",
    "            for x, target, evo_mat in test_loader:\n",
    "                x,  target = x.to(device), target.to(device)\n",
    "                if evo_mat.numel() == 0:\n",
    "                    evo_mat = None\n",
    "                else:\n",
    "                    evo_mat = evo_mat.to(device)\n",
    "                logits, prob, mask_idx = model(x, cid)\n",
    "                loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:,mask_idx], evo_mat) \n",
    "                test_loss += loss.item()\n",
    "                for k, v in logs.items():\n",
    "                    test_logs_sum[k] += v\n",
    "                test_prob.append(prob[:, mask_idx].detach())\n",
    "                test_gts.append(target[:,mask_idx].detach())\n",
    "\n",
    "        test_prob = torch.cat(test_prob, dim=0)\n",
    "        test_gts    = torch.cat(test_gts,    dim=0)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_test_loss   = test_loss   / len(test_loader)\n",
    "        avg_train_logs = {k: v / len(train_loader) for k, v in train_logs_sum.items()}\n",
    "        avg_test_logs = {k: v / len(test_loader) for k, v in test_logs_sum.items()}\n",
    "        \n",
    "        # 更新学习率\n",
    "        if scheduler_sparse:\n",
    "            scheduler_sparse.step(avg_test_loss)\n",
    "        scheduler_dense.step(avg_test_loss)\n",
    "        \n",
    "        current_denselr = optim_dense.param_groups[0]['lr']\n",
    "        current_sparselr = optim_sparse.param_groups[0]['lr'] if optim_sparse else 0\n",
    "\n",
    "        log_str = (f'Chunk {cid + 1}/{model.n_chunks}, '\n",
    "            f'Epoch {epoch + 1}/{max_epochs}, '\n",
    "            f'Total Loss, Train = {avg_train_loss:.1f}, '\n",
    "            f'Test = {avg_test_loss:.1f}, '\n",
    "            f'LR: {current_denselr:.2e}')\n",
    "\n",
    "        log_str += '\\n        Train'\n",
    "        for k, v in avg_train_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        log_str += '\\n        Test '\n",
    "        for k, v in avg_test_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        print(log_str)\n",
    "\n",
    "        # 清空累加器，供下一个 epoch 使用\n",
    "        train_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "        test_logs_sum   = {k: 0.0 for k in test_logs_sum}\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            patience_counter = 0\n",
    "            # 只存当前 chunk 专家 + 全局层\n",
    "            ckpt = {\n",
    "                'chunk_id': cid,\n",
    "                'chunk_embed_state': model.chunk_embeds[cid].state_dict(),\n",
    "                'chunk_module_state': model.chunk_modules[cid].state_dict(),\n",
    "                'global_out_state': model.global_out.state_dict(),\n",
    "                'best_test_loss': best_loss,\n",
    "            }\n",
    "            torch.save(ckpt, f'{work_dir}/models/{model_name}_chunk[{cid}].pth')\n",
    "            predres_with_bestloss = (train_prob, train_gts, test_prob, test_gts)\n",
    "            if verbose:\n",
    "                train_bins_metrics = metrics_by_maf(train_prob, train_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=train_mask)\n",
    "                test_bins_metrics   = metrics_by_maf(test_prob,  test_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"test\":  test_bins_metrics})\n",
    "                print(f'  --> updated {model_name}_chunk[{cid}].pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= earlystop_patience:\n",
    "                is_early_stopped = True\n",
    "                print(f'Chunk {cid + 1}/{model.n_chunks}, Early stopping triggered')\n",
    "                train_prob, train_gts, test_prob, test_gts = predres_with_bestloss\n",
    "                train_bins_metrics = metrics_by_maf(train_prob, train_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=train_mask)\n",
    "                test_bins_metrics   = metrics_by_maf(test_prob,   test_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"test\":   test_bins_metrics})\n",
    "                break\n",
    "\n",
    "    if not is_early_stopped:\n",
    "        train_bins_metrics = metrics_by_maf(train_prob, train_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=train_mask)\n",
    "        test_bins_metrics   = metrics_by_maf(test_prob,   test_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=None)\n",
    "        print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"test\":   test_bins_metrics})\n",
    "\n",
    "    best_ckpt_path = f'{work_dir}/models/{model_name}_chunk[{cid}].pth'\n",
    "    best_ckpt = torch.load(best_ckpt_path, map_location='cpu')\n",
    "    model.chunk_embeds[cid].load_state_dict(best_ckpt['chunk_embed_state'])\n",
    "    model.chunk_modules[cid].load_state_dict(best_ckpt['chunk_module_state'])\n",
    "    model.global_out.load_state_dict(best_ckpt['global_out_state'])\n",
    "    print(f'  --> Chunk {cid + 1} loaded best weights (test_loss={best_ckpt[\"best_test_loss\"]:.3f})')\n",
    "\n",
    "    # 清理优化器\n",
    "    del optim_sparse, optim_dense\n",
    "    if scheduler_sparse:\n",
    "        del scheduler_sparse\n",
    "    del scheduler_dense\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------- 全部 chunk 训练完成 -> 保存完整模型 ----------------\n",
    "final_ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'n_chunks': model.n_chunks,\n",
    "    'chunk_size': model.chunk_size,\n",
    "    'chunk_overlap': model.chunk_overlap,\n",
    "}\n",
    "torch.save(final_ckpt, f'{work_dir}/models/{model_name}_stage1.pth')\n",
    "print(f'==> STAGE1 (Chunk Module) training finished: {work_dir}/models/{model_name}_stage1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31c2bd",
   "metadata": {},
   "source": [
    "### 1.4 Ultra-Long-Range LD Module Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e507273",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs_per_pair = 100\n",
    "lr                 = 5e-4\n",
    "weight_decay       = 1e-5\n",
    "earlystop_patience = 15\n",
    "batch_size         = 8\n",
    "verbose            = True\n",
    "\n",
    "criterion = ImputationLoss(use_r2=True, use_evo=True, r2_weight=1, evo_weight=4, evo_lambda=10)\n",
    "\n",
    "# ----------- 逐个 chunk 加载权重-----------\n",
    "# for cid in range(model.n_chunks):\n",
    "#     chunk_file = f'{work_dir}/models/{model_name}_chunk_{cid}.pth'\n",
    "#     ckpt = torch.load(chunk_file, map_location='cpu')\n",
    "#     model.chunk_embeds[cid].load_state_dict(ckpt['chunk_embed_state'])\n",
    "#     model.chunk_modules[cid].load_state_dict(ckpt['chunk_module_state'])\n",
    "\n",
    "# ----------- 加载第一阶段完整权重-----------\n",
    "ckpt = torch.load(f'{work_dir}/models/{model_name}_stage1.pth', map_location='cpu')\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "\n",
    "model.eval()        # chunk 专家冻结（requires_grad=False）\n",
    "\n",
    "# 收集所有稀疏参数（主要是嵌入层）\n",
    "sparse_params = []\n",
    "dense_params = []\n",
    "# 全局输出层的卷积参数（密集）\n",
    "dense_params.extend([model.global_out.w1, model.global_out.b1])\n",
    "dense_params.extend([model.global_out.w2, model.global_out.b2])\n",
    "# ULR默认启用\n",
    "if hasattr(model.global_out, 'ulr_mamba'):\n",
    "    for name, param in model.global_out.ulr_mamba.named_parameters():\n",
    "        if 'idx_embed' in name:\n",
    "            sparse_params.append(param)\n",
    "        else:\n",
    "            dense_params.append(param)\n",
    "\n",
    "# 创建分离优化器\n",
    "optim_sparse = SparseAdam(sparse_params, lr=lr) if sparse_params else None\n",
    "optim_dense = AdamW(dense_params, lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler_sparse = ReduceLROnPlateau(optim_sparse, mode='min', factor=0.5, patience=5, min_lr=1e-9) if optim_sparse else None\n",
    "scheduler_dense = ReduceLROnPlateau(optim_dense, mode='min', factor=0.5, patience=5, min_lr=1e-9)\n",
    "\n",
    "pair_list = list(combinations(range(model.n_chunks), 2))\n",
    "np.random.shuffle(pair_list)          # 打乱\n",
    "total_pairs = len(pair_list)\n",
    "\n",
    "for pair_idx, (cid1, cid2) in enumerate(pair_list, 1):\n",
    "    # ====== 构造并集 mask ======\n",
    "    union_mask = (model.chunk_masks[cid1] + model.chunk_masks[cid2]).clamp(max=1).bool()\n",
    "    train_logs_sum = None\n",
    "    val_logs_sum   = None\n",
    "    \n",
    "    # 并集 MAF\n",
    "    union_maf, union_bin_cnt = precompute_maf(\n",
    "        gt_enc.X_gt[:, union_mask.cpu().numpy()].toarray(),\n",
    "        mask_int=gt_enc.seq_depth\n",
    "    )\n",
    "\n",
    "    # ====== 早停变量 ======\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    is_early_stopped = False\n",
    "\n",
    "    # ====== 训练循环 ======\n",
    "    for epoch in range(max_epochs_per_pair):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_prob, train_gts, train_mask = [], [], []\n",
    "\n",
    "        pbar = tqdm(train_loader,\n",
    "                    desc=f'Comb {pair_idx}/{total_pairs}  '\n",
    "                         f'{cid1+1}-{cid2+1}  Epoch {epoch+1}/{max_epochs_per_pair}',\n",
    "                    leave=False)\n",
    "        for x, target, evo_mat in pbar:\n",
    "            x,  target = x.to(device), target.to(device)\n",
    "            if evo_mat.numel() == 0:\n",
    "                evo_mat = None\n",
    "            else:\n",
    "                evo_mat = evo_mat.to(device)\n",
    "\n",
    "            optim_sparse.zero_grad()\n",
    "            optim_dense.zero_grad()\n",
    "\n",
    "            logits, prob, mask_idx = model(x, [cid1, cid2])\n",
    "            loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:,mask_idx], evo_mat) \n",
    "            loss.backward()\n",
    "\n",
    "            optim_sparse.step()   # 只更新嵌入表\n",
    "            optim_dense.step()    # 更新其余所有参数\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if train_logs_sum is None:          # 第一次初始化\n",
    "                train_logs_sum = {k: 0.0 for k in logs}\n",
    "            for k, v in logs.items():\n",
    "                train_logs_sum[k] += v\n",
    "            # pbar.set_postfix({'loss': loss.item(), 'ce':logs['ce'], 'r2':logs['r2'], 'evo':logs['evo']})\n",
    "\n",
    "            # 收集指标\n",
    "            miss_mask = x[:,union_mask][..., -1].bool()\n",
    "            train_prob.append(prob[:, mask_idx].detach())\n",
    "            train_gts.append(target[:,mask_idx].detach())\n",
    "            train_mask.append(miss_mask)\n",
    "\n",
    "        # 训练集 MAF\n",
    "        train_prob = torch.cat(train_prob, dim=0)\n",
    "        train_gts  = torch.cat(train_gts,    dim=0)\n",
    "        train_mask = torch.cat(train_mask,   dim=0)\n",
    "\n",
    "        # ----------- 验证 -----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_prob, val_gts = [], []\n",
    "        with torch.no_grad():\n",
    "            if val_logs_sum is None:\n",
    "                val_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "            for x, target, evo_mat in test_loader:\n",
    "                x = x.to(device)\n",
    "                target = target.to(device)\n",
    "                evo_mat = evo_mat.to(device) if evo_mat.numel() else None\n",
    "                logits, prob, mask_idx = model(x, [cid1, cid2])\n",
    "                loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:,mask_idx], evo_mat)\n",
    "                val_loss += loss.item()\n",
    "                for k, v in logs.items():\n",
    "                    val_logs_sum[k] += v\n",
    "                val_prob.append(prob[:,mask_idx])\n",
    "                val_gts.append(target[:,mask_idx])\n",
    "\n",
    "        val_prob = torch.cat(val_prob, dim=0)\n",
    "        val_gts  = torch.cat(val_gts,    dim=0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss   = val_loss   / len(test_loader)\n",
    "        avg_train_logs = {k: v / len(train_loader) for k, v in train_logs_sum.items()}\n",
    "        avg_val_logs = {k: v / len(test_loader) for k, v in val_logs_sum.items()}\n",
    "        \n",
    "        scheduler_sparse.step(val_loss)\n",
    "        scheduler_dense.step(val_loss)\n",
    "\n",
    "        current_denselr = optim_dense.param_groups[0]['lr']\n",
    "        current_sparselr = optim_sparse.param_groups[0]['lr']\n",
    "\n",
    "        log_str = (f'Comb {pair_idx}/{total_pairs}  '\n",
    "            f'{cid1+1}-{cid2+1}  Epoch {epoch+1}/{max_epochs_per_pair} '\n",
    "            f'Total Loss, Train = {avg_train_loss:.1f}, '\n",
    "            f'Val = {avg_val_loss:.1f}, '\n",
    "            f'dense LR: {current_denselr:.2e}, '\n",
    "            f'sparse LR: {current_sparselr:.2e}')\n",
    "        log_str += '\\n        Train'\n",
    "        for k, v in avg_train_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        log_str += '\\n        Val  '\n",
    "        for k, v in avg_val_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        print(log_str)\n",
    "        # 清空累加器，供下一个 epoch 使用\n",
    "        train_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "        val_logs_sum   = {k: 0.0 for k in val_logs_sum}\n",
    "        # 早停\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'comb': (cid1, cid2),\n",
    "                'global_out': model.global_out.state_dict(),\n",
    "                'best_val_loss': best_loss,\n",
    "                'epoch': epoch,\n",
    "            }, f'{work_dir}/models/{model_name}_chunk[{cid1}-{cid2}].pth')\n",
    "            # MAF 表格\n",
    "            predres_with_bestloss = (train_prob, train_gts, val_prob, val_gts)\n",
    "            if verbose:\n",
    "                train_bins_metrics = metrics_by_maf(train_prob, train_gts, gt_enc.hap_map, union_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_prob,   val_gts, gt_enc.hap_map, union_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"val\":   val_bins_metrics})\n",
    "                print(f'  --> updated {model_name}_chunk[{cid1+1}-{cid2+1}].pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= earlystop_patience:\n",
    "                is_early_stopped = True\n",
    "                print(f'Pair {cid1+1}-{cid2+1} early stopping')\n",
    "                train_prob, train_gts, val_prob, val_gts = predres_with_bestloss\n",
    "                train_bins_metrics = metrics_by_maf(train_prob, train_gts, gt_enc.hap_map, union_maf, mask=train_mask)\n",
    "                val_bins_metrics   = metrics_by_maf(val_prob,   val_gts, gt_enc.hap_map, union_maf, mask=None)\n",
    "                print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"val\":   val_bins_metrics})\n",
    "                break\n",
    "            \n",
    "    if not is_early_stopped:\n",
    "        predres_with_bestloss = (train_prob, train_gts, val_prob, val_gts)\n",
    "        train_bins_metrics = metrics_by_maf(train_prob, train_gts, gt_enc.hap_map, union_maf, mask=train_mask)\n",
    "        val_bins_metrics   = metrics_by_maf(val_prob,   val_gts, gt_enc.hap_map, union_maf, mask=None)\n",
    "        print_maf_stat_df(chunk_bin_cnt,\n",
    "                      {\"train\": train_bins_metrics,\n",
    "                       \"val\":   val_bins_metrics})\n",
    "\n",
    "    # del optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ----------- 全部 pair 结束 -> 保存最终模型 -----------\n",
    "torch.save({\n",
    "    'model_state': model.state_dict(),\n",
    "    'ulr_enabled': True,\n",
    "}, f'{work_dir}/models/{model_name}_stage2_final.pth')\n",
    "print(f'==> STAGE2 training finished: {work_dir}/models/{model_name}_stage2_final.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db68d3e",
   "metadata": {},
   "source": [
    "## 2. Augmentation (aDNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b10fb",
   "metadata": {},
   "source": [
    "### 2.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ba2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 15,792 个位点  \n",
      "[DATA] EvoMat shape: (1000, 1000)\n",
      "[DATA] 结果已写入 /mnt/qmtang/EvoFill_data/20251118_v4.3/augment\n",
      "[DATA] 位点矩阵 = (1000, 15792)，稀疏度 = 80.47%，缺失率 = 0.00%\n",
      "[DATA] 位点字典 = {'0|0': 0, '0|1': 1, '1|1': 2, '.|.': 3, '0/0': 0, '0/1': 1, '1/1': 2, './.': 3}，字典深度 = 4\n",
      "[DATA] 1,000 Samples\n",
      "[DATA] 15,792 Variants Sites\n",
      "[DATA] 4 seq_depth\n"
     ]
    }
   ],
   "source": [
    "gt_enc_aug = GenotypeEncoder(phased=False, gts012=False, save2disk = True, save_dir = Path(work_dir / \"augment\"))\n",
    "gt_enc_aug = gt_enc_aug.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pretrain\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        default_gt    = 'miss',\n",
    "        vcf_path      = \"/mnt/qmtang/EvoFill_data/20251118_v4.3/AADR_chr22_15972sites.hg19.top1000_nearest_BEB_CDX_GIH.vcf.gz\",\n",
    "        evo_mat       = \"/mnt/qmtang/EvoFill_data/20251118_v4.3/AADR_chr22_evomat.tsv\")\n",
    "\n",
    "print(f\"[DATA] {gt_enc_aug.n_samples:,} Samples\")\n",
    "print(f\"[DATA] {gt_enc_aug.n_variants:,} Variants Sites\")\n",
    "print(f\"[DATA] {gt_enc_aug.seq_depth} seq_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7832654",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_enc_aug = GenotypeEncoder.loadfromdisk(Path(work_dir / \"augment\"))\n",
    "print(f'{gt_enc_aug.n_samples:,} samples, {gt_enc_aug.n_variants:,} variants, {gt_enc_aug.seq_depth} seq-depth.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f703a",
   "metadata": {},
   "source": [
    "### 2.2 Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3c27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage3] Stage-1 weights loaded.\n"
     ]
    }
   ],
   "source": [
    "# %%  载入 Stage-2 最终权重\n",
    "ckpt = torch.load(f'{work_dir}/models/{model_name}_stage1.pth', map_location='cpu')\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "\n",
    "criterion = ImputationLoss_Missing(use_r2=True, use_evo=False, r2_weight=1)\n",
    "print('[Stage3] Stage-1 weights loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fe20e",
   "metadata": {},
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fed0124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872 samples in train\n",
      "128 samples in test\n"
     ]
    }
   ],
   "source": [
    "test_n_samples = 128\n",
    "batch_size    = 16\n",
    "max_mr        = 0.7\n",
    "min_mr        = 0.3\n",
    "\n",
    "x_train_indices, x_test_indices = train_test_split(\n",
    "    range(gt_enc_aug.n_samples),\n",
    "    test_size=test_n_samples,\n",
    "    random_state=3047,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"{len(x_train_indices):,} samples in train\")\n",
    "print(f\"{len(x_test_indices):,} samples in test\")\n",
    "# print(f\"evo_mat: {gt_enc.evo_mat.shape}\")\n",
    "\n",
    "train_dataset = GenomicDataset_Missing(\n",
    "    gt_enc_aug,\n",
    "    evo_mat=gt_enc_aug.evo_mat,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_train_indices\n",
    ")\n",
    "\n",
    "test_dataset = GenomicDataset_Missing(\n",
    "    gt_enc_aug,\n",
    "    evo_mat=gt_enc_aug.evo_mat,\n",
    "    mask=True,\n",
    "    masking_rates=(min_mr, max_mr),\n",
    "    indices=x_test_indices\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_onehot = torch.stack([item[0] for item in batch])\n",
    "    y_onehot = torch.stack([item[1] for item in batch])\n",
    "    real_idx_list = [item[2] for item in batch]\n",
    "    # 提取 evo_mat 子矩阵\n",
    "    if train_dataset.evo_mat is not None:\n",
    "        evo_mat_batch = train_dataset.evo_mat[np.ix_(real_idx_list, real_idx_list)]\n",
    "        evo_mat_batch = torch.FloatTensor(evo_mat_batch)\n",
    "    else:\n",
    "        evo_mat_batch = torch.empty(0)\n",
    "\n",
    "    return x_onehot, y_onehot, evo_mat_batch\n",
    "    \n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591d963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 1/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 1/100, Total Loss, Train = 190986.5, Test = 167024.9, LR: 1.00e-03\n",
      "        Train, ce: 190986.5, r2: 60370.5, evo: 0.0\n",
      "        Test , ce: 167024.9, r2: 30550.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 2/100: 100%|██████████| 55/55 [00:11<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 2/100, Total Loss, Train = 159780.4, Test = 162852.4, LR: 1.00e-03\n",
      "        Train, ce: 159780.4, r2: 41093.5, evo: 0.0\n",
      "        Test , ce: 162852.4, r2: 24829.3, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 3/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 3/100, Total Loss, Train = 156245.2, Test = 163274.3, LR: 1.00e-03\n",
      "        Train, ce: 156245.2, r2: 40025.4, evo: 0.0\n",
      "        Test , ce: 163274.3, r2: 22934.9, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 4/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 4/100, Total Loss, Train = 153915.3, Test = 156155.9, LR: 1.00e-03\n",
      "        Train, ce: 153915.3, r2: 40296.4, evo: 0.0\n",
      "        Test , ce: 156155.9, r2: 27817.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 5/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 5/100, Total Loss, Train = 151412.4, Test = 153858.9, LR: 1.00e-03\n",
      "        Train, ce: 151412.4, r2: 39744.9, evo: 0.0\n",
      "        Test , ce: 153858.9, r2: 27504.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 6/100: 100%|██████████| 55/55 [00:11<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 6/100, Total Loss, Train = 150589.7, Test = 151266.2, LR: 1.00e-03\n",
      "        Train, ce: 150589.7, r2: 39605.1, evo: 0.0\n",
      "        Test , ce: 151266.2, r2: 31627.3, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 7/100: 100%|██████████| 55/55 [00:11<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 7/100, Total Loss, Train = 149373.8, Test = 152286.2, LR: 1.00e-03\n",
      "        Train, ce: 149373.8, r2: 39785.9, evo: 0.0\n",
      "        Test , ce: 152286.2, r2: 29077.2, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 8/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 8/100, Total Loss, Train = 148687.6, Test = 150371.3, LR: 1.00e-03\n",
      "        Train, ce: 148687.6, r2: 39614.9, evo: 0.0\n",
      "        Test , ce: 150371.3, r2: 30935.9, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 9/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 9/100, Total Loss, Train = 148327.9, Test = 152531.7, LR: 1.00e-03\n",
      "        Train, ce: 148327.9, r2: 39621.5, evo: 0.0\n",
      "        Test , ce: 152531.7, r2: 30649.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 10/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 10/100, Total Loss, Train = 148236.1, Test = 150289.5, LR: 1.00e-03\n",
      "        Train, ce: 148236.1, r2: 39726.4, evo: 0.0\n",
      "        Test , ce: 150289.5, r2: 34000.2, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 11/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 11/100, Total Loss, Train = 147266.4, Test = 148462.7, LR: 1.00e-03\n",
      "        Train, ce: 147266.4, r2: 39185.5, evo: 0.0\n",
      "        Test , ce: 148462.7, r2: 31742.5, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 12/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 12/100, Total Loss, Train = 147063.1, Test = 151408.5, LR: 1.00e-03\n",
      "        Train, ce: 147063.1, r2: 39592.9, evo: 0.0\n",
      "        Test , ce: 151408.5, r2: 26994.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 13/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 13/100, Total Loss, Train = 147040.6, Test = 148724.5, LR: 1.00e-03\n",
      "        Train, ce: 147040.6, r2: 39326.5, evo: 0.0\n",
      "        Test , ce: 148724.5, r2: 36312.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 14/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 14/100, Total Loss, Train = 146496.1, Test = 147788.8, LR: 1.00e-03\n",
      "        Train, ce: 146496.1, r2: 39472.8, evo: 0.0\n",
      "        Test , ce: 147788.8, r2: 32030.1, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 15/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 15/100, Total Loss, Train = 146072.2, Test = 149599.6, LR: 1.00e-03\n",
      "        Train, ce: 146072.2, r2: 39204.1, evo: 0.0\n",
      "        Test , ce: 149599.6, r2: 28820.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 16/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 16/100, Total Loss, Train = 145780.8, Test = 149371.7, LR: 1.00e-03\n",
      "        Train, ce: 145780.8, r2: 39277.3, evo: 0.0\n",
      "        Test , ce: 149371.7, r2: 32808.2, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 17/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 17/100, Total Loss, Train = 145493.2, Test = 149614.1, LR: 1.00e-03\n",
      "        Train, ce: 145493.2, r2: 39427.3, evo: 0.0\n",
      "        Test , ce: 149614.1, r2: 26761.0, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 18/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 18/100, Total Loss, Train = 146274.2, Test = 148207.5, LR: 1.00e-03\n",
      "        Train, ce: 146274.2, r2: 39440.8, evo: 0.0\n",
      "        Test , ce: 148207.5, r2: 30226.5, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 19/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 19/100, Total Loss, Train = 145723.7, Test = 148526.8, LR: 1.00e-03\n",
      "        Train, ce: 145723.7, r2: 39466.8, evo: 0.0\n",
      "        Test , ce: 148526.8, r2: 28796.2, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 20/100: 100%|██████████| 55/55 [00:11<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 20/100, Total Loss, Train = 145755.6, Test = 149820.2, LR: 5.00e-04\n",
      "        Train, ce: 145755.6, r2: 39411.4, evo: 0.0\n",
      "        Test , ce: 149820.2, r2: 30132.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 21/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 21/100, Total Loss, Train = 145325.3, Test = 145563.5, LR: 5.00e-04\n",
      "        Train, ce: 145325.3, r2: 39228.8, evo: 0.0\n",
      "        Test , ce: 145563.5, r2: 32847.8, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 22/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 22/100, Total Loss, Train = 144090.2, Test = 145922.6, LR: 5.00e-04\n",
      "        Train, ce: 144090.2, r2: 38949.4, evo: 0.0\n",
      "        Test , ce: 145922.6, r2: 33702.1, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 23/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 23/100, Total Loss, Train = 144181.4, Test = 145680.7, LR: 5.00e-04\n",
      "        Train, ce: 144181.4, r2: 39512.6, evo: 0.0\n",
      "        Test , ce: 145680.7, r2: 32171.7, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 24/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 24/100, Total Loss, Train = 144638.4, Test = 146413.2, LR: 5.00e-04\n",
      "        Train, ce: 144638.4, r2: 39226.5, evo: 0.0\n",
      "        Test , ce: 146413.2, r2: 32184.8, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 25/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 25/100, Total Loss, Train = 144822.6, Test = 145109.5, LR: 5.00e-04\n",
      "        Train, ce: 144822.6, r2: 38994.4, evo: 0.0\n",
      "        Test , ce: 145109.5, r2: 35508.5, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 26/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 26/100, Total Loss, Train = 143922.9, Test = 145727.8, LR: 5.00e-04\n",
      "        Train, ce: 143922.9, r2: 39227.7, evo: 0.0\n",
      "        Test , ce: 145727.8, r2: 33313.2, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 27/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 27/100, Total Loss, Train = 144173.7, Test = 145802.3, LR: 5.00e-04\n",
      "        Train, ce: 144173.7, r2: 39287.5, evo: 0.0\n",
      "        Test , ce: 145802.3, r2: 33722.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 28/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 28/100, Total Loss, Train = 144055.0, Test = 145036.6, LR: 5.00e-04\n",
      "        Train, ce: 144055.0, r2: 39129.4, evo: 0.0\n",
      "        Test , ce: 145036.6, r2: 32260.0, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 29/100: 100%|██████████| 55/55 [00:11<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 29/100, Total Loss, Train = 144167.2, Test = 146024.6, LR: 5.00e-04\n",
      "        Train, ce: 144167.2, r2: 39140.2, evo: 0.0\n",
      "        Test , ce: 146024.6, r2: 33255.7, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 30/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 30/100, Total Loss, Train = 143661.0, Test = 145554.3, LR: 5.00e-04\n",
      "        Train, ce: 143661.0, r2: 39074.5, evo: 0.0\n",
      "        Test , ce: 145554.3, r2: 36269.8, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 31/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 31/100, Total Loss, Train = 143859.1, Test = 145446.0, LR: 5.00e-04\n",
      "        Train, ce: 143859.1, r2: 39090.6, evo: 0.0\n",
      "        Test , ce: 145446.0, r2: 39625.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 32/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 32/100, Total Loss, Train = 144367.9, Test = 146265.3, LR: 5.00e-04\n",
      "        Train, ce: 144367.9, r2: 39469.8, evo: 0.0\n",
      "        Test , ce: 146265.3, r2: 34358.9, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 33/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 33/100, Total Loss, Train = 143894.5, Test = 147259.5, LR: 5.00e-04\n",
      "        Train, ce: 143894.5, r2: 39221.6, evo: 0.0\n",
      "        Test , ce: 147259.5, r2: 31247.3, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 34/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 34/100, Total Loss, Train = 144320.5, Test = 145375.6, LR: 2.50e-04\n",
      "        Train, ce: 144320.5, r2: 39111.1, evo: 0.0\n",
      "        Test , ce: 145375.6, r2: 32696.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 35/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 35/100, Total Loss, Train = 143575.3, Test = 143868.1, LR: 2.50e-04\n",
      "        Train, ce: 143575.3, r2: 38999.5, evo: 0.0\n",
      "        Test , ce: 143868.1, r2: 36598.1, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 36/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 36/100, Total Loss, Train = 143020.4, Test = 144089.4, LR: 2.50e-04\n",
      "        Train, ce: 143020.4, r2: 39111.3, evo: 0.0\n",
      "        Test , ce: 144089.4, r2: 37500.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 37/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 37/100, Total Loss, Train = 143383.4, Test = 144967.6, LR: 2.50e-04\n",
      "        Train, ce: 143383.4, r2: 39071.8, evo: 0.0\n",
      "        Test , ce: 144967.6, r2: 36052.3, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 38/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 38/100, Total Loss, Train = 143383.2, Test = 142912.5, LR: 2.50e-04\n",
      "        Train, ce: 143383.2, r2: 39288.7, evo: 0.0\n",
      "        Test , ce: 142912.5, r2: 36705.7, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 39/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 39/100, Total Loss, Train = 142784.1, Test = 144883.6, LR: 2.50e-04\n",
      "        Train, ce: 142784.1, r2: 39024.9, evo: 0.0\n",
      "        Test , ce: 144883.6, r2: 35617.7, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 40/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 40/100, Total Loss, Train = 142265.3, Test = 144190.4, LR: 2.50e-04\n",
      "        Train, ce: 142265.3, r2: 39108.4, evo: 0.0\n",
      "        Test , ce: 144190.4, r2: 35709.3, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 41/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 41/100, Total Loss, Train = 143051.2, Test = 144734.2, LR: 2.50e-04\n",
      "        Train, ce: 143051.2, r2: 39027.5, evo: 0.0\n",
      "        Test , ce: 144734.2, r2: 38441.3, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 42/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 42/100, Total Loss, Train = 142923.4, Test = 144159.4, LR: 2.50e-04\n",
      "        Train, ce: 142923.4, r2: 39382.1, evo: 0.0\n",
      "        Test , ce: 144159.4, r2: 34075.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 43/100: 100%|██████████| 55/55 [00:11<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 43/100, Total Loss, Train = 142947.3, Test = 144624.6, LR: 2.50e-04\n",
      "        Train, ce: 142947.3, r2: 39066.0, evo: 0.0\n",
      "        Test , ce: 144624.6, r2: 37303.1, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 44/100: 100%|██████████| 55/55 [00:11<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 44/100, Total Loss, Train = 143089.0, Test = 143889.2, LR: 1.25e-04\n",
      "        Train, ce: 143089.0, r2: 39118.8, evo: 0.0\n",
      "        Test , ce: 143889.2, r2: 34093.8, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 45/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 45/100, Total Loss, Train = 142329.8, Test = 143452.9, LR: 1.25e-04\n",
      "        Train, ce: 142329.8, r2: 38867.8, evo: 0.0\n",
      "        Test , ce: 143452.9, r2: 37415.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 46/100: 100%|██████████| 55/55 [00:11<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 46/100, Total Loss, Train = 142555.0, Test = 143354.4, LR: 1.25e-04\n",
      "        Train, ce: 142555.0, r2: 39321.2, evo: 0.0\n",
      "        Test , ce: 143354.4, r2: 37591.4, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 47/100: 100%|██████████| 55/55 [00:11<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 47/100, Total Loss, Train = 142646.1, Test = 144497.9, LR: 1.25e-04\n",
      "        Train, ce: 142646.1, r2: 39073.2, evo: 0.0\n",
      "        Test , ce: 144497.9, r2: 35796.9, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 48/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 48/100, Total Loss, Train = 142403.3, Test = 143236.3, LR: 1.25e-04\n",
      "        Train, ce: 142403.3, r2: 39164.3, evo: 0.0\n",
      "        Test , ce: 143236.3, r2: 35914.6, evo: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 49/100: 100%|██████████| 55/55 [00:11<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1, Epoch 49/100, Total Loss, Train = 142245.2, Test = 143638.8, LR: 1.25e-04\n",
      "        Train, ce: 142245.2, r2: 38802.6, evo: 0.0\n",
      "        Test , ce: 143638.8, r2: 39411.5, evo: 0.0\n",
      "Chunk 1/1, Early stopping triggered\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[22], line 185\u001b[0m\n",
      "\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcid\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mn_chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Early stopping triggered\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m    184\u001b[0m train_prob, train_gts, test_prob, test_gts \u001b[38;5;241m=\u001b[39m predres_with_bestloss\n",
      "\u001b[0;32m--> 185\u001b[0m train_bins_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics_by_maf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_gts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhap_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgt_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhap_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaf_vec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk_maf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    186\u001b[0m test_bins_metrics   \u001b[38;5;241m=\u001b[39m metrics_by_maf(test_prob,   test_gts, hap_map \u001b[38;5;241m=\u001b[39m gt_enc\u001b[38;5;241m.\u001b[39mhap_map, maf_vec \u001b[38;5;241m=\u001b[39m chunk_maf, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;32m    187\u001b[0m print_maf_stat_df(chunk_bin_cnt,\n",
      "\u001b[1;32m    188\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_bins_metrics,\n",
      "\u001b[1;32m    189\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:   test_bins_metrics})\n",
      "\n",
      "File \u001b[0;32m/mnt/qmtang/EvoFill/src/utils.py:214\u001b[0m, in \u001b[0;36mmetrics_by_maf\u001b[0;34m(prob, y_true, hap_map, maf_vec, bins, mask)\u001b[0m\n",
      "\u001b[1;32m    211\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# 三分类\u001b[39;00m\n",
      "\u001b[0;32m--> 214\u001b[0m prob3, y3 \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_three_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhap_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# --- 5.1 accuracy 向量化 ---\u001b[39;00m\n",
      "\u001b[1;32m    217\u001b[0m preds \u001b[38;5;241m=\u001b[39m prob3\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/mnt/qmtang/EvoFill/src/utils.py:135\u001b[0m, in \u001b[0;36maggregate_three_classes\u001b[0;34m(prob, y_true, hap_map)\u001b[0m\n",
      "\u001b[1;32m    133\u001b[0m W[np\u001b[38;5;241m.\u001b[39marange(C), gmap] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[1;32m    134\u001b[0m prob3 \u001b[38;5;241m=\u001b[39m prob \u001b[38;5;241m@\u001b[39m W\n",
      "\u001b[0;32m--> 135\u001b[0m y3    \u001b[38;5;241m=\u001b[39m \u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\n",
      "\u001b[1;32m    136\u001b[0m prob3 \u001b[38;5;241m=\u001b[39m prob3 \u001b[38;5;241m/\u001b[39m prob3\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n",
      "\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob3, y3\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)"
     ]
    }
   ],
   "source": [
    "verbose            = False\n",
    "max_epochs         = 100\n",
    "lr                 = 0.001\n",
    "weight_decay       = 1e-5\n",
    "earlystop_patience = 11\n",
    "scheduler_factor   = 0.5\n",
    "scheduler_patience = 5\n",
    "scheduler_min_lr   = 1e-8\n",
    "\n",
    "\n",
    "for cid in range(model.n_chunks):\n",
    "    chunk_mask = model.chunk_masks[cid].cpu()\n",
    "    chunk_maf, chunk_bin_cnt = precompute_maf(gt_enc.X_gt[:,chunk_mask.bool().cpu().numpy()].toarray(),  mask_int=gt_enc.seq_depth)\n",
    "    if verbose:\n",
    "        print(f\"=== Chunk {cid + 1} STAT ===\")\n",
    "        maf_df = pd.DataFrame({\n",
    "            'MAF_bin': ['(0.00, 0.05)', '(0.05, 0.10)', '(0.10, 0.20)',\n",
    "                        '(0.20, 0.30)', '(0.30, 0.40)', '(0.40, 0.50)'],\n",
    "            'Counts':  [f\"{c}\" for c in chunk_bin_cnt],\n",
    "        })\n",
    "        print(maf_df.to_string(index=False))\n",
    "\n",
    "    # 收集所有稀疏参数（主要是嵌入层）\n",
    "    sparse_params = []\n",
    "    dense_params = []\n",
    "    # 当前chunk的模块参数（密集）\n",
    "    dense_params.extend(model.chunk_modules[cid].parameters())\n",
    "    \n",
    "    # 全局输出层的卷积参数（密集）\n",
    "    dense_params.extend([model.global_out.w1, model.global_out.b1])\n",
    "    dense_params.extend([model.global_out.w2, model.global_out.b2])\n",
    "    # ULR默认启用\n",
    "    if hasattr(model.global_out, 'ulr_mamba'):\n",
    "        for name, param in model.global_out.ulr_mamba.named_parameters():\n",
    "            if 'idx_embed' in name:\n",
    "                sparse_params.append(param)\n",
    "            else:\n",
    "                dense_params.append(param)\n",
    "    \n",
    "    # 创建分离优化器\n",
    "    optim_sparse = SparseAdam(sparse_params, lr=lr) if sparse_params else None\n",
    "    optim_dense = AdamW(dense_params, lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler_sparse = ReduceLROnPlateau(optim_sparse, mode='min', factor=scheduler_factor, \n",
    "                        patience=scheduler_patience, min_lr=scheduler_min_lr) if optim_sparse else None\n",
    "    scheduler_dense = ReduceLROnPlateau(optim_dense, mode='min', factor=scheduler_factor, \n",
    "                        patience=scheduler_patience, min_lr=scheduler_min_lr)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = earlystop_patience\n",
    "    patience_counter = 0\n",
    "    is_early_stopped = False\n",
    "    train_logs_sum = None\n",
    "    test_logs_sum   = None\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_prob, train_gts, train_mask = [], [], []\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f'Chunk {cid + 1}/{model.n_chunks}, Epoch {epoch + 1}/{max_epochs}',) # leave=False\n",
    "        for batch_idx, (x, target, evo_mat) in enumerate(train_pbar):\n",
    "            x,  target = x.to(device), target.to(device)\n",
    "            if evo_mat.numel() == 0:\n",
    "                evo_mat = None\n",
    "            else:\n",
    "                evo_mat = evo_mat.to(device)\n",
    "\n",
    "            # 清零梯度\n",
    "            if optim_sparse:\n",
    "                optim_sparse.zero_grad()\n",
    "            optim_dense.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            logits, prob, mask_idx = model(x, cid)\n",
    "            loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:, mask_idx], evo_mat)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 更新参数\n",
    "            if optim_sparse:\n",
    "                optim_sparse.step()\n",
    "            optim_dense.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if train_logs_sum is None:          # 第一次初始化\n",
    "                train_logs_sum = {k: 0.0 for k in logs}\n",
    "            for k, v in logs.items():\n",
    "                train_logs_sum[k] += v\n",
    "            # train_pbar.set_postfix({'loss': loss.item(), 'ce':logs['ce'], 'r2':logs['r2'], 'evo':logs['evo']})\n",
    "\n",
    "            # === 收集训练结果 ===\n",
    "            miss_mask = x[:, mask_idx][..., -1].bool()         # 只关心被 mask 的位点\n",
    "            train_prob.append(prob[:, mask_idx].detach())\n",
    "            train_gts.append(target[:,mask_idx].detach())\n",
    "            train_mask.append(miss_mask)\n",
    "\n",
    "        # 训练集 MAF-acc\n",
    "        train_prob = torch.cat(train_prob, dim=0)\n",
    "        train_gts  = torch.cat(train_gts,    dim=0)\n",
    "        train_mask = torch.cat(train_mask,   dim=0)\n",
    "\n",
    "        # ----------- 验证循环同理 ------------\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_prob, test_gts = [], []\n",
    "        if test_logs_sum is None:\n",
    "            test_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "        with torch.no_grad():\n",
    "            for x, target, evo_mat in test_loader:\n",
    "                x,  target = x.to(device), target.to(device)\n",
    "                if evo_mat.numel() == 0:\n",
    "                    evo_mat = None\n",
    "                else:\n",
    "                    evo_mat = evo_mat.to(device)\n",
    "                logits, prob, mask_idx = model(x, cid)\n",
    "                loss, logs = criterion(logits[:, mask_idx], prob[:, mask_idx], target[:,mask_idx], evo_mat) \n",
    "                test_loss += loss.item()\n",
    "                for k, v in logs.items():\n",
    "                    test_logs_sum[k] += v\n",
    "                test_prob.append(prob[:, mask_idx].detach())\n",
    "                test_gts.append(target[:,mask_idx].detach())\n",
    "\n",
    "        test_prob = torch.cat(test_prob, dim=0)\n",
    "        test_gts    = torch.cat(test_gts,    dim=0)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_test_loss   = test_loss   / len(test_loader)\n",
    "        avg_train_logs = {k: v / len(train_loader) for k, v in train_logs_sum.items()}\n",
    "        avg_test_logs = {k: v / len(test_loader) for k, v in test_logs_sum.items()}\n",
    "        \n",
    "        # 更新学习率\n",
    "        if scheduler_sparse:\n",
    "            scheduler_sparse.step(avg_test_loss)\n",
    "        scheduler_dense.step(avg_test_loss)\n",
    "        \n",
    "        current_denselr = optim_dense.param_groups[0]['lr']\n",
    "        current_sparselr = optim_sparse.param_groups[0]['lr'] if optim_sparse else 0\n",
    "\n",
    "        log_str = (f'Chunk {cid + 1}/{model.n_chunks}, '\n",
    "            f'Epoch {epoch + 1}/{max_epochs}, '\n",
    "            f'Total Loss, Train = {avg_train_loss:.1f}, '\n",
    "            f'Test = {avg_test_loss:.1f}, '\n",
    "            f'LR: {current_denselr:.2e}')\n",
    "\n",
    "        log_str += '\\n        Train'\n",
    "        for k, v in avg_train_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        log_str += '\\n        Test '\n",
    "        for k, v in avg_test_logs.items():\n",
    "            log_str += f', {k}: {v:.1f}'\n",
    "        print(log_str)\n",
    "\n",
    "        # 清空累加器，供下一个 epoch 使用\n",
    "        train_logs_sum = {k: 0.0 for k in train_logs_sum}\n",
    "        test_logs_sum   = {k: 0.0 for k in test_logs_sum}\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            patience_counter = 0\n",
    "            # 只存当前 chunk 专家 + 全局层\n",
    "            ckpt = {\n",
    "                'chunk_id': cid,\n",
    "                'chunk_embed_state': model.chunk_embeds[cid].state_dict(),\n",
    "                'chunk_module_state': model.chunk_modules[cid].state_dict(),\n",
    "                'global_out_state': model.global_out.state_dict(),\n",
    "                'best_test_loss': best_loss,\n",
    "            }\n",
    "            torch.save(ckpt, f'{work_dir}/models/{model_name}_stage3_chunk[{cid}].pth')\n",
    "            predres_with_bestloss = (train_prob, train_gts, test_prob, test_gts)\n",
    "            if verbose:\n",
    "                # train_bins_metrics = metrics_by_maf(train_prob, train_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=train_mask)\n",
    "                # test_bins_metrics   = metrics_by_maf(test_prob,  test_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=None)\n",
    "                # print_maf_stat_df(chunk_bin_cnt,\n",
    "                #       {\"train\": train_bins_metrics,\n",
    "                #        \"test\":  test_bins_metrics})\n",
    "                print(f'  --> updated {model_name}_stage3_chunk[{cid}].pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= earlystop_patience:\n",
    "                is_early_stopped = True\n",
    "                print(f'Chunk {cid + 1}/{model.n_chunks}, Early stopping triggered')\n",
    "                # train_prob, train_gts, test_prob, test_gts = predres_with_bestloss\n",
    "                # train_bins_metrics = metrics_by_maf(train_prob, train_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=train_mask)\n",
    "                # test_bins_metrics   = metrics_by_maf(test_prob,   test_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=None)\n",
    "                # print_maf_stat_df(chunk_bin_cnt,\n",
    "                #       {\"train\": train_bins_metrics,\n",
    "                #        \"test\":   test_bins_metrics})\n",
    "                break\n",
    "\n",
    "    # if not is_early_stopped:\n",
    "    #     train_bins_metrics = metrics_by_maf(train_prob, train_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=train_mask)\n",
    "    #     test_bins_metrics   = metrics_by_maf(test_prob,   test_gts, hap_map = gt_enc.hap_map, maf_vec = chunk_maf, mask=None)\n",
    "    #     print_maf_stat_df(chunk_bin_cnt,\n",
    "    #                   {\"train\": train_bins_metrics,\n",
    "    #                    \"test\":   test_bins_metrics})\n",
    "\n",
    "    best_ckpt_path = f'{work_dir}/models/{model_name}_stage3_chunk[{cid}].pth'\n",
    "    best_ckpt = torch.load(best_ckpt_path, map_location='cpu')\n",
    "    model.chunk_embeds[cid].load_state_dict(best_ckpt['chunk_embed_state'])\n",
    "    model.chunk_modules[cid].load_state_dict(best_ckpt['chunk_module_state'])\n",
    "    model.global_out.load_state_dict(best_ckpt['global_out_state'])\n",
    "    print(f'  --> Chunk {cid + 1} loaded best weights (test_loss={best_ckpt[\"best_test_loss\"]:.3f})')\n",
    "\n",
    "    # 清理优化器\n",
    "    del optim_sparse, optim_dense\n",
    "    if scheduler_sparse:\n",
    "        del scheduler_sparse\n",
    "    del scheduler_dense\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------- 全部 chunk 训练完成 -> 保存完整模型 ----------------\n",
    "final_ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'n_chunks': model.n_chunks,\n",
    "    'chunk_size': model.chunk_size,\n",
    "    'chunk_overlap': model.chunk_overlap,\n",
    "}\n",
    "torch.save(final_ckpt, f'{work_dir}/models/{model_name}_stage3.pth')\n",
    "print(f'==> STAGE1 (Chunk Module) training finished: {work_dir}/models/{model_name}_stage3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca99878",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning (Few-shot URP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0498be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 99,314 个位点  \n",
      "[DATA] EvoMat shape: (16, 16)\n",
      "[DATA] 位点矩阵 = (16, 99314)，稀疏度 = 27.35%，缺失率 = 0.00%\n",
      "[DATA] 位点字典 = {'0|0': 0, '0|1': 1, '1|1': 2, '.|.': 3}，字典深度 = 4\n",
      "[URP] 16 samples, 99314 variants\n"
     ]
    }
   ],
   "source": [
    "# %%  载入 URP 微调数据\n",
    "gt_enc_urp = GenotypeEncoder(phased=False, gts012=False, save2disk=False)\n",
    "gt_enc_urp = gt_enc_urp.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pre_train\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        vcf_path      = work_dir/\"urp_finetune\"/\"minor_pops.10pct.vcf.gz\",\n",
    "        evo_mat       = work_dir/\"urp_finetune\"/\"evo_mat_minor_pops.10pct.tsv\")\n",
    "\n",
    "print(f'[URP] {gt_enc_urp.n_samples} samples, {gt_enc_urp.n_variants} variants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62237762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# %%  Stage-3 超参与配置\n",
    "model_name  = 'hg19_chr22trim'\n",
    "stage3_tag       = 'stage3'\n",
    "max_epochs       = 50\n",
    "warmup_epochs    = 3\n",
    "lr_dense         = 1e-4          # GlobalOut 中稠密参数\n",
    "lr_sparse        = 5e-5          # ULR 中的 idx_embed\n",
    "weight_decay     = 1e-4\n",
    "earlystop_pat    = 9\n",
    "mask_rate_range  = (0.2, 0.6)    # 数据增强：随机缺失率\n",
    "k_fold           = 5             # 交叉验证\n",
    "batch_size       = 4             # 样本少，用小 batch\n",
    "accumulate_grad  = 2             # 梯度累加，等效 batch=8\n",
    "\n",
    "# %%  重新建立「微调」Dataset / Loader\n",
    "urp_dataset = GenomicDataset(\n",
    "        gt_enc_urp.X_gt,\n",
    "        evo_mat      = gt_enc_urp.evo_mat,\n",
    "        seq_depth    = gt_enc_urp.seq_depth,\n",
    "        mask         = True,\n",
    "        masking_rates= mask_rate_range,\n",
    "        indices      = None)               # 全部用于微调\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x = torch.stack([b[0] for b in batch])\n",
    "    y = torch.stack([b[1] for b in batch])\n",
    "    idx = [b[2] for b in batch]\n",
    "    if gt_enc_urp.evo_mat is not None:\n",
    "        evo = gt_enc_urp.evo_mat[np.ix_(idx, idx)]\n",
    "        evo = torch.FloatTensor(evo)\n",
    "    else:\n",
    "        evo = torch.empty(0)\n",
    "    return x, y, evo\n",
    "\n",
    "urp_loader = DataLoader(urp_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4,\n",
    "                        pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# 1. 准备 URP 数据\n",
    "urp_idx = np.arange(gt_enc_urp.n_samples)\n",
    "kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a16f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage3] Stage-2 weights loaded.\n",
      "Epoch 1: avg_val_loss=68964.580, lr_dense=0.0e+00\n",
      "Epoch 2: avg_val_loss=60720.118, lr_dense=3.3e-05\n",
      "Epoch 3: avg_val_loss=49563.400, lr_dense=6.7e-05\n",
      "Epoch 4: avg_val_loss=43253.447, lr_dense=1.0e-04\n",
      "Epoch 5: avg_val_loss=41547.148, lr_dense=1.0e-04\n",
      "Epoch 6: avg_val_loss=34705.204, lr_dense=1.0e-04\n",
      "Epoch 7: avg_val_loss=34851.017, lr_dense=9.9e-05\n",
      "Epoch 8: avg_val_loss=31193.683, lr_dense=9.8e-05\n",
      "Epoch 9: avg_val_loss=25390.002, lr_dense=9.7e-05\n",
      "Epoch 10: avg_val_loss=32677.517, lr_dense=9.6e-05\n",
      "Epoch 11: avg_val_loss=29019.098, lr_dense=9.5e-05\n",
      "Epoch 12: avg_val_loss=25310.655, lr_dense=9.3e-05\n",
      "Epoch 13: avg_val_loss=30964.865, lr_dense=9.1e-05\n",
      "Epoch 14: avg_val_loss=22583.397, lr_dense=8.9e-05\n",
      "Epoch 15: avg_val_loss=26063.507, lr_dense=8.7e-05\n",
      "Epoch 16: avg_val_loss=21442.901, lr_dense=8.5e-05\n",
      "Epoch 17: avg_val_loss=25226.832, lr_dense=8.2e-05\n",
      "Epoch 18: avg_val_loss=21774.188, lr_dense=8.0e-05\n",
      "Epoch 19: avg_val_loss=22828.542, lr_dense=7.7e-05\n",
      "Epoch 20: avg_val_loss=24742.759, lr_dense=7.4e-05\n",
      "Epoch 21: avg_val_loss=23651.618, lr_dense=7.1e-05\n",
      "Epoch 22: avg_val_loss=18767.613, lr_dense=6.8e-05\n",
      "Epoch 23: avg_val_loss=17076.952, lr_dense=6.5e-05\n",
      "Epoch 24: avg_val_loss=19388.626, lr_dense=6.2e-05\n",
      "Epoch 25: avg_val_loss=18005.484, lr_dense=5.8e-05\n",
      "Epoch 26: avg_val_loss=20153.156, lr_dense=5.5e-05\n",
      "Epoch 27: avg_val_loss=18616.133, lr_dense=5.2e-05\n",
      "Epoch 28: avg_val_loss=22284.545, lr_dense=4.8e-05\n",
      "Epoch 29: avg_val_loss=15514.380, lr_dense=4.5e-05\n",
      "Epoch 30: avg_val_loss=20566.186, lr_dense=4.2e-05\n",
      "Epoch 31: avg_val_loss=19128.202, lr_dense=3.8e-05\n",
      "Epoch 32: avg_val_loss=20435.949, lr_dense=3.5e-05\n",
      "Epoch 33: avg_val_loss=18375.392, lr_dense=3.2e-05\n",
      "Epoch 34: avg_val_loss=16949.377, lr_dense=2.9e-05\n",
      "Epoch 35: avg_val_loss=16244.510, lr_dense=2.6e-05\n",
      "Epoch 36: avg_val_loss=18029.811, lr_dense=2.3e-05\n",
      "Epoch 37: avg_val_loss=16218.810, lr_dense=2.0e-05\n",
      "Epoch 38: avg_val_loss=15077.565, lr_dense=1.8e-05\n",
      "Epoch 39: avg_val_loss=15867.217, lr_dense=1.5e-05\n",
      "Epoch 40: avg_val_loss=18540.564, lr_dense=1.3e-05\n",
      "Epoch 41: avg_val_loss=17621.958, lr_dense=1.1e-05\n",
      "Epoch 42: avg_val_loss=14552.724, lr_dense=8.8e-06\n",
      "Epoch 43: avg_val_loss=17220.939, lr_dense=7.0e-06\n",
      "Epoch 44: avg_val_loss=15995.278, lr_dense=5.4e-06\n",
      "Epoch 45: avg_val_loss=14565.957, lr_dense=4.0e-06\n",
      "Epoch 46: avg_val_loss=15530.843, lr_dense=2.8e-06\n",
      "Epoch 47: avg_val_loss=17135.007, lr_dense=1.8e-06\n",
      "Epoch 48: avg_val_loss=15153.898, lr_dense=1.0e-06\n",
      "Epoch 49: avg_val_loss=13754.592, lr_dense=4.5e-07\n",
      "Epoch 50: avg_val_loss=19393.079, lr_dense=1.1e-07\n",
      "==> Stage-3 KFold-loss fine-tuning finished. Best avg_val_loss=13754.592\n"
     ]
    }
   ],
   "source": [
    "# %%  载入 Stage-2 最终权重\n",
    "ckpt = torch.load(f'{work_dir}/models/{model_name}_stage1.pth', map_location='cpu')\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "print('[Stage3] Stage-2 weights loaded.')\n",
    "\n",
    "# %%  参数分组 & 优化器\n",
    "for p in model.parameters():                # 先全部冻结\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 只解冻需要的部分\n",
    "trainable_dense, trainable_sparse = [], []\n",
    "# 1. GlobalOut 全部\n",
    "for name, p in model.global_out.named_parameters():\n",
    "    if 'idx_embed' in name:\n",
    "        trainable_sparse.append(p)\n",
    "    else:\n",
    "        trainable_dense.append(p)\n",
    "# 2. Chunk-Embedding（可选，若显存紧张可留冻）\n",
    "for emb in model.chunk_embeds:\n",
    "    for p in emb.parameters():\n",
    "        trainable_dense.append(p)\n",
    "\n",
    "for p in trainable_dense+trainable_sparse:\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt_dense  = AdamW(trainable_dense,  lr=lr_dense,\n",
    "                   weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "opt_sparse = SparseAdam(trainable_sparse, lr=lr_sparse)\n",
    "\n",
    "# 余弦退火 + 热身\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return epoch / warmup_epochs\n",
    "    return 0.5*(1+np.cos(np.pi*(epoch-warmup_epochs)/(max_epochs-warmup_epochs)))\n",
    "\n",
    "sched_dense  = torch.optim.lr_scheduler.LambdaLR(opt_dense,  lr_lambda)\n",
    "sched_sparse = torch.optim.lr_scheduler.LambdaLR(opt_sparse, lr_lambda)\n",
    "\n",
    "# %%  训练循环\n",
    "criterion = ImputationLoss(use_r2=True, use_evo=True,\n",
    "                           r2_weight=1, evo_weight=4, evo_lambda=10)\n",
    "\n",
    "best_avg_val_loss, patience_counter = np.inf, 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    fold_val_loss = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(urp_idx)):\n",
    "        # ---- 当前折数据 ----\n",
    "        train_dataset = GenomicDataset(\n",
    "                gt_enc_urp.X_gt, evo_mat=gt_enc_urp.evo_mat,\n",
    "                seq_depth=gt_enc_urp.seq_depth, mask=True,\n",
    "                masking_rates=(0.2, 0.6), indices=train_idx)\n",
    "        val_dataset   = GenomicDataset(\n",
    "                gt_enc_urp.X_gt, evo_mat=gt_enc_urp.evo_mat,\n",
    "                seq_depth=gt_enc_urp.seq_depth, mask=True,\n",
    "                masking_rates=(0.2, 0.6), indices=val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8,\n",
    "                                  shuffle=True, num_workers=2,\n",
    "                                  collate_fn=collate_fn, pin_memory=True)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=8,\n",
    "                                  shuffle=False, num_workers=2,\n",
    "                                  collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "        # ---- 训练 ----\n",
    "        for step, (x, y, evo) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            evo  = evo.to(device) if evo.numel() else None\n",
    "\n",
    "            logits, prob, mask_idx = model(x)\n",
    "            loss, _ = criterion(logits[:, mask_idx], prob[:, mask_idx],\n",
    "                                y[:, mask_idx], evo)\n",
    "            loss.backward()\n",
    "\n",
    "            if (step+1) % accumulate_grad == 0 or (step+1) == len(train_loader):\n",
    "                opt_dense.step(); opt_sparse.step()\n",
    "                opt_dense.zero_grad(set_to_none=True); opt_sparse.zero_grad(set_to_none=True)\n",
    "\n",
    "        # ---- 验证 ----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y, evo in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                evo  = evo.to(device) if evo.numel() else None\n",
    "                logits, prob, mask_idx = model(x)\n",
    "                loss, _ = criterion(logits[:, mask_idx], prob[:, mask_idx],\n",
    "                                    y[:, mask_idx], evo)\n",
    "                val_loss += loss.item()\n",
    "        fold_val_loss.append(val_loss / len(val_loader))\n",
    "\n",
    "    # ---- epoch 级日志 & 调度 ----\n",
    "    avg_val_loss = np.mean(fold_val_loss)\n",
    "    print(f'Epoch {epoch+1}: avg_val_loss={avg_val_loss:.3f}, '\n",
    "          f'lr_dense={opt_dense.param_groups[0][\"lr\"]:.1e}')\n",
    "    sched_dense.step(); sched_sparse.step()\n",
    "\n",
    "    # ---- 早停 ----\n",
    "    if avg_val_loss < best_avg_val_loss:\n",
    "        best_avg_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({'model_state': model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'avg_val_loss': avg_val_loss},\n",
    "                   f'{work_dir}/models/{model_name}_{stage3_tag}_best.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= earlystop_pat:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "torch.save({'model_state': model.state_dict(),\n",
    "            'stage3_tag': stage3_tag},\n",
    "           f'{work_dir}/models/{model_name}_{stage3_tag}_final.pth')\n",
    "print(f'==> Stage-3 KFold-loss fine-tuning finished. Best avg_val_loss={best_avg_val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718f797",
   "metadata": {},
   "source": [
    "## 4. Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034fdcb",
   "metadata": {},
   "source": [
    "### 3.1 Load the trained model\n",
    "\n",
    "Choose a path where including `<work_dir>/model` and have trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c2e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Dir: /mnt/qmtang/EvoFill_data/20251107_ver4\n",
      "[INF] Model[hg19_chr22trim] loaded.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Work Dir: {work_dir}\")\n",
    "\n",
    "# ---- 1. 加载模型 ----\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "json_path = f\"{work_dir}/models/model_meta.json\"\n",
    "meta = json.load(open(json_path))\n",
    "model = EvoFill(\n",
    "    d_model=int(meta[\"d_model\"]),\n",
    "    n_alleles=int(meta[\"alleles\"]),\n",
    "    total_sites=int(meta[\"total_sites\"]),\n",
    "    chunk_size=int(meta[\"chunk_size\"]),\n",
    "    chunk_overlap=int(meta[\"overlap\"])\n",
    ").to(device)\n",
    "\n",
    "# ckpt = torch.load(f'{work_dir}/models/{meta[\"model_name\"]}_stage1.pth', map_location=device)\n",
    "# ckpt = torch.load(f'{work_dir}/models/{meta[\"model_name\"]}_stage2_best.pth', map_location=device)\n",
    "ckpt = torch.load(f'{work_dir}/models/{meta[\"model_name\"]}_stage3_best.pth', map_location=device)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "model.eval()\n",
    "print(f'[INF] Model[{meta[\"model_name\"]}] loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542df7b",
   "metadata": {},
   "source": [
    "### 3.2 Encode .vcf file need be impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e661be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 99,314 个位点  \n",
      "[DATA] 位点矩阵 = (152, 99314)，稀疏度 = 63.43%，缺失率 = 50.01%\n",
      "[DATA] 位点字典 = {'0|0': 0, '0|1': 1, '1|1': 2, '.|.': 3}，字典深度 = 4\n",
      "[INFER] 152 samples, 99314 variants\n",
      "[ImputationDataset] 152 samples, missing rate = 50.01%\n"
     ]
    }
   ],
   "source": [
    "gt_enc_imp = GenotypeEncoder(phased=False, gts012=False, save2disk=False)\n",
    "gt_enc_imp = gt_enc_imp.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pre_train\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        vcf_path      = work_dir/\"impute_in\"/\"minor_pops.90pct.masked50p.vcf.gz\" )\n",
    "\n",
    "print(f'[INFER] {gt_enc_imp.n_samples} samples, {gt_enc_imp.n_variants} variants')\n",
    "\n",
    "# ---- 2. 构建推理 Dataset / Loader ----\n",
    "imp_dataset = ImputationDataset(\n",
    "    x_gts_sparse=gt_enc_imp.X_gt,\n",
    "    seq_depth=gt_enc_imp.seq_depth,\n",
    "    indices=None                 # 可传入指定样本索引\n",
    ")\n",
    "imp_dataset.print_missing_stat()          # 查看原始缺失比例\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_onehot = torch.stack([item[0] for item in batch])\n",
    "    real_idx_list = [item[1] for item in batch]\n",
    "    return x_onehot, real_idx_list   # 无 y\n",
    "\n",
    "imp_loader = torch.utils.data.DataLoader(\n",
    "    imp_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809730fd",
   "metadata": {},
   "source": [
    "### 3.3 Inferring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d5223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputing: 100%|██████████| 3/3 [00:20<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INF] 概率矩阵已保存 → /mnt/qmtang/EvoFill_data/20251107_ver4/impute_out/impute_prob.npy with shape = (152, 99314, 3) \n"
     ]
    }
   ],
   "source": [
    "y_prob = []\n",
    "y_mask = []\n",
    "with torch.no_grad():\n",
    "    for x_onehot, real_idx in tqdm(imp_loader, desc='Imputing'):\n",
    "        x_onehot = x_onehot.to(device)\n",
    "        _, prob, _ = model(x_onehot)\n",
    "        miss_mask = x_onehot[..., -1].bool()\n",
    "        y_prob.append(prob)\n",
    "        y_mask.append(miss_mask)\n",
    "y_prob = torch.cat(y_prob, dim=0).cpu().numpy()\n",
    "y_mask = torch.cat(y_mask, dim=0).cpu().numpy()\n",
    "# 4. 保存\n",
    "out_dir = os.path.join(work_dir, 'impute_out')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "np.save(os.path.join(out_dir, 'impute_prob.npy'), y_prob)\n",
    "np.save(os.path.join(out_dir, 'impute_mask.npy'), y_mask)\n",
    "print(f'[INF] 概率矩阵已保存 → {out_dir}/impute_prob.npy '\n",
    "      f'with shape = {y_prob.shape} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27d710",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the imputation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b435b8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] 总计 99,314 个位点  \n",
      "[DATA] 位点矩阵 = (152, 99314)，稀疏度 = 26.84%，缺失率 = 0.00%\n",
      "[DATA] 位点字典 = {'0|0': 0, '0|1': 1, '1|1': 2, '.|.': 3}，字典深度 = 4\n",
      "     MAF_bin Counts val_Acc val_INFO val_IQS val_MaCH\n",
      "(0.00, 0.05)  41365   0.976    0.379   0.082    0.939\n",
      "(0.05, 0.10)   9108   0.930    0.506   0.384    0.770\n",
      "(0.10, 0.20)  11962   0.898    0.614   0.544    0.836\n",
      "(0.20, 0.30)  12887   0.857    0.725   0.668    0.919\n",
      "(0.30, 0.40)  17081   0.844    0.774   0.720    0.962\n",
      "(0.40, 0.50)   6909   0.840    0.784   0.722    0.960\n"
     ]
    }
   ],
   "source": [
    "gt_enc_true = GenotypeEncoder(phased=False, gts012=False, save2disk=False)\n",
    "gt_enc_true = gt_enc_true.encode_ref(\n",
    "        ref_meta_json = work_dir/\"pre_train\"/\"gt_enc_meta.json\",   # 与 Stage1 同构\n",
    "        vcf_path      = work_dir/\"impute_out\"/\"minor_pops.90pct.vcf.gz\" )\n",
    "y_true = gt_enc_true.X_gt.toarray()\n",
    "maf, bin_cnt = precompute_maf(y_true,  mask_int=gt_enc_true.seq_depth)\n",
    "y_true_oh = np.eye(gt_enc_true.seq_depth - 1)[y_true]\n",
    "bins_metrics   = metrics_by_maf(y_prob, y_true_oh, hap_map = gt_enc_true.hap_map, maf_vec = maf, mask=y_mask)\n",
    "print_maf_stat_df(bin_cnt,{'val': bins_metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94e3af",
   "metadata": {},
   "source": [
    "### 3.5 Saving to .vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3c07302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INF] 缺失位点填充完成 → /mnt/qmtang/EvoFill_data/20251107_ver4/impute_out/imputed.vcf.gz\n"
     ]
    }
   ],
   "source": [
    "from cyvcf2 import VCF, Writer\n",
    "\n",
    "# 0. 路径\n",
    "ref_vcf = \"/mnt/qmtang/EvoFill_data/20251107_ver4/minor_pops.masked30p.vcf.gz\"\n",
    "out_vcf = os.path.join(out_dir, 'imputed.vcf.gz')\n",
    "\n",
    "n_site = gt_enc.n_variants\n",
    "n_samp = gt_enc.n_samples\n",
    "n_alleles = gt_enc.seq_depth - 1\n",
    "assert y_prob.shape == (n_samp, n_site, n_alleles)\n",
    "\n",
    "# 2. 反向映射  idx -> '0|0' / '0|1' / ...\n",
    "rev_hap_map = {v: k for k, v in gt_enc.hap_map.items()}\n",
    "\n",
    "samp2idx = {sid: i for i, sid in enumerate(gt_enc.sample_ids)}\n",
    "\n",
    "# 4. 打开参考 VCF\n",
    "invcf = VCF(ref_vcf)\n",
    "tmpl  = invcf\n",
    "tmpl.set_samples(gt_enc.sample_ids)   # 替换样本列\n",
    "\n",
    "out = Writer(out_vcf, tmpl, mode='wz')\n",
    "\n",
    "for rec_idx, rec in enumerate(invcf):\n",
    "    # 当前位点全部样本的 GT\n",
    "    gt_int_pairs = []\n",
    "    for samp_idx, sample_id in enumerate(gt_enc.sample_ids):\n",
    "        old_gt = rec.genotypes[samp_idx]          # [allele1, allele2, phased]\n",
    "        if old_gt[0] == -1 or old_gt[1] == -1:    # 缺失\n",
    "            prob_vec = y_prob[samp_idx, rec_idx, :].ravel()\n",
    "            best_idx = int(prob_vec.argmax())\n",
    "            gt_str   = rev_hap_map[best_idx]\n",
    "            alleles  = list(map(int, gt_str.split('|')))\n",
    "            phased   = old_gt[2] if old_gt[2] != -1 else 1\n",
    "            gt_int_pairs.append([alleles[0], alleles[1], phased])\n",
    "        else:                                       # 非缺失，保持原样\n",
    "            gt_int_pairs.append(old_gt)\n",
    "\n",
    "    # 转成 int8 二维数组  (n_sample, 3)  last dim = [a1,a2,phased]\n",
    "    gt_array = np.array(gt_int_pairs, dtype=np.int8)\n",
    "    rec.set_format('GT', gt_array)\n",
    "    out.write_record(rec)\n",
    "\n",
    "invcf.close()\n",
    "out.close()\n",
    "\n",
    "# 5. tabix\n",
    "os.system(f'tabix -fp vcf {out_vcf}')\n",
    "print(f'[INF] 缺失位点填充完成 → {out_vcf}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
